{
  "requirements": [
    "Step 1: Find an available port by randomly generating port numbers and checking if they are available (/workspace/womd/tools/scripts/dist_train.sh:8-16)",
    "Step 2: Launch distributed training using PyTorch's distributed launch utility with the specified number of GPUs and port (/workspace/womd/tools/scripts/dist_train.sh:18)",
    "Step 3: Pass all additional command-line arguments to the training script (/workspace/womd/tools/scripts/dist_train.sh:5,18)",
    "Step 4: For testing, find an available port using the same method as in training (/workspace/womd/tools/scripts/dist_test.sh:8-16)",
    "Step 5: Launch distributed testing using PyTorch's distributed launch utility with the specified number of GPUs and port (/workspace/womd/tools/scripts/dist_test.sh:18)",
    "Step 6: Pass all additional command-line arguments to the testing script (/workspace/womd/tools/scripts/dist_test.sh:5,18)"
  ],
  "agent_instructions": "Create two bash scripts for distributed training and testing of motion prediction models on the Waymo Open Motion Dataset:\n\n1. Create a distributed training script that:\n   - Takes the number of GPUs as the first argument\n   - Passes all remaining arguments to the training script\n   - Finds an available network port dynamically\n   - Launches training using PyTorch's distributed launch utility\n\n2. Create a distributed testing script that:\n   - Takes the number of GPUs as the first argument\n   - Passes all remaining arguments to the testing script\n   - Finds an available network port dynamically\n   - Launches testing using PyTorch's distributed launch utility\n\nThese scripts will be used to train and evaluate baseline models (MTR) and BeTop-enhanced models, comparing their performance on motion prediction metrics (minADE, minFDE, Miss Rate, and mAP).",
  "masked_source": [
    "/workspace/womd/tools/scripts/dist_train.sh",
    "/workspace/womd/tools/scripts/dist_test.sh"
  ]
}