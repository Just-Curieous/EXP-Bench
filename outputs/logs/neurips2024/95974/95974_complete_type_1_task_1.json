{
  "questions": [
    {
      "hypothesis": "BeTopNet provides enhanced marginal and joint motion prediction accuracy on the WOMD dataset compared to existing state-of-the-art methods without relying on model ensembles or extra data.",
      "method": "Implement the marginal and joint prediction experiments using the WOMD dataset following the leaderboard protocols. For marginal prediction, measure metrics including minADE, minFDE, Miss Rate, mAP, and Soft mAP as described in Table 4. For joint prediction, evaluate using similar metrics with a primary focus on mAP and Soft mAP as detailed in Table 5. Ensure that the experimental settings\u2014including data splits, scenario definitions, and evaluation metrics\u2014strictly adhere to those provided in the paper. Compare BeTopNet\u2019s performance with baselines such as TestReCoAt, HDGT, MTR/MTR++, MGTR, and recent game-theoretic approaches. Additionally, record and report the percentage improvements (for example, +2.7% to +3.4% improvement in mAP for marginal prediction and +4.1% improvement in mAP for joint prediction) to comprehensively assess the effectiveness of the proposed method. Include qualitative analyses from relevant figures, if available, to support the quantitative results.",
      "expected_outcome": "Results are expected to demonstrate that BeTopNet outperforms all baseline methods in key evaluation metrics such as mAP, Soft mAP, and minFDE. The improvements should confirm that the topological formulations coupled with local attention mechanisms offer robust performance gains in both marginal and joint prediction tasks. Detailed percentage improvements, as well as qualitative simulation results, will further establish the efficacy of the proposed approach.",
      "subsection_source": "4.1 Main Result",
      "source": [
        "/workspace/womd/tools/test.py",
        "/workspace/womd/tools/submission.py"
      ],
      "usage_instructions": "To reproduce the marginal and joint motion prediction experiments on the WOMD dataset as described in Tables 4 and 5 of the paper:\n\n1. First, prepare the data following the instructions in /workspace/docs/womd/DataPrep_pred.md\n\n2. For marginal prediction evaluation (Table 4 metrics including minADE, minFDE, Miss Rate, mAP, and Soft mAP):\n   ```bash\n   cd /workspace/womd/tools\n   bash scripts/dist_test.sh [N_GPUS] --cfg_file cfg/BeTopNet_full_64.yaml --batch_size [BATCH_SIZE] --ckpt [PATH_TO_CHECKPOINT]\n   ```\n\n3. For joint prediction evaluation (Table 5 metrics with focus on mAP and Soft mAP):\n   ```bash\n   cd /workspace/womd/tools\n   python submission.py --cfg_file cfg/BeTopNet_full_64.yaml --batch_size [BATCH_SIZE] --ckpt [PATH_TO_CHECKPOINT] --output_dir [OUTPUT_DIR] --interactive\n   ```\n\n4. To compare with baseline methods (TestReCoAt, HDGT, MTR/MTR++, MGTR), you can run the same commands with different configuration files:\n   - For MTR++: `--cfg_file cfg/MTR_PlusPlus.yaml`\n   - For Wayformer: `--cfg_file cfg/Wayformer.yaml`\n\nThe evaluation results will be saved in the specified output directory, showing the performance metrics mentioned in Tables 4 and 5 of the paper, including the percentage improvements of BeTopNet over baseline methods.",
      "requirements": [
        "Step 1: Parse command line arguments including configuration file path, batch size, checkpoint path, and other parameters (/workspace/womd/tools/test.py:28-61, /workspace/womd/tools/submission.py:43-80)",
        "Step 2: Load configuration from YAML file and set up environment (/workspace/womd/tools/test.py:52-59, /workspace/womd/tools/submission.py:71-78)",
        "Step 3: Initialize distributed testing environment if specified (/workspace/womd/tools/test.py:86-93)",
        "Step 4: Set up output directories and logging (/workspace/womd/tools/test.py:101-130, /workspace/womd/tools/submission.py:215-217)",
        "Step 5: Build data loader for test dataset with appropriate configuration (/workspace/womd/tools/test.py:140-145, /workspace/womd/tools/submission.py:219-224)",
        "Step 6: Build model according to configuration (/workspace/womd/tools/test.py:146, /workspace/womd/tools/submission.py:226)",
        "Step 7: Load model parameters from checkpoint (/workspace/womd/tools/test.py:67-68, /workspace/womd/tools/submission.py:228)",
        "Step 8: For marginal prediction: Evaluate model on test data and calculate metrics (/workspace/womd/tools/test.py:77-81)",
        "Step 9: For joint prediction: Generate predictions and serialize them to the required format (/workspace/womd/tools/submission.py:165-181)",
        "Step 10: For joint prediction: Save serialized predictions to file in the proper format for submission (/workspace/womd/tools/submission.py:83-113)",
        "Final Step: Output evaluation results or submission file (/workspace/womd/tools/test.py:148-151, /workspace/womd/tools/submission.py:232-233)"
      ],
      "agent_instructions": "Create two Python scripts for evaluating motion prediction models on the Waymo Open Motion Dataset (WOMD) as described in the paper. The scripts should support both marginal prediction evaluation and joint prediction evaluation.\n\nScript 1: Evaluation Script\n- Create a script that evaluates a trained model on the WOMD test set for marginal motion prediction\n- The script should accept command line arguments for configuration file, batch size, checkpoint path, and other parameters\n- It should load model configuration from a YAML file\n- Support distributed testing across multiple GPUs\n- Build a data loader for the test dataset\n- Load a pre-trained model from a checkpoint\n- Evaluate the model on test data and calculate metrics including minADE, minFDE, Miss Rate, mAP, and Soft mAP\n- Save evaluation results to an output directory\n\nScript 2: Submission Script\n- Create a script that generates predictions for submission to the WOMD challenge\n- The script should accept similar command line arguments as the evaluation script\n- Support both marginal and joint prediction modes (controlled by an --interactive flag)\n- Load a pre-trained model from a checkpoint\n- Generate predictions on the test set\n- Serialize the predictions into the format required by the WOMD challenge\n- For joint prediction, properly handle the serialization of multiple agents' trajectories\n- Save the serialized predictions to a file in the proper format for submission\n\nBoth scripts should work with the BeTopNet model architecture and the WOMD dataset structure as described in the paper. The scripts should be compatible with the data preparation process outlined in the documentation.",
      "masked_source": [
        "/workspace/womd/tools/test.py",
        "/workspace/womd/tools/submission.py"
      ]
    }
  ]
}