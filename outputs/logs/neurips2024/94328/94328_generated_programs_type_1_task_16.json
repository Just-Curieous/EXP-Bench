{
    "source": ["/workspace/e2e/benchmark_layer.py"],
    "usage_instructions": "To run the experiment, execute the benchmark_layer.py script with different prefill sequence lengths and for both models. First, uncomment the LLAMA2-70B model in the model_configs list (line 17). Then run the following commands:\n\n```bash\npython /workspace/e2e/benchmark_layer.py --batch_size 16 --prefill_seq_len 256 --decode_steps 50\npython /workspace/e2e/benchmark_layer.py --batch_size 16 --prefill_seq_len 512 --decode_steps 50\npython /workspace/e2e/benchmark_layer.py --batch_size 16 --prefill_seq_len 1024 --decode_steps 50\npython /workspace/e2e/benchmark_layer.py --batch_size 16 --prefill_seq_len 2048 --decode_steps 50\npython /workspace/e2e/benchmark_layer.py --batch_size 16 --prefill_seq_len 4096 --decode_steps 50\n```\n\nThe script will output the memory saving factor (fp16_peak_mem / quaroT_peak_mem) for each sequence length and model. The output will include lines like 'Memory saving: X.XXXx' which directly answers the question."
}