{
    "source": ["/workspace/fake_quant/main.py"],
    "usage_instructions": "To compare the impact of Hadamard vs. random orthogonal matrices on LLAMA-2 models under 4-bit QuaRot quantization, run the following commands:\n\n1. For LLAMA2-7B with Hadamard matrix (default):\n   ```\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --rotate --rotate_mode hadamard --w_bits 4 --eval_dataset wikitext2\n   ```\n\n2. For LLAMA2-7B with random orthogonal matrix:\n   ```\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --rotate --rotate_mode random --w_bits 4 --eval_dataset wikitext2\n   ```\n\n3. Repeat the above commands for LLAMA2-13B and LLAMA2-70B by changing the model parameter:\n   ```\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-13b-hf --rotate --rotate_mode hadamard --w_bits 4 --eval_dataset wikitext2\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-13b-hf --rotate --rotate_mode random --w_bits 4 --eval_dataset wikitext2\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-70b-hf --rotate --rotate_mode hadamard --w_bits 4 --eval_dataset wikitext2\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-70b-hf --rotate --rotate_mode random --w_bits 4 --eval_dataset wikitext2\n   ```\n\n4. For baseline (FP16) results, run without quantization:\n   ```\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --eval_dataset wikitext2\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-13b-hf --eval_dataset wikitext2\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-70b-hf --eval_dataset wikitext2\n   ```\n\nThe script will output perplexity values for each configuration, which can be compiled into a table similar to Table 8 in the paper's ablation study."
}