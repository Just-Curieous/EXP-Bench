{
    "source": ["/workspace/fake_quant/main.py"],
    "usage_instructions": "To reproduce the zero-shot evaluation results for QuaRot with full 4-bit quantization (A4W4KV4) across PIQA, WinoGrande, HellaSwag, ARC-Easy, ARC-Challenge, and LAMBADA tasks for Llama2 models, run the following commands:\n\n1. For FP16 baseline evaluation of Llama2-7B:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada\n   ```\n\n2. For QuaRot 4-bit quantized evaluation of Llama2-7B:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --rotate --a_bits 4 --w_bits 4 --v_bits 4 --k_bits 4 --w_clip --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada\n   ```\n\n3. Repeat the above commands for Llama2-13B and Llama2-70B by changing the model parameter:\n   ```bash\n   # For Llama2-13B FP16\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-13b-hf --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada\n   \n   # For Llama2-13B QuaRot 4-bit\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-13b-hf --rotate --a_bits 4 --w_bits 4 --v_bits 4 --k_bits 4 --w_clip --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada\n   \n   # For Llama2-70B FP16\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-70b-hf --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada\n   \n   # For Llama2-70B QuaRot 4-bit\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-70b-hf --rotate --a_bits 4 --w_bits 4 --v_bits 4 --k_bits 4 --w_clip --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada\n   ```\n\nThe script will output the accuracy scores for each task and the average accuracy, which can be compared to reproduce the results in Table 2 of the paper."
}