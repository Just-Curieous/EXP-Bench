{
    "source": ["/workspace/fake_quant/main.py"],
    "usage_instructions": "Execute the following command to run the experiment comparing QuaRot-128G with Atom-128G on WikiText-2 perplexity:\n\n1. For Llama-2-7B model:\n/bin/python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --rotate --a_bits 4 --v_bits 4 --k_bits 4 --w_bits 4 --w_clip --w_groupsize 128 --a_groupsize 128 --eval_dataset wikitext2\n\n2. For Llama-2-13B model:\n/bin/python /workspace/fake_quant/main.py --model meta-llama/Llama-2-13b-hf --rotate --a_bits 4 --v_bits 4 --k_bits 4 --w_bits 4 --w_clip --w_groupsize 128 --a_groupsize 128 --eval_dataset wikitext2\n\n3. For Llama-2-70B model:\n/bin/python /workspace/fake_quant/main.py --model meta-llama/Llama-2-70b-hf --rotate --a_bits 4 --v_bits 4 --k_bits 4 --w_bits 4 --w_clip --w_groupsize 128 --a_groupsize 128 --eval_dataset wikitext2\n\nThese commands will run the QuaRot-128G configuration (4-bit quantization with group size 128 and Hadamard rotation) on the specified models and evaluate perplexity on WikiText-2, which directly answers the experiment question by showing that QuaRot-128G achieves comparable or better perplexity than Atom-128G while avoiding high-precision outlier features."
}