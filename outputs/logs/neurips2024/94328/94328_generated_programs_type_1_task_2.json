{
    "source": ["/workspace/fake_quant/main.py"],
    "usage_instructions": "To run the experiment on how clipping ratio affects perplexity on LLAMA2-7B with different quantization approaches, execute the following commands:\n\n1. For symmetric input quantization with different clipping ratios:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --eval_dataset wikitext2 --a_bits 4 --a_clip_ratio 1.0 --w_bits 16\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --eval_dataset wikitext2 --a_bits 4 --a_clip_ratio 0.95 --w_bits 16\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --eval_dataset wikitext2 --a_bits 4 --a_clip_ratio 0.9 --w_bits 16\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --eval_dataset wikitext2 --a_bits 4 --a_clip_ratio 0.85 --w_bits 16\n   ```\n\n2. For asymmetric KV cache quantization with different clipping ratios:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --eval_dataset wikitext2 --k_bits 4 --v_bits 4 --k_asym --v_asym --k_clip_ratio 1.0 --v_clip_ratio 1.0 --w_bits 16\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --eval_dataset wikitext2 --k_bits 4 --v_bits 4 --k_asym --v_asym --k_clip_ratio 0.95 --v_clip_ratio 0.95 --w_bits 16\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --eval_dataset wikitext2 --k_bits 4 --v_bits 4 --k_asym --v_asym --k_clip_ratio 0.9 --v_clip_ratio 0.9 --w_bits 16\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --eval_dataset wikitext2 --k_bits 4 --v_bits 4 --k_asym --v_asym --k_clip_ratio 0.85 --v_clip_ratio 0.85 --w_bits 16\n   ```\n\nNote: The `--w_bits 16` parameter ensures that only the specified components are quantized while keeping the weights in full precision (FP16). The batch size can be adjusted with the `--bsz` parameter if needed (default is 32)."
}