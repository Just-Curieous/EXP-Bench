{
    "source": ["/workspace/fake_quant/main.py"],
    "usage_instructions": "To run the experiment comparing 4-bit QuaRot quantization with FP16 baseline for LLAMA-3 models (8B and 70B), execute the following commands:\n\n1. For LLAMA-3-8B FP16 baseline:\n   ```\n   python /workspace/fake_quant/main.py --model meta-llama/Meta-Llama-3-8B --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada_openai --lm_eval_batch_size 8 --max_length 2048 --use_cache\n   ```\n\n2. For LLAMA-3-8B with QuaRot 4-bit quantization:\n   ```\n   python /workspace/fake_quant/main.py --model meta-llama/Meta-Llama-3-8B --rotate --w_bits 4 --w_clip --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada_openai --lm_eval_batch_size 8 --max_length 2048 --use_cache\n   ```\n\n3. For LLAMA-3-70B FP16 baseline:\n   ```\n   python /workspace/fake_quant/main.py --model meta-llama/Meta-Llama-3-70B --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada_openai --lm_eval_batch_size 8 --max_length 2048 --use_cache\n   ```\n\n4. For LLAMA-3-70B with QuaRot 4-bit quantization:\n   ```\n   python /workspace/fake_quant/main.py --model meta-llama/Meta-Llama-3-70B --rotate --w_bits 4 --w_clip --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada_openai --lm_eval_batch_size 8 --max_length 2048 --use_cache\n   ```\n\nThese commands will run the zero-shot evaluation on the six tasks specified in the experiment question (PIQA, WinoGrande, HellaSwag, Arc-Easy, Arc-Challenge, and LAMBADA) and report the accuracy for each task as well as the average accuracy across all tasks."
}