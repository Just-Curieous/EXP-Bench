{
    "source": ["/workspace/fake_quant/main.py"],
    "usage_instructions": "To reproduce the QuaRot results from Table 1 for LLAMA-2 models on WikiText-2 perplexity, run the following commands:\n\n1. For LLAMA 2-7B:\n```bash\npython /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --rotate --a_bits 4 --v_bits 4 --k_bits 4 --w_bits 4 --w_clip --eval_dataset wikitext2 --bsz 32\n```\n\n2. For LLAMA 2-13B:\n```bash\npython /workspace/fake_quant/main.py --model meta-llama/Llama-2-13b-hf --rotate --a_bits 4 --v_bits 4 --k_bits 4 --w_bits 4 --w_clip --eval_dataset wikitext2 --bsz 32\n```\n\n3. For LLAMA 2-70B:\n```bash\npython /workspace/fake_quant/main.py --model meta-llama/Llama-2-70b-hf --rotate --a_bits 4 --v_bits 4 --k_bits 4 --w_bits 4 --w_clip --eval_dataset wikitext2 --bsz 32\n```\n\nThese commands will run the QuaRot quantization with 4-bit weights, activations, and KV cache, using the Hadamard transformation on W_down layers as described in the paper. The script will evaluate the perplexity on WikiText-2 and output the results."
}