{
    "source": ["/workspace/fake_quant/main.py"],
    "usage_instructions": "To run the experiment comparing QuaRot and QuaRot-128G on LLAMA-3 models, execute the following commands:\n\n1. For LLAMA-3-8B baseline (FP16):\n   python /workspace/fake_quant/main.py --model meta-llama/Meta-Llama-3-8B --eval_dataset wikitext2 --bsz 32\n\n2. For LLAMA-3-8B with QuaRot (standard GPTQ):\n   python /workspace/fake_quant/main.py --model meta-llama/Meta-Llama-3-8B --rotate --w_bits 4 --a_bits 16 --v_bits 16 --k_bits 16 --w_clip --eval_dataset wikitext2 --bsz 32\n\n3. For LLAMA-3-8B with QuaRot-128G (GPTQ with group size 128):\n   python /workspace/fake_quant/main.py --model meta-llama/Meta-Llama-3-8B --rotate --w_bits 4 --a_bits 16 --v_bits 16 --k_bits 16 --w_clip --w_groupsize 128 --a_groupsize 128 --eval_dataset wikitext2 --bsz 32\n\n4. For LLAMA-3-70B baseline (FP16):\n   python /workspace/fake_quant/main.py --model meta-llama/Meta-Llama-3-70B --eval_dataset wikitext2 --bsz 32\n\n5. For LLAMA-3-70B with QuaRot (standard GPTQ):\n   python /workspace/fake_quant/main.py --model meta-llama/Meta-Llama-3-70B --rotate --w_bits 4 --a_bits 16 --v_bits 16 --k_bits 16 --w_clip --eval_dataset wikitext2 --bsz 32\n\n6. For LLAMA-3-70B with QuaRot-128G (GPTQ with group size 128):\n   python /workspace/fake_quant/main.py --model meta-llama/Meta-Llama-3-70B --rotate --w_bits 4 --a_bits 16 --v_bits 16 --k_bits 16 --w_clip --w_groupsize 128 --a_groupsize 128 --eval_dataset wikitext2 --bsz 32\n\nNote: You may need to adjust the batch size (--bsz) depending on your GPU memory. For larger models like LLAMA-3-70B, you might need to use the --distribute flag to distribute the model across multiple GPUs."
}