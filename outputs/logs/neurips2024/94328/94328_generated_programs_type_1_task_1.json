{
    "source": ["/workspace/fake_quant/main.py"],
    "usage_instructions": "To run the experiment comparing different group sizes in QuaRot quantization for LLAMA-2 models, execute the main.py script with different group size settings. For each model size (7B, 13B, 70B) and each group size (64, 128, 256), run:\n\n1. For LLAMA2-7B with different group sizes:\n   ```bash\n   # For 256G group size\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --rotate --a_bits 4 --w_bits 4 --v_bits 4 --k_bits 4 --w_clip --a_groupsize 256 --w_groupsize 256 --v_groupsize 128 --k_groupsize 128 --eval_dataset wikitext2\n   \n   # For 128G group size\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --rotate --a_bits 4 --w_bits 4 --v_bits 4 --k_bits 4 --w_clip --a_groupsize 128 --w_groupsize 128 --v_groupsize 128 --k_groupsize 128 --eval_dataset wikitext2\n   \n   # For 64G group size\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --rotate --a_bits 4 --w_bits 4 --v_bits 4 --k_bits 4 --w_clip --a_groupsize 64 --w_groupsize 64 --v_groupsize 128 --k_groupsize 128 --eval_dataset wikitext2\n   ```\n\n2. Repeat the same commands for LLAMA2-13B by changing the model parameter:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-13b-hf --rotate --a_bits 4 --w_bits 4 --v_bits 4 --k_bits 4 --w_clip --a_groupsize [64/128/256] --w_groupsize [64/128/256] --v_groupsize 128 --k_groupsize 128 --eval_dataset wikitext2\n   ```\n\n3. Repeat the same commands for LLAMA2-70B by changing the model parameter:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-70b-hf --rotate --a_bits 4 --w_bits 4 --v_bits 4 --k_bits 4 --w_clip --a_groupsize [64/128/256] --w_groupsize [64/128/256] --v_groupsize 128 --k_groupsize 128 --eval_dataset wikitext2\n   ```\n\n4. For the baseline (FP16 model without quantization), run:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --eval_dataset wikitext2\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-13b-hf --eval_dataset wikitext2\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-70b-hf --eval_dataset wikitext2\n   ```\n\n5. For the QuaRot default (non-grouped), run:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --rotate --a_bits 4 --w_bits 4 --v_bits 4 --k_bits 4 --w_clip --eval_dataset wikitext2\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-13b-hf --rotate --a_bits 4 --w_bits 4 --v_bits 4 --k_bits 4 --w_clip --eval_dataset wikitext2\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-70b-hf --rotate --a_bits 4 --w_bits 4 --v_bits 4 --k_bits 4 --w_clip --eval_dataset wikitext2\n   ```\n\nThe script will output the perplexity (PPL) for each configuration, which can be compared to understand how group size affects model accuracy."
}