{
    "source": ["/workspace/fake_quant/main.py"],
    "usage_instructions": "To compare FP32 vs FP16 precision for Hadamard transformation, run the following commands:\n\n1. For LLAMA2-7B with FP32 Hadamard:\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --rotate --fp32_had --a_bits 4 --v_bits 4 --k_bits 4 --w_bits 4 --w_clip --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada\n\n2. For LLAMA2-7B with FP16 Hadamard (default):\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --rotate --a_bits 4 --v_bits 4 --k_bits 4 --w_bits 4 --w_clip --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada\n\n3. For LLAMA2-13B with FP32 Hadamard:\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-13b-hf --rotate --fp32_had --a_bits 4 --v_bits 4 --k_bits 4 --w_bits 4 --w_clip --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada\n\n4. For LLAMA2-13B with FP16 Hadamard (default):\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-13b-hf --rotate --a_bits 4 --v_bits 4 --k_bits 4 --w_bits 4 --w_clip --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada\n\nEach command will output perplexity on WikiText-2 and zero-shot accuracy on the six tasks, which can be compiled into a table similar to the expected outcome."
}