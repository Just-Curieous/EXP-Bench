{
    "source": ["/workspace/fake_quant/main.py"],
    "usage_instructions": "To compare QuaRot with RTN weight quantization to the FP16 baseline across WikiText-2 perplexity and zero-shot accuracy tasks, run the following commands:\n\n1. For FP16 baseline on Llama-2-7B:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada\n   ```\n\n2. For QuaRot-RTN INT8 on Llama-2-7B:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --rotate --w_bits 8 --w_rtn --a_bits 8 --v_bits 8 --k_bits 8 --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada\n   ```\n\n3. For QuaRot-RTN INT4 on Llama-2-7B:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-7b-hf --rotate --w_bits 4 --w_rtn --a_bits 4 --v_bits 4 --k_bits 4 --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada\n   ```\n\n4. For FP16 baseline on Llama-2-70B:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-70b-hf --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada --distribute\n   ```\n\n5. For QuaRot-RTN INT8 on Llama-2-70B:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-70b-hf --rotate --w_bits 8 --w_rtn --a_bits 8 --v_bits 8 --k_bits 8 --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada --distribute\n   ```\n\n6. For QuaRot-RTN INT4 on Llama-2-70B:\n   ```bash\n   python /workspace/fake_quant/main.py --model meta-llama/Llama-2-70b-hf --rotate --w_bits 4 --w_rtn --a_bits 4 --v_bits 4 --k_bits 4 --lm_eval --tasks piqa winogrande hellaswag arc_easy arc_challenge lambada --distribute\n   ```\n\nNote: The `--distribute` flag is used for the 70B model to distribute it across multiple GPUs. You may need to provide a Hugging Face token using `--hf_token YOUR_TOKEN` to access the Llama-2 models."
}