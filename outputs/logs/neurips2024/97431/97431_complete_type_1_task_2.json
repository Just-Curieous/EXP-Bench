{
  "questions": [
    {
      "question": "How effective are various defense mechanisms in reducing the Attack Success Rate (ASRAgent) against jailbreak attacks on Vicuna-13B using the MaliciousInstruct dataset?",
      "method": "Model: **Vicuna-13B** (fine-tuned version of LLaMA-13B on user-shared conversations)\n\nDataset: **MaliciousInstruct**: Contains 300 malicious instruction samples categorized across 11 attack intentions. See Appendix A.1 of the paper for details.\n\n#### Attack Methods\n\n| Category     | Method      | Attack Level        | Description                    |\n| ------------ | ----------- | ------------------- | ------------------------------ |\n| Token-level  | GCG         | Gradient-based      | Greedy token-wise optimization |\n| Token-level  | AutoDAN     | Completion-based    | Auto-decoding DAN formulation  |\n| Token-level  | AmpleGCG    | Sample-based        | GCG with gradient sampling     |\n| Prompt-level | AdvPrompter | Iterative prompting | Uses instruction generator     |\n| Prompt-level | PAIR        | Instruction rewrite | Prompt auto-rewriting          |\n| Prompt-level | TAP         | Triggered prompt    | Uses backdoor-like trigger     |\n| Prompt-level | GPTFuzz     | GPT-based attack    | Uses GPT-3.5 to fuzz prompt    |\n\n#### Defense Methods\n\n| Category     | Name                 | Description                                    |\n| ------------ | -------------------- | ---------------------------------------------- |\n| System-Level | Self-Reminder        | Adds safety warnings before generation         |\n| System-Level | RPO                  | Refusal Preference Optimization tuning         |\n| System-Level | SmoothLLM            | Smooths logits to reduce exploitable gradients |\n| Model-Level  | Adversarial Training | Fine-tuned using adversarial inputs            |\n| Model-Level  | Unlearning           | Unlearns malicious outputs using interventions |\n| Model-Level  | Safety Training      | Trained with additional safe instructions      |\n\n#### Evaluation Metrics\n\n- **ASRAgent (%)**: Determined using GPT-4 as a judge (based on prompt and output). Outputs flagged as complying with the malicious intent are considered successful attacks.\n\n#### Procedure\n\n1. Load **Vicuna-13B** with default system prompt and standard prompt template.\n2. For each of the 7 attack methods:\n   - Run a **baseline experiment** (No Defense).\n   - Repeat with each of the 6 selected defense methods individually applied.\n3. Ensure a **consistent attack configuration**:\n   - Attack budget: 60\u2013100 queries (as per Figure 7).\n   - Adversarial suffix lengths: 50\u2013100 tokens (as per Figure 6b).\n4. Sample **100 prompts** per experiment from **MaliciousInstruct**, stratified across the 11 intentions (Figure 8).\n5. Record the **% of successful jailbreaks** as judged by GPT-4 (ASRAgent).",
      "expected_outcome": "Based on results in the paper (Table 4), the following patterns are anticipated:\n\n- **AutoDAN and GPTFuzz** have the **highest ASR** under no defense, indicating vulnerability to advanced token-level and prompt-level attacks.\n- **SmoothLLM**, **Adversarial Training**, and **Unlearning** are expected to provide the **best protection**:\n  - ASR drops from **88% \u2192 19%** (AutoDAN under SmoothLLM).\n  - ASR drops from **93% \u2192 4%** (GPTFuzz under SmoothLLM).\n- **Self-Reminder** offers moderate defense but is less consistent across attack types.\n- **RPO** provides moderate results for some attacks like AutoDAN (88% \u2192 52%) but is ineffective against GPTFuzz (ASR remains 68%+).\n- Certain attacks like **AmpleGCG** and **TAP** are easier to defend against across the board.\n\n| Method      | No Defense | Self-Reminder | RPO   | SmoothLLM | Adv. Training | Unlearning | Safety Training |\n| ----------- | ---------- | ------------- | ----- | --------- | ------------- | ---------- | --------------- |\n| GCG         | 4.00       | 10.00         | 9.00  | 0.00      | 9.00          | 16.00      | 4.00            |\n| AutoDAN     | 88.00      | 48.00         | 52.00 | 19.00     | 63.00         | 66.00      | 60.00           |\n| AmpleGCG    | 12.00      | 6.00          | 0.00  | 11.00     | 2.00          | 25.00      | 9.00            |\n| AdvPrompter | 45.00      | 12.00         | 0.00  | 14.00     | 38.00         | 45.00      | 25.00           |\n| PAIR        | 41.00      | 31.00         | 38.00 | 41.00     | 26.00         | 81.00      | 20.00           |\n| TAP         | 38.00      | 31.00         | 25.00 | 21.00     | 27.00         | 27.00      | 27.00           |\n| GPTFuzz     | 93.00      | 79.00         | 68.00 | 4.00      | 93.00         | 84.00      | 84.00           |",
      "design_complexity": {
        "constant_variables": {
          "llm_model": "Vicuna-13B fine-tuned version (with default system prompt and standard prompt template)",
          "dataset": "MaliciousInstruct dataset with 300 malicious instruction samples across 11 attack intentions",
          "attack_configuration": "Attack budget fixed between 60\u2013100 queries and adversarial suffix lengths fixed between 50\u2013100 tokens for all experiments"
        },
        "independent_variables": {
          "attack_methods": [
            "GCG",
            "AutoDAN",
            "AmpleGCG",
            "AdvPrompter",
            "PAIR",
            "TAP",
            "GPTFuzz"
          ],
          "defense_methods": [
            "No Defense (baseline)",
            "Self-Reminder",
            "RPO",
            "SmoothLLM",
            "Adversarial Training",
            "Unlearning",
            "Safety Training"
          ],
          "attack_intentions": "11 distinct categories of malicious instructions as defined in the dataset"
        },
        "dependent_variables": {
          "ASRAgent": "Percentage of successful jailbreak attacks as determined by GPT-4 (ASRAgent %)"
        }
      },
      "design_ambiguity": {
        "ambiguous_variables": {
          "fine_tuning_details": "The specifics of the fine-tuning process for Vicuna-13B (such as number of epochs, learning rates, data specifics) are not explicitly mentioned.",
          "defense_method_parameters": "The internal configuration parameters for each defense method (e.g., exact implementation of Self-Reminder, RPO, etc.) are not fully detailed.",
          "attack_config_variability": "Although ranges for attack budget and adversarial suffix lengths are given, it is ambiguous whether a fixed value within these ranges is used for each experiment or if they vary within the range.",
          "dataset_composition": "The categorization and distribution of the 300 malicious instruction samples across 11 attack intentions are mentioned but not detailed in the experiment design."
        },
        "possible_modifications": {
          "modification_add_fine_tuning_details": [
            "Include explicit parameters used in the fine-tuning of Vicuna-13B (e.g., epochs, batch size, learning rate)"
          ],
          "modification_refine_defense_parameters": [
            "Detail the individual configuration settings for each defense method to remove ambiguity in implementation"
          ],
          "modification_attack_config": [
            "Clarify whether a fixed value or a dynamic/random value is selected from the given ranges for attack queries and adversarial suffix lengths"
          ],
          "modification_dataset_details": [
            "Provide an extended breakdown of the MaliciousInstruct dataset, such as the number of samples per attack intention and any preprocessing steps applied"
          ]
        }
      },
      "experiment_setup_complexity": {
        "components": [
          "LLM Model: Vicuna-13B fine-tuned version (with default system prompt and standard prompt template)",
          "Dataset: MaliciousInstruct containing 300 malicious instruction samples across 11 attack intentions",
          "Attack Methods: 7 methods (GCG, AutoDAN, AmpleGCG, AdvPrompter, PAIR, TAP, GPTFuzz) with varying attack techniques (token-level and prompt-level)",
          "Defense Methods: 7 methods (No Defense, Self-Reminder, RPO, SmoothLLM, Adversarial Training, Unlearning, Safety Training)",
          "Evaluation Metric: Attack Success Rate (ASRAgent %) determined by GPT-4 judgment",
          "Attack Configuration: Settings including an attack budget of 60\u2013100 queries and adversarial suffix lengths of 50\u2013100 tokens",
          "Sampling Scheme: 100 prompts sampled from MaliciousInstruct, stratified across 11 attack intentions"
        ],
        "setup_steps": [
          "Load Vicuna-13B with the default system prompt and standard prompt template",
          "For each of the 7 attack methods, run a baseline experiment (No Defense) followed by experiments with each of the 6 selected defense methods applied individually",
          "Ensure a consistent attack configuration by fixing the attack budget (60\u2013100 queries) and adversarial suffix length (50\u2013100 tokens) across all experiments",
          "Sample 100 prompts from the MaliciousInstruct dataset, ensuring stratification across the 11 defined attack intentions",
          "Record the percentage of successful jailbreak attacks (ASRAgent) as evaluated by GPT-4"
        ],
        "optional_other_sources_of_complexity": [
          {
            "source": "Attack Methods Implementation",
            "description": "Different attack methods use varied strategies (e.g., gradient-based, completion-based, sample-based, iterative prompting) adding to the overall complexity."
          },
          {
            "source": "Defense Methods Configuration",
            "description": "Each defense method involves unique internal parameters and modifications (e.g., smoothing logits, adversarial training details) that complicate implementation."
          },
          {
            "source": "Attack Configuration Variability",
            "description": "The prescribed ranges for the attack budget and adversarial suffix lengths introduce variability that can impact experimental consistency if not fixed."
          },
          {
            "source": "Dataset Stratification",
            "description": "Ensuring that 100 prompts are correctly stratified across 11 malicious attack intentions requires careful sample selection and handling."
          }
        ]
      },
      "experiment_setup_ambiguity": {
        "ambiguous_components": [
          "Fine-tuning details of Vicuna-13B, such as epochs, learning rates, and batch sizes",
          "Internal configuration parameters for each defense method (e.g., exact implementation steps for Self-Reminder, RPO, etc.)",
          "Detailed composition and categorization methodology of the MaliciousInstruct dataset across the 11 attack intentions"
        ],
        "ambiguous_setup_steps": [
          "Clarification on whether the attack configuration (attack budget and adversarial suffix lengths) uses fixed values or varies randomly within the given ranges",
          "Specifics on how the default system prompt and standard prompt template are loaded and applied are not fully detailed"
        ],
        "possible_modifications": {
          "modification_add_fine_tuning_details": [
            "Include explicit parameters used in the fine-tuning of Vicuna-13B (e.g., number of epochs, batch size, learning rate)"
          ],
          "modification_refine_defense_parameters": [
            "Detail the individual configuration settings for each defense method to remove ambiguity in their implementation"
          ],
          "modification_attack_config": [
            "Clarify whether a fixed value or a dynamic/random value is selected from the given ranges for attack queries and adversarial suffix lengths"
          ],
          "modification_dataset_details": [
            "Provide an extended breakdown of the MaliciousInstruct dataset, such as the number of samples per attack intention and any preprocessing steps applied"
          ]
        }
      },
      "experiment_constraints": {
        "resource_constraints": {},
        "time_constraints": {},
        "money_constraints": {},
        "possible_modifications": {
          "resource_constraints": [
            "Enforce a fixed attack configuration by selecting specific values from the provided ranges (for example, exactly 80 queries and 75 token adversarial suffix) to minimize computational variability and resource usage.",
            "For an extended task, consider testing whether a smaller model (such as a mini or 7B variant) can achieve comparable defense performance, thereby tightening resource usage constraints."
          ],
          "time_constraints": [
            "Standardize attack settings by fixing the number of queries and adversarial suffix tokens to reduce variability in experiment runtime.",
            "For time-sensitive extensions, reduce the total number of prompts sampled from the MaliciousInstruct dataset (currently 100 prompts stratified over 11 attack intentions) to shorten overall experiment duration."
          ],
          "money_constraints": [
            "In case of budget limitations, consider reducing the number of attack and defense combinations run or lower the number of sampled prompts to constrain computational cost.",
            "For extended experiments, if API or GPU usage costs are a concern, test a smaller or less computationally intensive variant while aiming for similar defensive effectiveness."
          ]
        }
      },
      "random_uncertainty": {
        "source": "Attack configuration variability and sample selection randomness",
        "description": "The experiment uses ranges for the attack budget (60\u2013100 queries) and adversarial suffix lengths (50\u2013100 tokens), and samples 100 prompts from the MaliciousInstruct dataset. Random selection within these ranges and stratification introduces stochastic variability in the experiment outcomes. Additionally, randomness inherent in query-based interactions with the LLM can further contribute to instability of gradient updates and unpredictable performance fluctuations.",
        "impact": "This randomness may lead to inconsistent ASR percentages across repeated runs, affecting the reliability of comparisons between different defense methods. Variations in the sampled prompts and the specific attack configurations could mask or exaggerate the observed defense effectiveness.",
        "possible_modifications": [
          "Fix the attack configuration by selecting specific values (e.g., exactly 80 queries and exactly 75 token adversarial suffix) to reduce variability.",
          "Increase the sample size or use multiple fixed sets of prompts to average out randomness in prompt selection.",
          "Control for randomness by running repeated experiments and reporting averaged results with confidence intervals."
        ]
      },
      "systematic_uncertainty": {
        "source": "Ambiguous experimental setup details and potential dataset bias",
        "description": "The experiment design includes ambiguous details such as the fine-tuning parameters for Vicuna-13B, the internal configurations of each defense method, and the precise distribution of the 300 malicious instruction samples across 11 attack intentions. This systematic uncertainty arises when one-time modifications (or unknown defaults) in dataset composition or training settings bias the model consistently in one direction.",
        "impact": "Systematic biases can lead to consistently skewed ASR measurements, misrepresenting the true defense effectiveness. For instance, if the dataset is inadvertently biased (e.g., a higher rate of negative labels for longer reviews) or the defense methods are not uniformly configured, the entire evaluation may misrepresent the LLM's vulnerability.",
        "possible_modifications": [
          "Clarify and include explicit fine-tuning details (number of epochs, learning rates, batch size) to standardize the model's baseline.",
          "Detail the specific internal configurations for each defense method to ensure replicability.",
          "Provide a comprehensive breakdown of the MaliciousInstruct dataset, including sample counts per attack intention and preprocessing steps, to eliminate potential data bias.",
          "Consider running a control experiment with a verified clean dataset to compare results and adjust for potential dataset corruption."
        ]
      },
      "source": [
        "/workspace/main.py",
        "/workspace/scripts/main_vicuna/1_data1_None_defense.sh",
        "/workspace/scripts/main_vicuna/8_data1_self_reminder.sh",
        "/workspace/scripts/main_vicuna/15_data1_RPO.sh",
        "/workspace/scripts/main_vicuna/22_data1_smoothLLM.sh",
        "/workspace/scripts/main_vicuna/29_data1_adv_training_noaug.sh",
        "/workspace/scripts/main_vicuna/36_data1_unlearn.sh",
        "/workspace/scripts/main_vicuna/43_data1_safety_tuning.sh"
      ],
      "usage_instructions": "To evaluate the effectiveness of various defense mechanisms against jailbreak attacks on Vicuna-13B using the MaliciousInstruct dataset, follow these steps:\n\n1. First, ensure you have the MaliciousInstruct dataset at `/workspace/data/MaliciousInstruct_100.csv`.\n\n2. Run the following scripts in sequence to test each attack method against each defense mechanism:\n\n   - For baseline (No Defense):\n     ```\n     bash /workspace/scripts/main_vicuna/1_data1_None_defense.sh  # AutoDAN attack\n     bash /workspace/scripts/main_vicuna/2_data1_None_defense.sh  # PAIR attack\n     bash /workspace/scripts/main_vicuna/3_data1_None_defense.sh  # TAP attack\n     bash /workspace/scripts/main_vicuna/4_data1_None_defense.sh  # GPTFuzz attack\n     bash /workspace/scripts/main_vicuna/5_data1_None_defense.sh  # GCG attack\n     bash /workspace/scripts/main_vicuna/6_data1_None_defense.sh  # AdvPrompter attack\n     bash /workspace/scripts/main_vicuna/7_data1_None_defense.sh  # AmpleGCG attack\n     ```\n\n   - For Self-Reminder defense:\n     ```\n     bash /workspace/scripts/main_vicuna/8_data1_self_reminder.sh  # AutoDAN attack\n     # Continue with scripts 9-14 for other attack methods\n     ```\n\n   - For RPO defense:\n     ```\n     bash /workspace/scripts/main_vicuna/15_data1_RPO.sh  # AutoDAN attack\n     # Continue with scripts 16-21 for other attack methods\n     ```\n\n   - For SmoothLLM defense:\n     ```\n     bash /workspace/scripts/main_vicuna/22_data1_smoothLLM.sh  # AutoDAN attack\n     # Continue with scripts 23-28 for other attack methods\n     ```\n\n   - For Adversarial Training defense:\n     ```\n     bash /workspace/scripts/main_vicuna/29_data1_adv_training_noaug.sh  # AutoDAN attack\n     # Continue with scripts 30-35 for other attack methods\n     ```\n\n   - For Unlearning defense:\n     ```\n     bash /workspace/scripts/main_vicuna/36_data1_unlearn.sh  # AutoDAN attack\n     # Continue with scripts 37-42 for other attack methods\n     ```\n\n   - For Safety Training defense:\n     ```\n     bash /workspace/scripts/main_vicuna/43_data1_safety_tuning.sh  # AutoDAN attack\n     # Continue with scripts 44-49 for other attack methods\n     ```\n\n3. The results will be saved in the `/workspace/exp_results/main_vicuna/` directory, and logs will be stored in `/workspace/exp_logs/main_vicuna/`.\n\n4. Each script uses GPT-4 as a judge to evaluate the Attack Success Rate (ASRAgent) by setting the `--agent_evaluation` flag.\n\nNote: Before running the scripts, make sure you have set up your OpenAI API key as mentioned in the README.md file, as GPT-4 is used for evaluation. You may also need to modify the paths to the defense models if they are not in the default locations.",
      "requirements": [
        "Step 1: Load command-line arguments to configure the experiment, including target model path, defense type, attack method, and evaluation settings (/workspace/main.py:402-407)",
        "Step 2: Set random seed and initialize experiment parameters including device, timestamp, and data paths (/workspace/main.py:320-326)",
        "Step 3: Load harmful prompts and targets from the specified dataset (/workspace/main.py:328)",
        "Step 4: Determine data range to process based on split configuration (/workspace/main.py:330-340)",
        "Step 5: Apply the specified defense mechanism to the input prompts by modifying them according to the defense type (self_reminder, RPO, smoothLLM, etc.) (/workspace/defense.py:44-70)",
        "Step 6: Load appropriate models for the selected attack method (/workspace/main.py:273-311)",
        "Step 7: For each prompt in the dataset, generate an attack result by applying the selected attack method to the defended prompt (/workspace/main.py:231-257)",
        "Step 8: Save intermediate results to file for later analysis (/workspace/main.py:261-266)",
        "Step 9: If using smoothLLM defense, apply additional testing by running the attack outputs through the smoothed model (/workspace/main.py:370-374)",
        "Step 10: If agent evaluation is enabled, use GPT-4 to evaluate whether each attack was successful in bypassing the defense (/workspace/main.py:377-399)",
        "Step 11: Save final evaluation results to file (/workspace/main.py:398)"
      ],
      "agent_instructions": "Your task is to implement a framework for evaluating the effectiveness of various defense mechanisms against jailbreak attacks on large language models. The framework should test how well different defense strategies protect a Vicuna-13B model against common jailbreak attacks using the MaliciousInstruct dataset.\n\nThe framework should support the following defense mechanisms:\n1. No Defense (baseline)\n2. Self-Reminder (prepending a safety reminder to prompts)\n3. RPO (Random Perturbation Optimization - adding specific tokens to prompts)\n4. SmoothLLM (applying random character-level perturbations to prompts)\n5. Adversarial Training (using a model fine-tuned on adversarial examples)\n6. Unlearning (using a model trained to unlearn harmful responses)\n7. Safety Tuning (using a model fine-tuned for safety)\n\nThe framework should support testing these defenses against various attack methods, including AutoDAN, PAIR, TAP, GPTFuzz, GCG, AdvPrompter, and AmpleGCG.\n\nImplement a command-line interface that allows specifying:\n- The target model path\n- The defense method to use\n- The attack method to use\n- The dataset of harmful prompts\n- Output paths for results and logs\n\nThe evaluation process should:\n1. Load the specified harmful prompts dataset\n2. Apply the selected defense mechanism to each prompt\n3. Apply the selected attack method to try to bypass the defense\n4. Evaluate whether the attack was successful using GPT-4 as a judge\n5. Calculate and report the Attack Success Rate (ASR)\n6. Save detailed results for analysis\n\nThe implementation should be modular to allow easy addition of new defense mechanisms and attack methods in the future.",
      "masked_source": [
        "/workspace/main.py",
        "/workspace/scripts/main_vicuna/1_data1_None_defense.sh",
        "/workspace/scripts/main_vicuna/8_data1_self_reminder.sh",
        "/workspace/scripts/main_vicuna/15_data1_RPO.sh",
        "/workspace/scripts/main_vicuna/22_data1_smoothLLM.sh",
        "/workspace/scripts/main_vicuna/29_data1_adv_training_noaug.sh",
        "/workspace/scripts/main_vicuna/36_data1_unlearn.sh",
        "/workspace/scripts/main_vicuna/43_data1_safety_tuning.sh",
        "/workspace/defense.py",
        "/workspace/initialize_args.py",
        "/workspace/GPTEvaluatorAgent/agent_eval.py",
        "/workspace/GPTEvaluatorAgent/main.py",
        "/workspace/GPTEvaluatorAgent/judges.py",
        "/workspace/GPTEvaluatorAgent/system_prompts.py",
        "/workspace/llm_smooth/smoothLLM.py"
      ]
    }
  ]
}