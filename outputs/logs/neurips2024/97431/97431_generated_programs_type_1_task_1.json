{
    "source": ["/workspace/main.py"],
    "usage_instructions": "To evaluate the effectiveness of various defense strategies in reducing the ASRAgent (Attack Success Rate as judged by a GPT-4 agent) for LLaMA-2-7B under different jailbreak attack methods on the AdvBench dataset, run the main.py script multiple times with different combinations of attack and defense methods.\n\nFor each combination of attack method and defense method, run:\n\npython -u main.py \\\n    --target_model_path meta-llama/Llama-2-7b-chat-hf \\\n    --defense_type <DEFENSE_METHOD> \\\n    --attack <ATTACK_METHOD> \\\n    --instructions_path ./data/harmful_bench_50.csv \\\n    --save_result_path ./exp_results/main_llama/ \\\n    --agent_evaluation \\\n    --resume_exp \\\n    --exp_name main_llama_<DEFENSE_METHOD>_<ATTACK_METHOD>\n\nWhere:\n- <DEFENSE_METHOD> is one of: None_defense, self_reminder, RPO, smoothLLM, adv_training_noaug, unlearn, safety_tuning\n- <ATTACK_METHOD> is one of: GCG, AutoDAN, AmpleGCG, AdvPrompter, PAIR, TAP, GPTFuzz\n\nFor example, to evaluate the effectiveness of Self-Reminder defense against GCG attack:\n\npython -u main.py \\\n    --target_model_path meta-llama/Llama-2-7b-chat-hf \\\n    --defense_type self_reminder \\\n    --attack GCG \\\n    --instructions_path ./data/harmful_bench_50.csv \\\n    --save_result_path ./exp_results/main_llama/ \\\n    --agent_evaluation \\\n    --resume_exp \\\n    --exp_name main_llama_self_reminder_GCG\n\nAfter running all 49 combinations (7 attack methods Ã— 7 defense methods), you can analyze the results using the analysis/report_trick.py script:\n\npython analysis/report_trick.py --exp ml\n\nThis will generate a table showing the ASRAgent success rates for each attack-defense combination, similar to the expected outcome table in the experiment question."
}