{
    "source": [
        "/workspace/scripts/6_trick_target_align_autodan.sh",
        "/workspace/scripts/6_trick_target_align_pair.sh"
    ],
    "usage_instructions": "To run the experiment comparing pre-finetuning and post-finetuning models' robustness to jailbreak attacks:\n\n1. First, execute the AutoDAN token-level attack script:\n   ```bash\n   bash /workspace/scripts/6_trick_target_align_autodan.sh\n   ```\n   This will test both Vicuna-7B and Vicuna-13B (which are fine-tuned versions of LLaMA) against AutoDAN attacks.\n\n2. Then, execute the PAIR prompt-level attack script:\n   ```bash\n   bash /workspace/scripts/6_trick_target_align_pair.sh\n   ```\n   This will test the same models against PAIR attacks.\n\n3. The scripts will automatically evaluate the Attack Success Rate (ASR) using both prefix-based evaluation and GPT-based agent evaluation (corresponding to ASRPrefix and ASRAgent in the experiment question).\n\n4. Results will be saved to `/workspace/exp_results/trick_target_align_autodan/` and `/workspace/exp_results/trick_target_align_pair/` respectively.\n\n5. To analyze the results and generate ASR metrics, run:\n   ```bash\n   python /workspace/analysis/report_trick.py --exp t\n   ```\n   This will show a table with ASR percentages for both pre-finetuned and post-finetuned models.\n\nNote: The scripts are already configured to use the appropriate models, attack methods, and evaluation metrics as specified in the experiment question. The repository supports various models including LLaMA, Mistral, Qwen1.5, Tulu, and Vicuna variants, which align with the models mentioned in the experiment question."
}