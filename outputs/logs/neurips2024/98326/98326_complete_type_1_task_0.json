{
  "questions": [
    {
      "hypothesis": "Does distributed training using TorchOpt on 8 GPUs achieve similar training accuracy as sequential training on 1 GPU while significantly reducing wall time?",
      "method": "Design an experiment comparing MAML training on the Omniglot dataset using two setups: one with sequential training on a single GPU and one with distributed training using TorchOpt on 8 GPUs.\n Detailed experiment setup: \nThe experiment will implement MAML (Model-Agnostic Meta-Learning) on the Omniglot dataset. Two training setups will be established: (1) a sequential setup running on a single GPU using standard TorchOpt training, and (2) a distributed setup using TorchOpt with 8 GPUs coordinated by a central controller via RPC, as described in the paper. Both setups will use the same hyperparameters, optimizer settings (e.g., Adam optimizer), and training procedures. The training process will monitor both training accuracy and wall time over a series of iterations (e.g., up to 2000 iterations). The key evaluation metrics will be the convergence of training accuracy (comparing the training accuracy curves) and the total wall time required to achieve similar accuracy levels. The figures in the paper (especially Figure 7) provide comparative training curves and wall time plots to validate the experiment.",
      "expected_outcome": "Based on the paper, the distributed approach should yield comparable training accuracy to sequential training but with markedly reduced training wall time (as seen in the MAML Omniglot experiments).",
      "source": [
        "/workspace/examples/few-shot/maml_omniglot.py",
        "/workspace/examples/distributed/few-shot/maml_omniglot.py"
      ],
      "usage_instructions": "First, run the sequential training script on a single GPU with: `python /workspace/examples/few-shot/maml_omniglot.py`. Then, run the distributed training script on 8 GPUs with: `torchrun --nnode 1 --nproc_per_node 8 /workspace/examples/distributed/few-shot/maml_omniglot.py`. Both scripts implement MAML on the Omniglot dataset with the same hyperparameters and will output training accuracy and wall time metrics. The distributed version uses TorchOpt's RPC-based distributed training framework to parallelize computation across 8 GPUs, which should achieve similar training accuracy but with significantly reduced wall time as shown in Figure 7 of the paper.",
      "requirements": [
        "Step 1: Set up the environment with necessary imports (PyTorch, NumPy, Matplotlib, etc.) (/workspace/examples/few-shot/maml_omniglot.py:42-59, /workspace/examples/distributed/few-shot/maml_omniglot.py:42-63)",
        "Step 2: Parse command line arguments for experiment configuration (n_way, k_spt, k_qry, task_num, seed) (/workspace/examples/few-shot/maml_omniglot.py:62-74, /workspace/examples/distributed/few-shot/maml_omniglot.py:125-136)",
        "Step 3: Set random seeds for reproducibility (/workspace/examples/few-shot/maml_omniglot.py:76-82, /workspace/examples/distributed/few-shot/maml_omniglot.py:138-143)",
        "Step 4: Load the Omniglot dataset with N-way K-shot configuration (/workspace/examples/few-shot/maml_omniglot.py:85-95, /workspace/examples/distributed/few-shot/maml_omniglot.py:107-120)",
        "Step 5: Create a CNN model for few-shot classification (/workspace/examples/few-shot/maml_omniglot.py:98-113, /workspace/examples/distributed/few-shot/maml_omniglot.py:88-104)",
        "Step 6: Initialize meta-optimizer (Adam) for updating model parameters (/workspace/examples/few-shot/maml_omniglot.py:117, /workspace/examples/distributed/few-shot/maml_omniglot.py:153)",
        "Step 7: Implement the training loop with inner and outer optimization (/workspace/examples/few-shot/maml_omniglot.py:127-194, /workspace/examples/distributed/few-shot/maml_omniglot.py:206-247)",
        "Step 8: For the distributed version, implement worker initialization and RPC-based parallelization (/workspace/examples/distributed/few-shot/maml_omniglot.py:66-86, 163-202)",
        "Step 9: Implement the testing procedure to evaluate model performance (/workspace/examples/few-shot/maml_omniglot.py:197-252, /workspace/examples/distributed/few-shot/maml_omniglot.py:250-287)",
        "Step 10: Create visualization of training and testing accuracy (/workspace/examples/few-shot/maml_omniglot.py:255-274, /workspace/examples/distributed/few-shot/maml_omniglot.py:290-310)",
        "Final Step: Execute the main function to run the experiment (/workspace/examples/few-shot/maml_omniglot.py:119-124, /workspace/examples/distributed/few-shot/maml_omniglot.py:156-160)"
      ],
      "agent_instructions": "Your task is to implement two versions of Model-Agnostic Meta-Learning (MAML) for few-shot classification on the Omniglot dataset: a sequential version and a distributed version.\n\n1. Sequential Version:\n   - Implement MAML for few-shot Omniglot classification that runs on a single GPU\n   - Use TorchOpt for optimization\n   - Create a CNN model with convolutional layers, batch normalization, ReLU activations, and max pooling\n   - Implement the meta-learning algorithm with:\n     * An inner loop that adapts model parameters to support set tasks\n     * An outer loop that updates meta-parameters based on query set performance\n   - Use Adam as the meta-optimizer\n   - Include functions for training, testing, and visualizing results\n\n2. Distributed Version:\n   - Adapt the sequential version to use TorchOpt's RPC-based distributed training framework\n   - Parallelize the inner loop computation across multiple GPUs\n   - Implement proper worker initialization for distributed training\n   - Use decorators to control which functions run on which workers\n   - Implement a reducer function to aggregate results from multiple workers\n\nBoth implementations should:\n- Support N-way K-shot classification (configurable via command line arguments)\n- Load the Omniglot dataset with appropriate transformations\n- Run for 10 epochs, reporting training and testing accuracy\n- Generate a plot showing training and testing accuracy over time\n\nThe distributed version should achieve similar accuracy to the sequential version but with significantly reduced wall time when run across 8 GPUs.",
      "masked_source": [
        "/workspace/examples/few-shot/maml_omniglot.py",
        "/workspace/examples/distributed/few-shot/maml_omniglot.py"
      ]
    }
  ]
}