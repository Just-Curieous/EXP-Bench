{
  "requirements": [
    "Step 1: Set up the argument parser to handle configuration options for fine-tuning, including task name, model name, optimizer settings, and training parameters (/workspace/roberta-superglue/badam_ft.py:19-42)",
    "Step 2: Define the block structure for RoBERTa-large by creating a list of parameter prefixes that divide the model into transformer layers, pooler, and task-specific components (/workspace/roberta-superglue/badam_ft.py:16-17)",
    "Step 3: Create a unique run name based on the configuration settings for tracking experiments (/workspace/roberta-superglue/badam_ft.py:50-61)",
    "Step 4: Select the appropriate block prefix list based on the model type (base or large) (/workspace/roberta-superglue/badam_ft.py:64-69)",
    "Step 5: Initialize wandb for experiment tracking with the configuration parameters (/workspace/roberta-superglue/badam_ft.py:74-77)",
    "Step 6: Set up the run configuration with all necessary parameters for fine-tuning, including the block optimizer settings (/workspace/roberta-superglue/badam_ft.py:80-102)",
    "Step 7: Implement the BlockOptimizer class that wraps a base optimizer and manages which parameters are trainable at each step (/workspace/src/badam/block_optim.py:39-131)",
    "Step 8: Implement the switch_trainable_params method to update which block of parameters is trainable based on the current block index and switching strategy (/workspace/src/badam/block_optim.py:403-438)",
    "Step 9: Implement the step method to perform optimization on the currently active block and switch blocks when needed (/workspace/src/badam/block_optim.py:217-227)",
    "Step 10: Integrate the BlockOptimizer with the training loop to fine-tune the model on the SuperGLUE task (/workspace/roberta-superglue/jiant/proj/main/runscript.py:158-172)",
    "Final Step: Run the fine-tuning process and evaluate the model on the validation set, reporting performance metrics (/workspace/roberta-superglue/jiant/proj/simple/runscript.py:114-287)"
  ],
  "agent_instructions": "Your task is to implement a script that fine-tunes a RoBERTa-large model on a SuperGLUE benchmark task using the BAdam (Block Adam) optimizer to reduce memory usage. The BAdam optimizer enables full parameter optimization with reduced memory footprint by sequentially updating different blocks of the model.\n\nFollow these steps:\n\n1. Create a script that accepts command-line arguments for configuring the fine-tuning process, including:\n   - Task name (e.g., boolq, wic, wsc, rte, multirc, or copa)\n   - Model name (roberta-large)\n   - Training parameters (batch size, learning rate, number of epochs)\n   - Block optimizer settings (switch frequency, switching mode)\n\n2. Implement the BAdam optimizer that:\n   - Divides the RoBERTa-large model into blocks (typically by layers)\n   - For RoBERTa-large, create 26 blocks: 24 transformer layers, pooler layer, and task-specific layers\n   - Trains only one block at a time, switching between blocks periodically\n   - Supports different switching strategies (ascending, descending, random, fixed)\n   - Manages the optimizer state to only keep active parameters in memory\n\n3. Set up the fine-tuning pipeline that:\n   - Downloads and preprocesses the specified SuperGLUE task dataset\n   - Initializes the RoBERTa-large model\n   - Configures the BAdam optimizer with the base optimizer (e.g., Adam)\n   - Trains the model using the block-wise optimization strategy\n   - Evaluates the model periodically on the validation set\n   - Reports performance metrics for the task\n\n4. Implement experiment tracking to monitor:\n   - Training and validation metrics\n   - Memory usage comparison between standard optimization and block-wise optimization\n   - Performance across different block switching strategies\n\nThe final output should be a fine-tuned RoBERTa-large model on the chosen SuperGLUE task with evaluation metrics reported, demonstrating significantly lower memory usage compared to standard Adam optimization."
}