{
  "requirements": [
    "Step 1: Import necessary libraries including torch, transformers, and datasets (/workspace/llama-alpaca/src/train_bash.py:1-3)",
    "Step 2: Load and preprocess the Alpaca-GPT4 dataset, including tokenization with appropriate sequence lengths (/workspace/llama-alpaca/src/llmtuner/tuner/sft/workflow.py:30-36)",
    "Step 3: Load a pretrained Llama model (2-7B or 3-8B) and tokenizer (/workspace/llama-alpaca/src/llmtuner/tuner/sft/workflow.py:34-35)",
    "Step 4: Implement a BlockOptimizer class that wraps a base optimizer (Adam) to update parameters block by block (/workspace/llama-alpaca/src/llmtuner/tuner/sft/block_optim.py:60-138)",
    "Step 5: Implement the block switching mechanism that changes which block of parameters is being updated based on a specified frequency and mode (random, ascending, or descending) (/workspace/llama-alpaca/src/llmtuner/tuner/sft/block_optim.py:251-337)",
    "Step 6: Implement high-precision parameter handling to maintain float32 copies of parameters while using lower precision for the model (/workspace/llama-alpaca/src/llmtuner/tuner/sft/block_optim.py:236-249)",
    "Step 7: Create a custom trainer class that uses the BlockOptimizer and handles gradient checkpointing for memory efficiency (/workspace/llama-alpaca/src/llmtuner/tuner/sft/trainer.py:162-188)",
    "Step 8: Configure the training arguments including learning rate, batch size, and BAdam-specific parameters (/workspace/llama-alpaca/src/llmtuner/tuner/sft/workflow.py:42-48)",
    "Step 9: Execute the training process with the configured trainer and BAdam optimizer (/workspace/llama-alpaca/src/llmtuner/tuner/sft/workflow.py:69-77)",
    "Step 10: Save and evaluate the fine-tuned model (/workspace/llama-alpaca/src/llmtuner/tuner/sft/workflow.py:75-94)"
  ],
  "agent_instructions": "Your task is to implement a script for fine-tuning a large language model (Llama 2-7B or Llama 3-8B) on the Alpaca-GPT4 dataset using a memory-efficient Block-wise Adam (BAdam) optimizer. This approach enables full parameter fine-tuning on a single consumer GPU with limited memory.\n\nThe core components you need to implement are:\n\n1. A BlockOptimizer class that wraps a standard Adam optimizer to update parameters block by block (typically one transformer layer at a time), rather than all at once. This optimizer should:\n   - Divide model parameters into logical blocks (e.g., by transformer layers)\n   - Only update one block of parameters at a time while freezing others\n   - Switch between blocks periodically based on a configurable frequency\n   - Support different block switching strategies (random, ascending, descending)\n   - Maintain high-precision copies of parameters for optimization\n\n2. A training script that:\n   - Loads a pretrained Llama model (2-7B or 3-8B)\n   - Processes the Alpaca-GPT4 instruction-following dataset\n   - Configures the BAdam optimizer with appropriate hyperparameters\n   - Implements gradient checkpointing for additional memory efficiency\n   - Trains the model using the BAdam optimizer\n   - Saves the fine-tuned model for evaluation\n\nThe script should accept command-line arguments for configuring:\n- Model path (Llama 2-7B or Llama 3-8B)\n- Dataset path (Alpaca-GPT4)\n- Output directory\n- Training hyperparameters (learning rate, batch size, etc.)\n- BAdam-specific parameters (block switching frequency, switching mode)\n\nAfter training, the model should be ready for evaluation on benchmarks like MT-bench to measure performance improvements over baseline models."
}