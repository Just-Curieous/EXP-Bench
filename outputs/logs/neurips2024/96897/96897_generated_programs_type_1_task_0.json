{
    "source": [
        "/workspace/llama-alpaca/src/train_bash.py"
    ],
    "usage_instructions": "To compare the wall-clock running time for the backward pass between BAdam, LOMO, and LoRA when finetuning Llama 3-8B model, run the following commands:\n\n1. For BAdam:\n```bash\ncd /workspace/llama-alpaca\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --template default \\\n    --finetuning_type block \\\n    --output_dir ./outputs/llama3-8b-badam \\\n    --overwrite_cache \\\n    --per_device_train_batch_size 2 \\\n    --gradient_accumulation_steps 1 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 1 \\\n    --save_steps 1000 \\\n    --learning_rate 1e-6 \\\n    --num_train_epochs 3 \\\n    --overwrite_output_dir \\\n    --switch_block_every 100 \\\n    --switch_mode random \\\n    --bf16 True\n```\n\n2. For LoRA:\n```bash\ncd /workspace/llama-alpaca\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --template default \\\n    --finetuning_type lora \\\n    --output_dir ./outputs/llama3-8b-lora \\\n    --overwrite_cache \\\n    --per_device_train_batch_size 2 \\\n    --gradient_accumulation_steps 1 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 1 \\\n    --save_steps 1000 \\\n    --learning_rate 1e-6 \\\n    --num_train_epochs 3 \\\n    --overwrite_output_dir \\\n    --bf16 True\n```\n\nThe repository doesn't include a direct implementation of LOMO, but the BAdam and LoRA implementations have timing instrumentation built in. The trainer.py file includes code that measures and logs the time for forward pass, backward pass, and update phases separately. After running the experiments, you can compare the logged backward_time values from the training logs to see the difference in wall-clock time for the backward pass between BAdam and LoRA, which should show that BAdam's backward pass is nearly half as long as that of LoRA, as mentioned in the expected outcome."
}