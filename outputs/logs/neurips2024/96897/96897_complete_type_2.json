[
  {
    "mode": "A",
    "question": "How can we fine-tune a RoBERTa-large model on the SuperGLUE benchmark using BAdam optimizer to reduce memory usage?",
    "method": "Implement a script that fine-tunes RoBERTa-large on a SuperGLUE task using the BAdam optimizer, which enables full parameter optimization with reduced memory footprint by sequentially updating different blocks of the model.",
    "expected_outcome": "A fine-tuned RoBERTa-large model on a SuperGLUE task (e.g., BoolQ, WiC, WSC, RTE, MultiRC, or COPA) with evaluation metrics reported. The memory usage should be significantly lower than using standard Adam optimizer.",
    "source": [
      "/workspace/roberta-superglue/badam_ft.py"
    ],
    "usage_instructions": "1. First, download the SuperGLUE dataset for the chosen task (e.g., BoolQ) using the provided script.\n2. Set up the block partition for RoBERTa-large by defining the block_prefix_list that divides the model into transformer layers.\n3. Initialize the BlockOptimizer with the base optimizer, named parameters list, and block switching strategy.\n4. Configure the training arguments including task name, number of epochs, evaluation frequency, and batch size.\n5. Run the fine-tuning process with the BlockOptimizer, which will train different blocks of the model sequentially.\n6. Evaluate the model on the validation set and report the performance metrics.",
    "requirements": [
      "Step 1: Set up the argument parser to handle configuration options for fine-tuning, including task name, model name, optimizer settings, and training parameters (/workspace/roberta-superglue/badam_ft.py:19-42)",
      "Step 2: Define the block structure for RoBERTa-large by creating a list of parameter prefixes that divide the model into transformer layers, pooler, and task-specific components (/workspace/roberta-superglue/badam_ft.py:16-17)",
      "Step 3: Create a unique run name based on the configuration settings for tracking experiments (/workspace/roberta-superglue/badam_ft.py:50-61)",
      "Step 4: Select the appropriate block prefix list based on the model type (base or large) (/workspace/roberta-superglue/badam_ft.py:64-69)",
      "Step 5: Initialize wandb for experiment tracking with the configuration parameters (/workspace/roberta-superglue/badam_ft.py:74-77)",
      "Step 6: Set up the run configuration with all necessary parameters for fine-tuning, including the block optimizer settings (/workspace/roberta-superglue/badam_ft.py:80-102)",
      "Step 7: Implement the BlockOptimizer class that wraps a base optimizer and manages which parameters are trainable at each step (/workspace/src/badam/block_optim.py:39-131)",
      "Step 8: Implement the switch_trainable_params method to update which block of parameters is trainable based on the current block index and switching strategy (/workspace/src/badam/block_optim.py:403-438)",
      "Step 9: Implement the step method to perform optimization on the currently active block and switch blocks when needed (/workspace/src/badam/block_optim.py:217-227)",
      "Step 10: Integrate the BlockOptimizer with the training loop to fine-tune the model on the SuperGLUE task (/workspace/roberta-superglue/jiant/proj/main/runscript.py:158-172)",
      "Final Step: Run the fine-tuning process and evaluate the model on the validation set, reporting performance metrics (/workspace/roberta-superglue/jiant/proj/simple/runscript.py:114-287)"
    ],
    "agent_instructions": "Your task is to implement a script that fine-tunes a RoBERTa-large model on a SuperGLUE benchmark task using the BAdam (Block Adam) optimizer to reduce memory usage. The BAdam optimizer enables full parameter optimization with reduced memory footprint by sequentially updating different blocks of the model.\n\nFollow these steps:\n\n1. Create a script that accepts command-line arguments for configuring the fine-tuning process, including:\n   - Task name (e.g., boolq, wic, wsc, rte, multirc, or copa)\n   - Model name (roberta-large)\n   - Training parameters (batch size, learning rate, number of epochs)\n   - Block optimizer settings (switch frequency, switching mode)\n\n2. Implement the BAdam optimizer that:\n   - Divides the RoBERTa-large model into blocks (typically by layers)\n   - For RoBERTa-large, create 26 blocks: 24 transformer layers, pooler layer, and task-specific layers\n   - Trains only one block at a time, switching between blocks periodically\n   - Supports different switching strategies (ascending, descending, random, fixed)\n   - Manages the optimizer state to only keep active parameters in memory\n\n3. Set up the fine-tuning pipeline that:\n   - Downloads and preprocesses the specified SuperGLUE task dataset\n   - Initializes the RoBERTa-large model\n   - Configures the BAdam optimizer with the base optimizer (e.g., Adam)\n   - Trains the model using the block-wise optimization strategy\n   - Evaluates the model periodically on the validation set\n   - Reports performance metrics for the task\n\n4. Implement experiment tracking to monitor:\n   - Training and validation metrics\n   - Memory usage comparison between standard optimization and block-wise optimization\n   - Performance across different block switching strategies\n\nThe final output should be a fine-tuned RoBERTa-large model on the chosen SuperGLUE task with evaluation metrics reported, demonstrating significantly lower memory usage compared to standard Adam optimization."
  },
  {
    "mode": "A",
    "question": "How can we fine-tune a Llama 2-7B or Llama 3-8B model on the Alpaca-GPT4 dataset using BAdam optimizer with limited GPU memory?",
    "method": "Implement a script that fine-tunes a large language model (Llama 2-7B or Llama 3-8B) on the Alpaca-GPT4 dataset using the BAdam optimizer to enable full parameter fine-tuning on a single consumer GPU with limited memory.",
    "expected_outcome": "A fine-tuned Llama model that can be evaluated on benchmarks like MT-bench, showing improved performance over baseline models while using significantly less memory than standard Adam optimization.",
    "source": [
      "/workspace/llama-alpaca/src/train_bash.py",
      "/workspace/llama-alpaca/src/llmtuner/tuner/sft/trainer.py"
    ],
    "usage_instructions": "1. Set up the training environment with the required dependencies.\n2. Configure the training arguments including model path, dataset, output directory, batch size, and learning rate.\n3. Specify the fine-tuning type as 'block' to use the BAdam optimizer.\n4. Set the block switching frequency (switch_block_every) and switching mode (random, ascending, or descending).\n5. Run the training script which will automatically create block partitions based on the model's transformer layers.\n6. The BAdam optimizer will sequentially update different blocks of parameters, allowing full parameter fine-tuning with reduced memory usage.\n7. After training, evaluate the model on benchmarks like MT-bench to measure performance improvements.",
    "requirements": [
      "Step 1: Import necessary libraries including torch, transformers, and datasets (/workspace/llama-alpaca/src/train_bash.py:1-3)",
      "Step 2: Load and preprocess the Alpaca-GPT4 dataset, including tokenization with appropriate sequence lengths (/workspace/llama-alpaca/src/llmtuner/tuner/sft/workflow.py:30-36)",
      "Step 3: Load a pretrained Llama model (2-7B or 3-8B) and tokenizer (/workspace/llama-alpaca/src/llmtuner/tuner/sft/workflow.py:34-35)",
      "Step 4: Implement a BlockOptimizer class that wraps a base optimizer (Adam) to update parameters block by block (/workspace/llama-alpaca/src/llmtuner/tuner/sft/block_optim.py:60-138)",
      "Step 5: Implement the block switching mechanism that changes which block of parameters is being updated based on a specified frequency and mode (random, ascending, or descending) (/workspace/llama-alpaca/src/llmtuner/tuner/sft/block_optim.py:251-337)",
      "Step 6: Implement high-precision parameter handling to maintain float32 copies of parameters while using lower precision for the model (/workspace/llama-alpaca/src/llmtuner/tuner/sft/block_optim.py:236-249)",
      "Step 7: Create a custom trainer class that uses the BlockOptimizer and handles gradient checkpointing for memory efficiency (/workspace/llama-alpaca/src/llmtuner/tuner/sft/trainer.py:162-188)",
      "Step 8: Configure the training arguments including learning rate, batch size, and BAdam-specific parameters (/workspace/llama-alpaca/src/llmtuner/tuner/sft/workflow.py:42-48)",
      "Step 9: Execute the training process with the configured trainer and BAdam optimizer (/workspace/llama-alpaca/src/llmtuner/tuner/sft/workflow.py:69-77)",
      "Step 10: Save and evaluate the fine-tuned model (/workspace/llama-alpaca/src/llmtuner/tuner/sft/workflow.py:75-94)"
    ],
    "agent_instructions": "Your task is to implement a script for fine-tuning a large language model (Llama 2-7B or Llama 3-8B) on the Alpaca-GPT4 dataset using a memory-efficient Block-wise Adam (BAdam) optimizer. This approach enables full parameter fine-tuning on a single consumer GPU with limited memory.\n\nThe core components you need to implement are:\n\n1. A BlockOptimizer class that wraps a standard Adam optimizer to update parameters block by block (typically one transformer layer at a time), rather than all at once. This optimizer should:\n   - Divide model parameters into logical blocks (e.g., by transformer layers)\n   - Only update one block of parameters at a time while freezing others\n   - Switch between blocks periodically based on a configurable frequency\n   - Support different block switching strategies (random, ascending, descending)\n   - Maintain high-precision copies of parameters for optimization\n\n2. A training script that:\n   - Loads a pretrained Llama model (2-7B or 3-8B)\n   - Processes the Alpaca-GPT4 instruction-following dataset\n   - Configures the BAdam optimizer with appropriate hyperparameters\n   - Implements gradient checkpointing for additional memory efficiency\n   - Trains the model using the BAdam optimizer\n   - Saves the fine-tuned model for evaluation\n\nThe script should accept command-line arguments for configuring:\n- Model path (Llama 2-7B or Llama 3-8B)\n- Dataset path (Alpaca-GPT4)\n- Output directory\n- Training hyperparameters (learning rate, batch size, etc.)\n- BAdam-specific parameters (block switching frequency, switching mode)\n\nAfter training, the model should be ready for evaluation on benchmarks like MT-bench to measure performance improvements over baseline models."
  },
  {
    "mode": "B",
    "question": "How can we implement a simple demonstration of the BAdam optimizer to show its memory efficiency compared to standard Adam optimization?",
    "method": "Create a script that demonstrates the BAdam optimizer on a simple transformer model, comparing its memory usage and training performance against standard Adam optimization.",
    "expected_outcome": "A comparison report showing the memory usage difference between BAdam and standard Adam optimization, along with training loss curves demonstrating that BAdam can achieve similar optimization results with significantly lower memory requirements.",
    "source": [
      "/workspace/src/badam/block_optim.py",
      "/workspace/src/badam/__init__.py"
    ],
    "usage_instructions": "1. Import the necessary libraries including torch and the BAdam implementation.\n2. Define a simple transformer model with multiple layers for demonstration purposes.\n3. Create synthetic training data for the model.\n4. Implement a training loop function that can work with either standard Adam or BAdam optimizer.\n5. First train the model with standard Adam optimizer and record memory usage and loss values.\n6. Then train an identical model with BAdam optimizer, configuring it to update blocks of parameters sequentially.\n7. Compare and visualize the memory usage and training loss between the two approaches.\n8. Demonstrate how BAdam allows full parameter optimization with significantly reduced memory footprint by updating parameters block by block.",
    "requirements": [
      "Step 1: Import necessary libraries including torch, transformers, and memory tracking utilities (block_optim.py:1-14)",
      "Step 2: Define a BlockOptimizer class that wraps a base optimizer (like Adam) to update parameters block by block (block_optim.py:39-131)",
      "Step 3: Implement the switch_trainable_params method to toggle which parameters are trainable based on the current block (block_optim.py:403-437)",
      "Step 4: Implement platform-specific parameter switching logic for both single GPU and distributed training scenarios (block_optim.py:439-544)",
      "Step 5: Implement the step method to perform optimization on the current block and switch blocks periodically (block_optim.py:217-264)",
      "Step 6: Implement memory-efficient parameter handling by converting between low and high precision (block_optim.py:265-288)",
      "Step 7: Create a BAdamCallback class to handle integration with training frameworks like Hugging Face Transformers (utils.py:11-38)",
      "Step 8: Implement custom gradient checkpointing to work efficiently with block-wise optimization (utils.py:41-82)"
    ],
    "agent_instructions": "Create a demonstration script that compares the memory efficiency of BAdam (Block-wise Adam) optimizer against standard Adam optimizer. Your script should:\n\n1. Create a simple transformer model with multiple layers (you can use a small custom transformer or a pre-trained model from Hugging Face).\n\n2. Implement the BAdam optimizer that updates parameters block by block:\n   - Divide model parameters into logical blocks (e.g., by transformer layers)\n   - Only update one block of parameters at a time\n   - Switch between blocks periodically during training\n   - Use mixed precision (store parameters in fp16/bf16, compute in fp32)\n   - Reset optimizer state when switching blocks\n\n3. Create a training loop that:\n   - First trains with standard Adam optimizer and measures memory usage\n   - Then trains with BAdam optimizer and measures memory usage\n   - Uses the same model architecture, data, and hyperparameters for both\n\n4. Track and visualize:\n   - Peak memory usage for both optimizers\n   - Training loss curves to show that BAdam achieves similar optimization results\n\n5. Include comments explaining how BAdam achieves memory efficiency compared to standard Adam.\n\nThe key insight is that BAdam reduces memory usage by only keeping optimizer states for the current block of parameters and only computing gradients for the current block, while still eventually updating all parameters."
  }
]