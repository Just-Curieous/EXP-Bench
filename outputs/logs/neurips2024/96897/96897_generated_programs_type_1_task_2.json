{
    "source": ["/workspace/llama-alpaca/src/train_bash.py"],
    "usage_instructions": "To measure memory consumption when finetuning Llama 3-8B with different optimization methods, run the following commands:\n\n1. For BAdam (block-wise optimization):\n   ```bash\n   cd /workspace/llama-alpaca\n   CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n       --stage sft \\\n       --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n       --do_train \\\n       --dataset alpaca_gpt4_en \\\n       --template default \\\n       --finetuning_type block \\\n       --output_dir ./outputs/llama3-8b-badam \\\n       --overwrite_cache \\\n       --per_device_train_batch_size 2 \\\n       --per_device_eval_batch_size 2 \\\n       --gradient_accumulation_steps 8 \\\n       --lr_scheduler_type cosine \\\n       --logging_steps 1 \\\n       --save_steps 1000 \\\n       --learning_rate 1e-6 \\\n       --num_train_epochs 3 \\\n       --overwrite_output_dir \\\n       --plot_loss \\\n       --switch_block_every 100 \\\n       --switch_mode random \\\n       --bf16 True\n   ```\n\n2. For Adam (full parameter optimization):\n   ```bash\n   cd /workspace/llama-alpaca\n   CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n       --stage sft \\\n       --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n       --do_train \\\n       --dataset alpaca_gpt4_en \\\n       --template default \\\n       --finetuning_type full \\\n       --output_dir ./outputs/llama3-8b-adam \\\n       --overwrite_cache \\\n       --per_device_train_batch_size 2 \\\n       --per_device_eval_batch_size 2 \\\n       --gradient_accumulation_steps 8 \\\n       --lr_scheduler_type cosine \\\n       --logging_steps 1 \\\n       --save_steps 1000 \\\n       --learning_rate 1e-6 \\\n       --num_train_epochs 3 \\\n       --overwrite_output_dir \\\n       --plot_loss \\\n       --bf16 True\n   ```\n\n3. For LoRA (parameter-efficient fine-tuning):\n   ```bash\n   cd /workspace/llama-alpaca\n   CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n       --stage sft \\\n       --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n       --do_train \\\n       --dataset alpaca_gpt4_en \\\n       --template default \\\n       --finetuning_type lora \\\n       --output_dir ./outputs/llama3-8b-lora \\\n       --overwrite_cache \\\n       --per_device_train_batch_size 2 \\\n       --per_device_eval_batch_size 2 \\\n       --gradient_accumulation_steps 8 \\\n       --lr_scheduler_type cosine \\\n       --logging_steps 1 \\\n       --save_steps 1000 \\\n       --learning_rate 1e-6 \\\n       --num_train_epochs 3 \\\n       --overwrite_output_dir \\\n       --plot_loss \\\n       --bf16 True\n   ```\n\nTo monitor memory usage during training, you can use the following command in a separate terminal:\n```bash\nwatch -n 1 nvidia-smi\n```\n\nAccording to the repository's README.md, you should observe the following memory consumption patterns:\n- BAdam: ~23.5 GB for Llama 3-8B\n- Adam (full): ~144 GB+ for Llama 3-8B (will likely fail on a single RTX3090-24GB GPU)\n- LoRA: Less memory than full Adam but more than BAdam\n\nNote that LOMO is not directly implemented in this repository, so it cannot be compared using these scripts."
}