{
    "source": ["/workspace/llama-alpaca/src/train_bash.py"],
    "usage_instructions": "To test the impact of different ordering schemes (random reshuffling, ascending, or descending order) in BAdam on LLM fine-tuning convergence behavior, run the following commands for each ordering scheme:\n\n1. For random reshuffling order:\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path meta-llama/Llama-2-7b-hf \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --template default \\\n    --finetuning_type block \\\n    --output_dir ./outputs/llama2-7b-random \\\n    --overwrite_cache \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 8 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 1 \\\n    --save_steps 1000 \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 3.0 \\\n    --plot_loss \\\n    --fp16 \\\n    --switch_block_every 100 \\\n    --switch_mode random\n```\n\n2. For ascending order (from input layer to output layer):\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path meta-llama/Llama-2-7b-hf \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --template default \\\n    --finetuning_type block \\\n    --output_dir ./outputs/llama2-7b-ascending \\\n    --overwrite_cache \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 8 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 1 \\\n    --save_steps 1000 \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 3.0 \\\n    --plot_loss \\\n    --fp16 \\\n    --switch_block_every 100 \\\n    --switch_mode ascending\n```\n\n3. For descending order (from output layer to input layer):\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path meta-llama/Llama-2-7b-hf \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --template default \\\n    --finetuning_type block \\\n    --output_dir ./outputs/llama2-7b-descending \\\n    --overwrite_cache \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 8 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 1 \\\n    --save_steps 1000 \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 3.0 \\\n    --plot_loss \\\n    --fp16 \\\n    --switch_block_every 100 \\\n    --switch_mode descending\n```\n\nAfter running these commands, compare the training loss curves and final performance metrics across the three ordering schemes. The repository's implementation allows for easy switching between these ordering schemes using the `--switch_mode` parameter, which can be set to 'random', 'ascending', or 'descending'."
}