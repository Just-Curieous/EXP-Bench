{
    "source": ["/workspace/llama-alpaca/src/train_bash.py"],
    "usage_instructions": "To compare BAdam with other methods (LoRA, Galore, Adam) on MT-bench scores when finetuning Llama 2-7B and Llama 3-8B models under different learning rate settings:\n\n1. First, finetune Llama 2-7B with BAdam using learning rate 1e-5:\n   ```bash\n   cd /workspace/llama-alpaca\n   CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n       --stage sft \\\n       --model_name_or_path meta-llama/Llama-2-7b-hf \\\n       --do_train \\\n       --dataset alpaca_gpt4_en \\\n       --template default \\\n       --finetuning_type block \\\n       --output_dir ./outputs/llama2-7b-badam-1e-5 \\\n       --overwrite_cache \\\n       --per_device_train_batch_size 2 \\\n       --per_device_eval_batch_size 2 \\\n       --gradient_accumulation_steps 8 \\\n       --lr_scheduler_type cosine \\\n       --learning_rate 1e-5 \\\n       --num_train_epochs 3 \\\n       --overwrite_output_dir \\\n       --switch_block_every 100 \\\n       --switch_mode random \\\n       --bf16 True\n   ```\n\n2. Then finetune Llama 2-7B with BAdam using learning rate 1e-6:\n   ```bash\n   CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n       --stage sft \\\n       --model_name_or_path meta-llama/Llama-2-7b-hf \\\n       --do_train \\\n       --dataset alpaca_gpt4_en \\\n       --template default \\\n       --finetuning_type block \\\n       --output_dir ./outputs/llama2-7b-badam-1e-6 \\\n       --overwrite_cache \\\n       --per_device_train_batch_size 2 \\\n       --per_device_eval_batch_size 2 \\\n       --gradient_accumulation_steps 8 \\\n       --lr_scheduler_type cosine \\\n       --learning_rate 1e-6 \\\n       --num_train_epochs 3 \\\n       --overwrite_output_dir \\\n       --switch_block_every 100 \\\n       --switch_mode random \\\n       --bf16 True\n   ```\n\n3. Repeat the same for Llama 3-8B with BAdam using learning rate 1e-5:\n   ```bash\n   CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n       --stage sft \\\n       --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n       --do_train \\\n       --dataset alpaca_gpt4_en \\\n       --template default \\\n       --finetuning_type block \\\n       --output_dir ./outputs/llama3-8b-badam-1e-5 \\\n       --overwrite_cache \\\n       --per_device_train_batch_size 2 \\\n       --per_device_eval_batch_size 2 \\\n       --gradient_accumulation_steps 8 \\\n       --lr_scheduler_type cosine \\\n       --learning_rate 1e-5 \\\n       --num_train_epochs 3 \\\n       --overwrite_output_dir \\\n       --switch_block_every 100 \\\n       --switch_mode random \\\n       --bf16 True\n   ```\n\n4. And Llama 3-8B with BAdam using learning rate 1e-6:\n   ```bash\n   CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n       --stage sft \\\n       --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n       --do_train \\\n       --dataset alpaca_gpt4_en \\\n       --template default \\\n       --finetuning_type block \\\n       --output_dir ./outputs/llama3-8b-badam-1e-6 \\\n       --overwrite_cache \\\n       --per_device_train_batch_size 2 \\\n       --per_device_eval_batch_size 2 \\\n       --gradient_accumulation_steps 8 \\\n       --lr_scheduler_type cosine \\\n       --learning_rate 1e-6 \\\n       --num_train_epochs 3 \\\n       --overwrite_output_dir \\\n       --switch_block_every 100 \\\n       --switch_mode random \\\n       --bf16 True\n   ```\n\n5. For comparison with other methods, repeat the above commands but change the `--finetuning_type` parameter:\n   - For Adam: `--finetuning_type full`\n   - For LoRA: `--finetuning_type lora`\n   - For Galore: `--finetuning_type sparse`\n\n6. After training, you'll need to evaluate the models using an external MT-bench evaluation tool, as this repository doesn't include a specific script for MT-bench evaluation. The README indicates that BAdam consistently outperforms LoRA in MT-bench scores, but the evaluation process itself is not included in this repository."
}