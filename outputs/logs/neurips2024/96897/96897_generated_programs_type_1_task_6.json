{
    "source": [
        "/workspace/llama-alpaca/src/train_bash.py",
        "/workspace/llama-alpaca/src/llmtuner/tuner/sft/block_optim.py"
    ],
    "usage_instructions": "To compare BCD variants (BAdam and BSGD) with their full counterparts (Adam and SGD) on MT-bench performance:\n\n1. First, train the BAdam variant by running:\n```bash\ncd /workspace/llama-alpaca\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --template default \\\n    --finetuning_type block \\\n    --output_dir ./outputs/llama3-8b-badam \\\n    --overwrite_cache \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 8 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 1 \\\n    --save_steps 1000 \\\n    --val_size 500 \\\n    --eval_steps 20 \\\n    --evaluation_strategy steps \\\n    --learning_rate 1e-6 \\\n    --num_train_epochs 3 \\\n    --overwrite_output_dir \\\n    --plot_loss \\\n    --switch_block_every 100 \\\n    --switch_mode random \\\n    --bf16 True\n```\n\n2. Then, train the BSGD variant by running the same command but changing the optimizer type:\n```bash\ncd /workspace/llama-alpaca\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --template default \\\n    --finetuning_type block \\\n    --output_dir ./outputs/llama3-8b-bsgd \\\n    --overwrite_cache \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 8 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 1 \\\n    --save_steps 1000 \\\n    --val_size 500 \\\n    --eval_steps 20 \\\n    --evaluation_strategy steps \\\n    --learning_rate 1e-6 \\\n    --num_train_epochs 3 \\\n    --overwrite_output_dir \\\n    --plot_loss \\\n    --switch_block_every 100 \\\n    --switch_mode random \\\n    --bf16 True \\\n    --optimizer_type sgd\n```\n\n3. Train the full Adam variant by running:\n```bash\ncd /workspace/llama-alpaca\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --template default \\\n    --finetuning_type full \\\n    --output_dir ./outputs/llama3-8b-adam \\\n    --overwrite_cache \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 8 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 1 \\\n    --save_steps 1000 \\\n    --val_size 500 \\\n    --eval_steps 20 \\\n    --evaluation_strategy steps \\\n    --learning_rate 1e-6 \\\n    --num_train_epochs 3 \\\n    --overwrite_output_dir \\\n    --plot_loss \\\n    --bf16 True\n```\n\n4. Train the full SGD variant by running:\n```bash\ncd /workspace/llama-alpaca\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --template default \\\n    --finetuning_type full \\\n    --output_dir ./outputs/llama3-8b-sgd \\\n    --overwrite_cache \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 8 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 1 \\\n    --save_steps 1000 \\\n    --val_size 500 \\\n    --eval_steps 20 \\\n    --evaluation_strategy steps \\\n    --learning_rate 1e-6 \\\n    --num_train_epochs 3 \\\n    --overwrite_output_dir \\\n    --plot_loss \\\n    --bf16 True \\\n    --optimizer_type sgd\n```\n\n5. After training, evaluate each model on MT-bench using an external MT-bench evaluation tool (not included in this repository). The repository mentions in the README that they achieved MT-bench scores of 6.67 for BAdam and 5.21 for BSGD, which were higher than their full counterparts, indicating that BCD variants better preserve general knowledge."
}