{
    "source": [
        "/workspace/llama-alpaca/src/train_bash.py"
    ],
    "usage_instructions": "To compare BAdam and LoRA with an initial learning rate of 1e-5 during finetuning of Llama 3-8B on Alpaca-GPT4 dataset, run the following two commands:\n\n1. First, run BAdam finetuning:\n```bash\nCUDA_VISIBLE_DEVICES=0 python /workspace/llama-alpaca/src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --template default \\\n    --finetuning_type block \\\n    --output_dir ./outputs/llama3-8b-badam \\\n    --overwrite_cache \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 8 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 1 \\\n    --save_steps 1000 \\\n    --val_size 500 \\\n    --eval_steps 20 \\\n    --evaluation_strategy steps \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 3 \\\n    --overwrite_output_dir \\\n    --plot_loss \\\n    --switch_block_every 100 \\\n    --switch_mode random \\\n    --bf16 True\n```\n\n2. Then, run LoRA finetuning with the same learning rate:\n```bash\nCUDA_VISIBLE_DEVICES=0 python /workspace/llama-alpaca/src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --template default \\\n    --finetuning_type lora \\\n    --output_dir ./outputs/llama3-8b-lora \\\n    --overwrite_cache \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 8 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 1 \\\n    --save_steps 1000 \\\n    --val_size 500 \\\n    --eval_steps 20 \\\n    --evaluation_strategy steps \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 3 \\\n    --overwrite_output_dir \\\n    --plot_loss \\\n    --bf16 True\n```\n\nAfter running both experiments, you can compare the training loss curves by examining the generated plots in the respective output directories:\n- BAdam loss curve: `./outputs/llama3-8b-badam/training_loss.png`\n- LoRA loss curve: `./outputs/llama3-8b-lora/training_loss.png`\n\nBased on the paper's findings (Figure 2), BAdam should demonstrate superior convergence with a lower training loss compared to LoRA when using an initial learning rate of 1e-5."
}