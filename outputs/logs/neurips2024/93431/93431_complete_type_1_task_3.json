{
  "questions": [
    {
      "question": "- **Can Trace optimize both prompts and functions in an LLM agent workflow more effectively than DSPy\u2019s COPRO, leading to better output correctness and formatting, on the Big-Bench Hard dataset, with 15 examples in the training set, 5 examples in the validation set, and the remaining in the test set?**",
      "method": "1. **Task Setup:**  \n   - **Goal:** Optimize a DSPy-based **LLM agent** by tuning both **prompts and functions**.  \n   - **Optimized Components:**  \n     - **`prompt_template`**: The **meta-prompt** guiding LLM responses.  \n     - **`create_prompt`**: A function that **modifies the prompt** based on the current query.  \n     - **`extract_answer`**: A function that **post-processes** the LLM output into a structured format.  \n   - **Pipeline:** The experiment evaluates an **end-to-end prompt-and-code optimization system**, where the LLM agent must generate both **correct answers and properly formatted responses**.  \n\n2. **Dataset:**  \n   - **Big-Bench Hard dataset** [21].  \n   - **Training set:** 15 examples.  \n   - **Validation set:** 5 examples.  \n   - **Test set:** Remaining data from the dataset.  \n\n3. **Comparative Evaluation:**  \n   - **Trace**: Optimizes the **full agent workflow** (prompt + functions).  \n   - **DSPy\u2019s COPRO**: Optimizes **only the meta-prompt**.  \n\n4. **Evaluation Metric:**  \n   - An **automatic evaluation function** compares LLM-generated outputs with the ground truth.  \n   - The outputs must be **both correct in content and follow a specified format**.",
      "expected_outcome": "- **Trace should achieve superior optimization** by jointly tuning prompts and functions, surpassing DSPy\u2019s COPRO.  \n- **DSPy\u2019s COPRO should be limited in its improvements** since it only optimizes the meta-prompt and not the full agent pipeline.",
      "source": [
        "/workspace/examples/bbh/run_prompt_bigbench_trace.py",
        "/workspace/examples/bbh/run_prompt_bigbench_dspy.py"
      ],
      "usage_instructions": "1. First, ensure you have the required dependencies installed (autogen, datasets, etc.). 2. Run the Trace optimization script with: `python /workspace/examples/bbh/run_prompt_bigbench_trace.py --task <task_name> --train`. 3. Run the DSPy COPRO optimization script with: `python /workspace/examples/bbh/run_prompt_bigbench_dspy.py --task <task_name> --copro`. 4. Compare the results from both approaches to see which one achieves better performance. The scripts will output accuracy metrics and save detailed results to the specified save paths.",
      "requirements": [
        "Step 1: Import necessary libraries including autogen, dspy, datasets, and optimization frameworks (opto.trace for Trace optimization and dspy.teleprompt for DSPy optimization) (/workspace/examples/bbh/run_prompt_bigbench_trace.py:1-17, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:1-11)",
        "Step 2: Define evaluation metric function that compares model predictions with ground truth answers, handling different answer formats (multiple choice or direct answers) (/workspace/examples/bbh/run_prompt_bigbench_trace.py:20-31, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:13-26)",
        "Step 3: For Trace optimization: Implement LLMCallable base class that handles LLM API calls with appropriate prompting and response handling (/workspace/examples/bbh/run_prompt_bigbench_trace.py:34-64)",
        "Step 4: For Trace optimization: Implement Predict model class that defines the standard prediction flow with customizable prompt templates and answer extraction logic (/workspace/examples/bbh/run_prompt_bigbench_trace.py:67-132)",
        "Step 5: For Trace optimization: Implement PredictCoT model class that extends the standard prediction with chain-of-thought reasoning (/workspace/examples/bbh/run_prompt_bigbench_trace.py:135-198)",
        "Step 6: For DSPy optimization: Implement BasicQA and CoT model classes using DSPy's module system (/workspace/examples/bbh/run_prompt_bigbench_dspy.py:29-44)",
        "Step 7: Implement evaluation function to assess model performance on test examples (/workspace/examples/bbh/run_prompt_bigbench_trace.py:287-300, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:46-58)",
        "Step 8: For Trace optimization: Implement learning function that optimizes the model using training examples and validates on validation examples (/workspace/examples/bbh/run_prompt_bigbench_trace.py:201-284)",
        "Step 9: Set up command-line argument parsing to control task selection, optimization methods, and other parameters (/workspace/examples/bbh/run_prompt_bigbench_trace.py:303-314, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:61-72)",
        "Step 10: Define the list of available BigBench Hard tasks (/workspace/examples/bbh/run_prompt_bigbench_trace.py:321-328, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:79-86)",
        "Step 11: Load dataset for the selected task from 'maveriq/bigbenchhard' (/workspace/examples/bbh/run_prompt_bigbench_trace.py:348, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:111-112)",
        "Step 12: Split dataset into training, validation, and test sets (/workspace/examples/bbh/run_prompt_bigbench_trace.py:352-354, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:115-116)",
        "Step 13: For Trace optimization: Initialize model (Predict or PredictCoT) and optimizer (OptoPrime), then train the model on training examples (/workspace/examples/bbh/run_prompt_bigbench_trace.py:358-369)",
        "Step 14: For DSPy optimization: Initialize model (BasicQA or CoT) and optimizer (BootstrapFewShotWithRandomSearch or COPRO), then compile the optimized model (/workspace/examples/bbh/run_prompt_bigbench_dspy.py:120-139)",
        "Step 15: Evaluate the optimized model on test examples and calculate accuracy (/workspace/examples/bbh/run_prompt_bigbench_trace.py:379-381, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:146-149)",
        "Step 16: Save results including accuracy, model responses, and optimized prompts to a pickle file (/workspace/examples/bbh/run_prompt_bigbench_trace.py:383-386, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:161-173)"
      ],
      "agent_instructions": "Create two scripts for optimizing prompts on BigBench Hard (BBH) tasks using different optimization frameworks: Trace and DSPy. The scripts should allow comparing the performance of these frameworks on various reasoning tasks.\n\nFor the Trace optimization script:\n1. Implement a system that can optimize prompts and answer extraction logic using the Trace framework\n2. Create two model variants: a standard prediction model and a chain-of-thought (CoT) model\n3. Include functionality to train the model on examples, save checkpoints, and evaluate performance\n4. Support command-line arguments for task selection, model type, and other parameters\n\nFor the DSPy optimization script:\n1. Implement a system that can optimize prompts using DSPy's optimization techniques (particularly COPRO)\n2. Create two model variants: a basic QA model and a chain-of-thought model\n3. Include functionality to evaluate model performance on test examples\n4. Support similar command-line arguments as the Trace script for consistency\n\nBoth scripts should:\n- Load datasets from the 'maveriq/bigbenchhard' collection\n- Support various BBH reasoning tasks\n- Implement the same evaluation metric for fair comparison\n- Save results in a structured format for later analysis\n- Allow for different optimization strategies (standard, few-shot, chain-of-thought)\n\nThe goal is to compare the effectiveness of Trace and DSPy optimization frameworks on improving language model performance on complex reasoning tasks.",
      "masked_source": [
        "/workspace/examples/bbh/run_prompt_bigbench_trace.py",
        "/workspace/examples/bbh/run_prompt_bigbench_dspy.py"
      ]
    }
  ]
}