{
  "questions": [
    {
      "question": "- **Can Trace optimize both prompts and functions in an LLM agent workflow more effectively than DSPy\u2019s COPRO, leading to better output correctness and formatting, on the Big-Bench Hard dataset, with 15 examples in the training set, 5 examples in the validation set, and the remaining in the test set? Analyze the results only for the NLP tasks within this dataset (there should be 12 tasks)**",
      "method": "1. **Task Setup:**  \n   - **Goal:** Optimize a DSPy-based **LLM agent** by tuning both **prompts and functions**.  \n   - **Optimized Components:**  \n     - **`prompt_template`**: The **meta-prompt** guiding LLM responses.  \n     - **`create_prompt`**: A function that **modifies the prompt** based on the current query.  \n     - **`extract_answer`**: A function that **post-processes** the LLM output into a structured format.  \n   - **Pipeline:** The experiment evaluates an **end-to-end prompt-and-code optimization system**, where the LLM agent must generate both **correct answers and properly formatted responses**.  \n\n2. **Dataset:**  \n   - **Big-Bench Hard dataset** [21].  \n   - **Training set:** 15 examples.  \n   - **Validation set:** 5 examples.  \n   - **Test set:** Remaining data from the dataset.  \n\n3. **Comparative Evaluation:**  \n   - **Trace**: Optimizes the **full agent workflow** (prompt + functions).  \n   - **DSPy\u2019s COPRO**: Optimizes **only the meta-prompt**.  \n\n4. **Evaluation Metric:**  \n   - An **automatic evaluation function** compares LLM-generated outputs with the ground truth.  \n   - The outputs must be **both correct in content and follow a specified format**.",
      "expected_outcome": "- **Trace should achieve superior optimization** by jointly tuning prompts and functions, surpassing DSPy\u2019s COPRO.  \n- **DSPy\u2019s COPRO should be limited in its improvements** since it only optimizes the meta-prompt and not the full agent pipeline.",
      "source": [
        "/workspace/examples/bbh/run_prompt_bigbench_trace.py",
        "/workspace/examples/bbh/run_prompt_bigbench_dspy.py"
      ],
      "usage_instructions": "1. First, run the DSPy COPRO optimization script with: `python /workspace/examples/bbh/run_prompt_bigbench_dspy.py --copro --task_start 0 --task_end 12 --save_path results/bigbench_dspy` to optimize only the meta-prompt on the 12 NLP tasks from the Big-Bench Hard dataset.\n2. Then, run the Trace optimization script with: `python /workspace/examples/bbh/run_prompt_bigbench_trace.py --task_start 0 --task_end 12 --save_path results/bigbench_trace` to optimize both prompts and functions on the same 12 NLP tasks.\n3. Both scripts use 15 examples for training, 5 examples for validation, and the remaining examples for testing, as specified in the experiment question. The results will be saved in the respective directories for comparison.",
      "requirements": [
        "Step 1: Load the Big-Bench Hard dataset for the specified NLP tasks (/workspace/examples/bbh/run_prompt_bigbench_trace.py:348-349, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:111-112)",
        "Step 2: Split the dataset into training (first 15 examples), validation (next 5 examples), and testing (remaining examples) sets (/workspace/examples/bbh/run_prompt_bigbench_trace.py:352-354, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:115-116, 134-135)",
        "Step 3: Define evaluation metrics that handle different answer formats (multiple choice, numerical, string) (/workspace/examples/bbh/run_prompt_bigbench_trace.py:20-31, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:13-26)",
        "Step 4: For DSPy implementation: Create basic QA and Chain-of-Thought modules that take questions and produce answers (/workspace/examples/bbh/run_prompt_bigbench_dspy.py:29-44)",
        "Step 5: For DSPy implementation: Set up COPRO optimization to improve the meta-prompt template using training examples (/workspace/examples/bbh/run_prompt_bigbench_dspy.py:136-139)",
        "Step 6: For Trace implementation: Define model classes with trainable prompt templates and functions for creating prompts and extracting answers (/workspace/examples/bbh/run_prompt_bigbench_trace.py:67-198)",
        "Step 7: For Trace implementation: Implement optimization loop that iteratively improves both prompt templates and functions based on feedback (/workspace/examples/bbh/run_prompt_bigbench_trace.py:201-284)",
        "Step 8: Evaluate model performance on test/validation sets using the defined evaluation metric (/workspace/examples/bbh/run_prompt_bigbench_trace.py:287-300, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:46-58, 141)",
        "Step 9: Save optimization results, including accuracy metrics and optimized prompts/functions (/workspace/examples/bbh/run_prompt_bigbench_trace.py:373-386, /workspace/examples/bbh/run_prompt_bigbench_dspy.py:151-173)"
      ],
      "agent_instructions": "Create two scripts for optimizing language model performance on NLP tasks from the Big-Bench Hard dataset:\n\n1. First script: Implement a DSPy-based solution that uses COPRO to optimize meta-prompts for question-answering tasks.\n   - Load the Big-Bench Hard dataset for specified tasks\n   - Create basic QA and Chain-of-Thought modules\n   - Implement COPRO optimization to improve prompt templates\n   - Use 15 examples for training, 5 for validation\n   - Evaluate performance and save results\n\n2. Second script: Implement a Trace-based solution that optimizes both prompts and functions.\n   - Load the same Big-Bench Hard dataset\n   - Define model classes with trainable prompt templates\n   - Create functions for prompt generation and answer extraction\n   - Implement an optimization loop that improves both prompts and functions based on feedback\n   - Use 15 examples for training, 5 for validation, remaining for testing\n   - Evaluate performance and save results\n\nBoth scripts should:\n- Support multiple NLP tasks from the Big-Bench Hard dataset\n- Include evaluation metrics that handle different answer formats (multiple choice, numerical, string)\n- Allow for command-line arguments to specify tasks, optimization methods, and save paths\n- Save optimization results including accuracy metrics",
      "masked_source": [
        "/workspace/examples/bbh/run_prompt_bigbench_trace.py",
        "/workspace/examples/bbh/run_prompt_bigbench_dspy.py"
      ]
    }
  ]
}