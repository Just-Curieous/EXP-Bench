{
  "requirements": [
    "Step 1: Set up the environment by importing necessary libraries and loading environment variables (/workspace/examples/textgrad_examples/evals/textgrad_solution_optimization.py:3-17, /workspace/examples/textgrad_examples/evals/textgrad_prompt_optimization.py:3-18)",
    "Step 2: Define utility functions for setting random seeds and parsing command line arguments (/workspace/examples/textgrad_examples/evals/textgrad_solution_optimization.py:27-41, /workspace/examples/textgrad_examples/evals/textgrad_prompt_optimization.py:19-35)",
    "Step 3: For solution optimization: Implement a majority voting ensemble method to combine multiple predictions (/workspace/examples/textgrad_examples/evals/textgrad_solution_optimization.py:44-57)",
    "Step 4: For solution optimization: Implement a function to get zero-shot answers from an LLM without optimization (/workspace/examples/textgrad_examples/evals/textgrad_solution_optimization.py:60-71)",
    "Step 5: For solution optimization: Implement the original TextGrad test-time training function that optimizes LLM responses through iterative feedback (/workspace/examples/textgrad_examples/evals/textgrad_solution_optimization.py:74-120)",
    "Step 6: For solution optimization: Implement the Trace-based test-time training function that uses OptoPrime or TextGrad optimizers (/workspace/examples/textgrad_examples/evals/textgrad_solution_optimization.py:142-210)",
    "Step 7: For solution optimization: Set up concurrent execution to process multiple test samples in parallel (/workspace/examples/textgrad_examples/evals/textgrad_solution_optimization.py:231-249)",
    "Step 8: For solution optimization: Calculate and save performance metrics and timing information (/workspace/examples/textgrad_examples/evals/textgrad_solution_optimization.py:251-274)",
    "Step 9: For prompt optimization: Implement functions to evaluate individual samples and entire datasets (/workspace/examples/textgrad_examples/evals/textgrad_prompt_optimization.py:39-72)",
    "Step 10: For prompt optimization: Implement validation logic to revert to previous prompts if performance decreases (/workspace/examples/textgrad_examples/evals/textgrad_prompt_optimization.py:74-87)",
    "Step 11: For prompt optimization: Set up the optimization environment with appropriate models and optimizers (/workspace/examples/textgrad_examples/evals/textgrad_prompt_optimization.py:89-122)",
    "Step 12: For prompt optimization: Define Trace operations by wrapping TextGrad code in trace bundles (/workspace/examples/textgrad_examples/evals/textgrad_prompt_optimization.py:132-153)",
    "Step 13: For prompt optimization: Implement the training loop that optimizes system prompts through iterative feedback (/workspace/examples/textgrad_examples/evals/textgrad_prompt_optimization.py:156-209)",
    "Final Step: For prompt optimization: Calculate and save final performance metrics and timing information (/workspace/examples/textgrad_examples/evals/textgrad_prompt_optimization.py:212-220)"
  ],
  "agent_instructions": "Create two scripts to compare the performance and speed of TextGrad and OptoPrime optimization algorithms for large language models:\n\n1. Solution Optimization Script:\n   - Create a script that optimizes LLM responses to multiple-choice questions at test time\n   - Implement two approaches: the original TextGrad implementation and the OptoPrime implementation\n   - The script should:\n     - Load a task (default: MMLU_machine_learning)\n     - Get initial zero-shot responses from an LLM\n     - Apply iterative optimization to improve responses\n     - Use majority voting to ensemble multiple predictions\n     - Run optimization in parallel using multiple threads\n     - Track and report performance metrics and timing information\n     - Save results to JSON files in a 'textgrad_figures' directory\n   - Support command-line arguments to select the algorithm, task, engine, etc.\n\n2. Prompt Optimization Script:\n   - Create a script that optimizes system prompts for improved LLM performance\n   - Implement TextGrad and OptoPrime approaches for prompt optimization\n   - The script should:\n     - Load a task (default: BBH_object_counting)\n     - Train a system prompt over multiple epochs\n     - Evaluate performance on validation and test sets\n     - Optionally revert to previous prompts if validation performance decreases\n     - Track and report performance metrics and timing information\n     - Save results to JSON files in a 'textgrad_figures' directory\n   - Support command-line arguments to select the algorithm, task, engines, etc.\n\nBoth scripts should be designed to demonstrate that OptoPrime is approximately 3x faster than TextGrad while achieving similar performance. The scripts should use the TextGrad and Opto libraries.",
  "masked_source": [
    "/workspace/examples/textgrad_examples/evals/textgrad_solution_optimization.py",
    "/workspace/examples/textgrad_examples/evals/textgrad_prompt_optimization.py"
  ]
}