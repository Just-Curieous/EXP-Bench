{
    "source": [
        "/workspace/train.sh",
        "/workspace/opencls/training/zero_shot.py"
    ],
    "usage_instructions": "To compare word-level and subword-level tokenizers for classification performance across different model sizes, follow these steps:\n\n1. First, train the models with word-level tokenizer (SimpleTokenizer) using:\n   ```\n   bash train.sh opencls/configs/cls_schedule/cls_vit_s16_s512m_bs16k.yaml opencls\n   bash train.sh opencls/configs/cls_schedule/cls_vit_b16_s512m_bs16k.yaml opencls\n   bash train.sh opencls/configs/cls_schedule/cls_vit_l16_s512m_bs16k.yaml opencls\n   ```\n\n2. Then, train the models with subword-level tokenizer by modifying the model configurations to use HFTokenizer. Create copies of the config files and add `\"hf_tokenizer_name\": \"bert-base-uncased\"` to the `text_cfg` section in each config file. Then run:\n   ```\n   bash train.sh opencls/configs/cls_schedule/cls_vit_s16_subword_s512m_bs16k.yaml opencls\n   bash train.sh opencls/configs/cls_schedule/cls_vit_b16_subword_s512m_bs16k.yaml opencls\n   bash train.sh opencls/configs/cls_schedule/cls_vit_l16_subword_s512m_bs16k.yaml opencls\n   ```\n\n3. The zero-shot evaluation on ImageNet-1K will be performed automatically during training if you set the appropriate zeroshot_frequency in the config files. The results will show that for ViT-S/16, the word-level tokenizer achieves better classification accuracy, while as model size increases to ViT-B/16 and ViT-L/16, the subword-level tokenizer gradually outperforms the word-level tokenizer, matching the expected outcome."
}