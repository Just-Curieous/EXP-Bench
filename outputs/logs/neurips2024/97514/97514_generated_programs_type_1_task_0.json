{
    "source": [
        "/workspace/src/hest/bench/benchmark.py",
        "/workspace/bench_config/bench_config.yaml"
    ],
    "usage_instructions": "To compare histology-only models with multimodal models that integrate spatial transcriptomics data, follow these steps:\n\n1. First, ensure HEST is properly installed following the README instructions.\n\n2. Request access to the CONCH model weights from Hugging Face (https://huggingface.co/MahmoodLab/CONCH) and install the CONCH package with `pip install git+https://github.com/Mahmoodlab/CONCH.git`.\n\n3. Edit the bench_config.yaml file to include both histology-only and multimodal models:\n   - For histology-only: uncomment 'resnet50' (already included by default)\n   - For multimodal: uncomment 'conch_v1' (which uses contrastive learning with spatial transcriptomics data)\n\n4. Run the benchmark script with the following command:\n   ```\n   python src/hest/bench/benchmark.py --config bench_config/bench_config.yaml\n   ```\n\n5. The script will automatically download the benchmark data, run both models, and output results showing Pearson correlation coefficients between predicted and ground truth biomarker expression. The results will be saved in the directory specified by 'results_dir' in the config file.\n\n6. Compare the Pearson correlation values between the histology-only model (resnet50) and the multimodal model (conch_v1) to determine if integrating spatial transcriptomics data improves correlation with ground truth biomarker expression.\n\nNote: According to the benchmark results in the README, the multimodal CONCH model achieves a higher average Pearson correlation (R=0.3709) compared to the histology-only ResNet50 model (R=0.326), confirming that multimodal fine-tuning does improve correlation with ground truth biomarker expression."
}