{
    "source": ["/workspace/tools/test.py"],
    "usage_instructions": "To evaluate whether Voxel Mamba offers a favorable trade-off between detection accuracy, inference speed, and GPU memory usage compared to other architectures, run the test.py script with the --infer_time flag for each model configuration. For example:\n\n1. For Voxel Mamba:\n   python test.py --cfg_file ./cfgs/voxel_mamba_models/voxel_mamba_waymo.yaml --batch_size 1 --ckpt /path/to/checkpoint --infer_time\n\n2. For DSVT:\n   python test.py --cfg_file ./cfgs/waymo_models/dsvt_voxel.yaml --batch_size 1 --ckpt /path/to/checkpoint --infer_time\n\n3. For PV-RCNN++:\n   python test.py --cfg_file ./cfgs/waymo_models/pv_rcnn_plusplus.yaml --batch_size 1 --ckpt /path/to/checkpoint --infer_time\n\n4. For CenterPoint-Pillar:\n   python test.py --cfg_file ./cfgs/waymo_models/centerpoint_pillar_1x.yaml --batch_size 1 --ckpt /path/to/checkpoint --infer_time\n\n5. For PointPillar:\n   python test.py --cfg_file ./cfgs/waymo_models/pointpillar_1x.yaml --batch_size 1 --ckpt /path/to/checkpoint --infer_time\n\nThe test.py script will output detection accuracy metrics (mAP@L2) and inference time (FPS). To measure GPU memory usage, you can use nvidia-smi in a separate terminal while running the tests. For example:\n   watch -n 0.1 'nvidia-smi --query-gpu=memory.used --format=csv'\n\nCompare the results across all models to determine if Voxel Mamba offers a favorable trade-off between detection accuracy, inference speed, and GPU memory usage."
}