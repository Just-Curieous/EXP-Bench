{
  "requirements": [
    "Step 1: Parse command-line arguments including model configuration file, checkpoint path, batch size, and inference time flag (/workspace/tools/test.py:21-55)",
    "Step 2: Set up distributed testing environment if specified (/workspace/tools/test.py:144-154)",
    "Step 3: Configure batch size based on available GPUs (/workspace/tools/test.py:156-160)",
    "Step 4: Create output directories for evaluation results (/workspace/tools/test.py:162-177)",
    "Step 5: Set up logging for the evaluation process (/workspace/tools/test.py:178-190)",
    "Step 6: Build the test dataloader with the specified configuration (/workspace/tools/test.py:194-199)",
    "Step 7: Build the 3D object detection network model based on configuration (/workspace/tools/test.py:201)",
    "Step 8: Load model weights from the specified checkpoint (/workspace/tools/test.py:58-62)",
    "Step 9: Set model to evaluation mode (/workspace/tools/test.py:53)",
    "Step 10: Process each batch from the dataloader, measuring inference time if requested (/workspace/tools/eval_utils/eval_utils.py:58-74)",
    "Step 11: Generate predictions for each batch and collect detection results (/workspace/tools/eval_utils/eval_utils.py:65-80)",
    "Step 12: Calculate and log performance metrics including recall values (/workspace/tools/eval_utils/eval_utils.py:107-114)",
    "Step 13: Evaluate detection results using dataset-specific evaluation metrics (e.g., mAP@L2) (/workspace/tools/eval_utils/eval_utils.py:125-129)",
    "Step 14: Log and return evaluation results including accuracy metrics and inference time (/workspace/tools/eval_utils/eval_utils.py:131-136)"
  ],
  "agent_instructions": "Create a script to evaluate 3D object detection models on point cloud data and compare their performance metrics. The script should:\n\n1. Accept command-line arguments for:\n   - Configuration file path (--cfg_file)\n   - Checkpoint path (--ckpt)\n   - Batch size (--batch_size)\n   - Flag to measure inference time (--infer_time)\n\n2. Load the specified model configuration from a YAML file\n\n3. Set up the evaluation environment:\n   - Configure GPU and distributed testing if needed\n   - Create output directories for results\n   - Set up logging\n\n4. Build a dataloader for the test dataset based on the configuration\n\n5. Build the 3D object detection network model according to the configuration\n\n6. Load the model weights from the specified checkpoint\n\n7. Implement an evaluation loop that:\n   - Processes batches from the test dataset\n   - Measures inference time when the --infer_time flag is set\n   - Generates predictions for each sample\n   - Collects detection results\n\n8. Calculate and report performance metrics:\n   - Detection accuracy (mAP@L2)\n   - Inference speed (FPS or ms per inference)\n   - Recall values at different thresholds\n\n9. Save evaluation results to files\n\nThe script should support evaluating different 3D detection architectures (like Voxel Mamba, DSVT, PV-RCNN++, CenterPoint-Pillar, PointPillar) by loading their respective configuration files.",
  "masked_source": [
    "/workspace/tools/test.py"
  ]
}