{
  "requirements": [
    "Step 1: Import necessary modules from benchmarl including Experiment, ExperimentConfig, MappoConfig, VmasTask, and MlpConfig (/workspace/examples/running/run_experiment.py:7-10)",
    "Step 2: Load the base experiment configuration using ExperimentConfig.get_from_yaml() (/workspace/examples/running/run_experiment.py:15)",
    "Step 3: Load the VMAS/balance task configuration using VmasTask.BALANCE.get_from_yaml() (/workspace/examples/running/run_experiment.py:18)",
    "Step 4: Load the MAPPO algorithm configuration using MappoConfig.get_from_yaml() (/workspace/examples/running/run_experiment.py:21)",
    "Step 5: Load MLP model configurations for both actor and critic using MlpConfig.get_from_yaml() (/workspace/examples/running/run_experiment.py:24-25)",
    "Step 6: Create an Experiment object with the task, algorithm configuration, model configurations, seed, and experiment configuration (/workspace/examples/running/run_experiment.py:27-34)",
    "Step 7: Run the experiment using experiment.run() (/workspace/examples/running/run_experiment.py:35)",
    "Final Step: Analyze the training progress and final performance, expecting a mean return of approximately -6.5 after 12,000 frames of training (/workspace/examples/running/run_experiment.py:35)"
  ],
  "agent_instructions": "Your task is to run a single experiment with the MAPPO (Multi-Agent Proximal Policy Optimization) algorithm on the VMAS/balance task and analyze its performance. Follow these steps:\n\n1. Import the necessary modules from the benchmarl library, including Experiment, ExperimentConfig, MappoConfig, VmasTask, and MlpConfig.\n\n2. Set up the experiment by loading configurations:\n   - Load the base experiment configuration\n   - Load the VMAS/balance task configuration\n   - Load the MAPPO algorithm configuration\n   - Load MLP model configurations for both actor and critic networks\n\n3. Configure the experiment with the following parameters:\n   - Set max_n_frames to 12,000 (to limit training time)\n   - Disable loggers that might slow down the experiment\n   - Disable rendering during training\n\n4. Create an Experiment object with:\n   - The VMAS/balance task\n   - The MAPPO algorithm configuration\n   - The MLP model configurations for actor and critic\n   - A fixed seed (e.g., 0) for reproducibility\n   - The experiment configuration\n\n5. Run the experiment and analyze the training progress and final performance.\n\n6. The experiment should show the agents learning to balance an object, with the mean return improving over time. By the end of training (12,000 frames), you should expect to see a mean return of approximately -6.5.\n\nNote: The VMAS/balance task involves multiple agents (typically 4) trying to balance a package with a specific mass. The agents need to coordinate their movements to keep the package balanced."
}