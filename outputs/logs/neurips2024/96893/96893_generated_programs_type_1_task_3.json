{
    "source": ["/workspace/train.py", "/workspace/inference_demo.py"],
    "usage_instructions": "To conduct the ablation study on scaling up training dataset size with the BTCV dataset, follow these steps:\n\n1. First, modify the dataset_codes parameter in train.py to include different numbers of datasets:\n   - For one dataset: Change line 21 to `parser.add_argument(\"--dataset_codes\", type = list, default=['0004'])` (assuming BTCV has code 0004)\n   - For two datasets: Use default `parser.add_argument(\"--dataset_codes\", type = list, default=['0004', '0005'])`\n   - For eight datasets: Change to `parser.add_argument(\"--dataset_codes\", type = list, default=['0004', '0005', '0006', '0007', '0008', '0009', '0010', '0011'])`\n   - For all 25 datasets: Include all dataset codes from 0001 to 0025\n\n2. For each configuration, run the training script with 500 epochs (default setting in line 44):\n   ```\n   export SEGVOL_CKPT=\"path/to/SegVol_v1.pth\"\n   export WORK_DIR=\"./work_dir_[dataset_size]\"\n   export DATA_DIR=\"path/to/dataset_post\"\n   CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python train.py --resume $SEGVOL_CKPT -work_dir $WORK_DIR --data_dir $DATA_DIR\n   ```\n\n3. After training each model, evaluate it on the BTCV test split using inference_demo.py:\n   ```\n   export segvol_ckpt=\"./work_dir_[dataset_size]/[checkpoint_file].pth\"\n   export work_dir=\"./results_[dataset_size]\"\n   export demo_config_path=\"./config/config_btcv.json\"\n   CUDA_VISIBLE_DEVICES=0 python inference_demo.py --resume $segvol_ckpt -work_dir $work_dir --demo_config $demo_config_path\n   ```\n   Note: You'll need to create a config_btcv.json file similar to config_demo.json but with BTCV dataset paths and organ categories.\n\n4. Make sure to set args.use_text_prompt = True in inference_demo.py (line 209) to assess the text prompt setting as specified in the experiment.\n\n5. Compare the Dice scores reported in the output of each inference run to observe how performance scales with increasing training data size."
}