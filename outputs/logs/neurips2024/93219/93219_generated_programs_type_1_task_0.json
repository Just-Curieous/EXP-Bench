{
    "source": [
        "/workspace/train.py",
        "/workspace/app.py"
    ],
    "usage_instructions": "To evaluate the impact of explicit motion information on video anomaly description generation and question-answering on the Hawk dataset:\n\n1. First, train the full model with motion components using:\n   ```\n   NCCL_P2P_DISABLE=1 CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 --master_port='12001' train.py --cfg-path ./configs/train_configs/stage2_finetune.yaml\n   ```\n   This trains the complete model with all motion components (fm, Pm, and motion input Xm).\n\n2. Then, create a modified configuration file by copying stage2_finetune.yaml to stage2_finetune_no_motion.yaml and modify it to disable motion components by setting the motion-related parameters to False or removing them.\n\n3. Train the ablated model without motion components using:\n   ```\n   NCCL_P2P_DISABLE=1 CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 --master_port='12001' train.py --cfg-path ./configs/train_configs/stage2_finetune_no_motion.yaml\n   ```\n\n4. For evaluation on both tasks (anomaly video description generation and question-answering), use app.py with the appropriate model checkpoints:\n   ```\n   # For full model with motion\n   python app.py --cfg-path configs/eval_configs/eval.yaml --model_type llama_v2 --gpu-id 0\n   \n   # For ablated model without motion\n   python app.py --cfg-path configs/eval_configs/eval_no_motion.yaml --model_type llama_v2 --gpu-id 0\n   ```\n\n5. Compare the performance of both models using the Text-Level metrics (BLEU-1, BLEU-2, BLEU-3, BLEU-4) and GPT-guided metrics (Reasonability, Detail, Consistency) as specified in the experiment description."
}