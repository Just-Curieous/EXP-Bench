{
    "source": [
        "/workspace/scripts/CycleNet/Linear-Input-96/etth1.sh",
        "/workspace/scripts/CycleNet/Linear-Input-96/etth2.sh",
        "/workspace/scripts/CycleNet/MLP-Input-96/etth1.sh",
        "/workspace/scripts/CycleNet/MLP-Input-96/etth2.sh",
        "/workspace/run.py"
    ],
    "usage_instructions": "1. First, create two new scripts for iTransformer on ETTh1 and ETTh2 datasets:\n\nFor ETTh1, create a file named `/workspace/scripts/iTransformer/etth1.sh` with the following content:\n```bash\nmodel_name=Transformer\n\nroot_path_name=./dataset/\ndata_path_name=ETTh1.csv\nmodel_id_name=ETTh1\ndata_name=ETTh1\n\nseq_len=96\nfor pred_len in 96 192 336 720\ndo\nfor random_seed in 2024 2025 2026 2027 2028\ndo\n    python -u run.py \\\n      --is_training 1 \\\n      --root_path $root_path_name \\\n      --data_path $data_path_name \\\n      --model_id $model_id_name'_'$seq_len'_'$pred_len \\\n      --model $model_name \\\n      --data $data_name \\\n      --features M \\\n      --seq_len $seq_len \\\n      --pred_len $pred_len \\\n      --enc_in 7 \\\n      --d_model 512 \\\n      --d_ff 512 \\\n      --dropout 0.1 \\\n      --e_layers 3 \\\n      --train_epochs 30 \\\n      --patience 5 \\\n      --itr 1 --batch_size 256 --learning_rate 0.0001 --random_seed $random_seed\ndone\ndone\n```\n\nFor ETTh2, create a file named `/workspace/scripts/iTransformer/etth2.sh` with the following content:\n```bash\nmodel_name=Transformer\n\nroot_path_name=./dataset/\ndata_path_name=ETTh2.csv\nmodel_id_name=ETTh2\ndata_name=ETTh2\n\nseq_len=96\nfor pred_len in 96 192 336 720\ndo\nfor random_seed in 2024 2025 2026 2027 2028\ndo\n    python -u run.py \\\n      --is_training 1 \\\n      --root_path $root_path_name \\\n      --data_path $data_path_name \\\n      --model_id $model_id_name'_'$seq_len'_'$pred_len \\\n      --model $model_name \\\n      --data $data_name \\\n      --features M \\\n      --seq_len $seq_len \\\n      --pred_len $pred_len \\\n      --enc_in 7 \\\n      --d_model 512 \\\n      --d_ff 512 \\\n      --dropout 0.1 \\\n      --e_layers 3 \\\n      --train_epochs 30 \\\n      --patience 5 \\\n      --itr 1 --batch_size 256 --learning_rate 0.0001 --random_seed $random_seed\ndone\ndone\n```\n\n2. Then, run the following commands to execute the experiments:\n\n   a. Run CycleNet with Linear backbone on ETTh1 and ETTh2:\n      ```\n      sh scripts/CycleNet/Linear-Input-96/etth1.sh\n      sh scripts/CycleNet/Linear-Input-96/etth2.sh\n      ```\n\n   b. Run CycleNet with MLP backbone on ETTh1 and ETTh2:\n      ```\n      sh scripts/CycleNet/MLP-Input-96/etth1.sh\n      sh scripts/CycleNet/MLP-Input-96/etth2.sh\n      ```\n\n   c. Run iTransformer on ETTh1 and ETTh2:\n      ```\n      sh scripts/iTransformer/etth1.sh\n      sh scripts/iTransformer/etth2.sh\n      ```\n\n3. The scripts will run experiments with different forecast horizons (96, 192, 336, 720) and multiple random seeds for statistical significance. The results will include MSE and MAE metrics for each model, allowing you to compare the performance of CycleNet (with both Linear and MLP backbones) against iTransformer on ETTh1 and ETTh2 datasets."
}