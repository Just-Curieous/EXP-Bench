{
    "source": [
        "/workspace/the_well/benchmark/train.py"
    ],
    "usage_instructions": "To compare the one-step prediction advantage of CNextU-net versus its performance over longer prediction horizons compared to other architectures (including the original U-net), run the following commands:\n\n1. First, run the one-step prediction evaluation for CNextU-net on the turbulent_radiative_layer_2D dataset:\n   ```\n   cd /workspace/the_well/benchmark\n   python train.py experiment=unet_convnext data=turbulent_radiative_layer_2D server=local\n   ```\n\n2. Then, run the one-step prediction evaluation for the original U-net on the same dataset:\n   ```\n   python train.py experiment=unet_classic data=turbulent_radiative_layer_2D server=local\n   ```\n\n3. Optionally, run evaluations for other baseline architectures (e.g., FNO, AFNO, etc.) using similar commands:\n   ```\n   python train.py experiment=fno data=turbulent_radiative_layer_2D server=local\n   ```\n\nThe training script automatically performs both one-step and long-horizon autoregressive predictions through its validation loop. The `rollout_model` method in the Trainer class performs autoregressive predictions for multiple timesteps, and the metrics (including VRMSE) are computed for both one-step and multi-step horizons. The results are logged to wandb and can be visualized to compare the performance discrepancies between one-step and multi-step predictions across different models.\n\nNote: You may need to adjust the `well_base_path` in the data configuration file (/workspace/the_well/benchmark/configs/data/turbulent_radiative_layer_2D.yaml) to point to the correct location of the dataset. The default trainer configuration already sets `max_rollout_steps` to 100, which should be sufficient for long-horizon evaluation."
}