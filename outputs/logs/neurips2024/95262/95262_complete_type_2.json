[
  {
    "mode": "B",
    "question": "How can you use MoE Jetpack to perform image classification on a custom image using a pre-trained model?",
    "method": "Create a script that uses the MoE Jetpack framework to classify an image using a pre-trained model. The script should load a model, process an input image, and output the classification results.",
    "expected_outcome": "A JSON output containing the classification results for the input image, including the predicted class and confidence score.",
    "source": [
      "/workspace/demo/image_demo.py"
    ],
    "usage_instructions": "1. Import necessary libraries including mmengine.fileio and rich for output formatting\n2. Define a main function that parses command-line arguments for image path, model name, and optional checkpoint path\n3. Create an ImageClassificationInferencer with the specified model and checkpoint\n4. Run inference on the input image\n5. Format and display the classification results\n6. Handle any errors that might occur during model loading or inference",
    "requirements": [
      "Step 1: Import necessary libraries including ArgumentParser, mmengine.fileio.dump, rich.print_json, and mmpretrain.apis.ImageClassificationInferencer (/workspace/demo/image_demo.py:2-7)",
      "Step 2: Define a main function that parses command-line arguments for image path, model name, and optional parameters like checkpoint path (/workspace/demo/image_demo.py:10-24)",
      "Step 3: Create an ImageClassificationInferencer with the specified model and checkpoint, handling potential errors if the model is unavailable (/workspace/demo/image_demo.py:27-36)",
      "Step 4: Run inference on the input image using the created inferencer (/workspace/demo/image_demo.py:37)",
      "Step 5: Process the inference results by removing verbose prediction scores (/workspace/demo/image_demo.py:39)",
      "Final Step: Format and display the classification results as JSON (/workspace/demo/image_demo.py:40)"
    ],
    "agent_instructions": "Create a Python script that uses the MoE Jetpack framework to perform image classification on a custom image using a pre-trained model. Your script should:\n\n1. Accept command-line arguments for:\n   - Input image path\n   - Model name or configuration\n   - Optional checkpoint path\n   - Optional display settings\n\n2. Use the mmpretrain library's ImageClassificationInferencer to load the specified model and perform inference on the input image.\n\n3. Handle potential errors gracefully, especially if the requested model is unavailable.\n\n4. Process the inference results to make them more readable (e.g., removing overly verbose data).\n\n5. Output the classification results in a well-formatted JSON structure that includes the predicted class and confidence information.\n\nThe script should be user-friendly and provide clear error messages if something goes wrong during model loading or inference."
  },
  {
    "mode": "A",
    "question": "How can you convert a pre-trained dense Vision Transformer (ViT) model into a Mixture of Experts (MoE) model using MoE Jetpack's checkpoint recycling technique?",
    "method": "Use the MoE Jetpack framework to convert a pre-trained dense ViT model into a Mixture of Experts model by initializing MoE weights from the dense model using the checkpoint recycling technique.",
    "expected_outcome": "A saved MoE model checkpoint file that has been initialized from a dense model using the checkpoint recycling technique.",
    "source": [
      "/workspace/moejet/tools/gen_ViT_MoE_weight.py"
    ],
    "usage_instructions": "1. Set up the configuration for the MoE model, including the number of experts, MoE layers, and other hyperparameters\n2. Load the pre-trained dense model (e.g., ViT-Tiny or ViT-Small)\n3. Prepare a dataset for feature analysis (optional, depending on the selection method)\n4. Analyze the features of the dense model to determine which weights to select for each expert\n5. Generate expert indices based on the feature analysis or using random/uniform selection\n6. Extract and organize the selected weights from the dense model\n7. Initialize the MoE model with the selected weights\n8. Save the initialized MoE model checkpoint",
    "requirements": [
      "Step 1: Set up the environment by importing necessary libraries (PyTorch, timm, etc.) and configuring paths for data, models, and saving checkpoints (/workspace/moejet/tools/gen_ViT_MoE_weight.py:1-44)",
      "Step 2: Prepare a dataset for feature analysis by loading a subset of ImageNet and creating a data loader (/workspace/moejet/tools/gen_ViT_MoE_weight.py:46-58)",
      "Step 3: Implement feature analysis functions to collect activation statistics from the dense model, including L1/L2 norms and activation patterns (/workspace/moejet/tools/gen_ViT_MoE_weight.py:61-162)",
      "Step 4: Implement graph partitioning methods (spectral clustering, METIS, Louvain) to analyze co-activation patterns (/workspace/moejet/tools/gen_ViT_MoE_weight.py:165-209)",
      "Step 5: Implement different weight selection strategies (random, uniform, feature-based, graph-based) to determine which weights from the dense model should be assigned to each expert (/workspace/moejet/tools/gen_ViT_MoE_weight.py:212-354)",
      "Step 6: Generate expert indices based on the chosen selection strategy (/workspace/moejet/tools/gen_ViT_MoE_weight.py:357-407)",
      "Step 7: Extract and organize the selected weights from the dense model, reshaping them appropriately for the MoE architecture (/workspace/moejet/tools/gen_ViT_MoE_weight.py:427-709)",
      "Step 8: Combine and stitch together the weights from different sources to create the complete MoE model (/workspace/moejet/tools/gen_ViT_MoE_weight.py:763-803)",
      "Step 9: Save the initialized MoE model checkpoint to disk (/workspace/moejet/tools/gen_ViT_MoE_weight.py:954-957)",
      "Final Step: Execute the main function that orchestrates the entire process of converting a dense ViT model to an MoE model (/workspace/moejet/tools/gen_ViT_MoE_weight.py:860-959)"
    ],
    "agent_instructions": "Your task is to create a script that converts a pre-trained dense Vision Transformer (ViT) model into a Mixture of Experts (MoE) model using the checkpoint recycling technique from the MoE Jetpack framework. This technique initializes MoE weights from a dense model to improve training efficiency.\n\nThe script should:\n\n1. Load a pre-trained dense ViT model (such as ViT-Tiny or ViT-Small) using a library like timm or a similar framework.\n\n2. Configure the MoE model parameters, including:\n   - Number of experts\n   - Which transformer layers to convert to MoE layers\n   - Whether to convert entire transformer blocks or just MLP layers\n   - Expert selection strategy\n\n3. Implement multiple strategies for selecting which weights from the dense model should be assigned to each expert:\n   - Simple strategies: random selection, uniform selection\n   - Feature-based strategies: selection based on L1/L2 norms or activation patterns\n   - Graph-based strategies: selection based on co-activation patterns\n\n4. For feature-based and graph-based strategies, implement a feature analysis process that:\n   - Loads a small dataset (e.g., subset of ImageNet)\n   - Passes data through the dense model\n   - Collects activation statistics at specified layers\n   - Uses these statistics to guide expert weight selection\n\n5. Extract and organize the selected weights from the dense model, reshaping them appropriately for the MoE architecture.\n\n6. Support dual-path MoE models with both core experts and universal experts if needed.\n\n7. Save the initialized MoE model checkpoint to disk.\n\nThe goal is to create a working implementation that can take any pre-trained dense ViT model and convert it into an MoE model with initialized weights, ready for further fine-tuning or evaluation."
  },
  {
    "mode": "A",
    "question": "How can you fine-tune a MoE Jetpack model on the ImageNet dataset using the provided configuration?",
    "method": "Use the MoE Jetpack framework to fine-tune a pre-initialized MoE model on the ImageNet dataset using the provided configuration file.",
    "expected_outcome": "A trained MoE model with improved accuracy compared to the dense baseline model.",
    "source": [
      "/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py",
      "/workspace/tools/dist_train.sh"
    ],
    "usage_instructions": "1. Ensure the MoE model has been initialized using checkpoint recycling\n2. Set up the training configuration, including batch size, learning rate, and other hyperparameters\n3. Configure the optimizer with layer-wise learning rate decay\n4. Set up the learning rate scheduler with warm-up and cosine annealing\n5. Run the distributed training script with the appropriate configuration file\n6. Monitor the training progress and validation accuracy\n7. Save and evaluate the fine-tuned model",
    "requirements": [
      "Step 1: Import the MoE Jetpack module to access the MoE model architecture and components (/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py:6-8)",
      "Step 2: Configure the ImageNet dataset with appropriate data preprocessing, augmentation, and loading parameters (/workspace/moejet/configs/_base_/datasets/imagenet_bs64_swin_224.py:1-86)",
      "Step 3: Set up the MoE model configuration with dual-path architecture, specifying core and universal experts (/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py:22-36, 38-77)",
      "Step 4: Load the pre-initialized MoE model weights from checkpoint recycling (/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py:35)",
      "Step 5: Configure the optimizer with AdamW and layer-wise learning rate decay for different model components (/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py:92-142)",
      "Step 6: Set up the learning rate scheduler with warm-up phase and cosine annealing (/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py:144-159)",
      "Step 7: Configure training parameters including epochs, validation interval, and checkpoint saving (/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py:168-182)",
      "Step 8: Launch distributed training using the PyTorch distributed module with the specified configuration (/workspace/tools/dist_train.sh:1-19)"
    ],
    "agent_instructions": "Your task is to fine-tune a Mixture of Experts (MoE) model on the ImageNet dataset using the MoE Jetpack framework. This involves setting up a configuration for a Vision Transformer (ViT) model with a dual-path MoE architecture and training it in a distributed environment.\n\nFollow these steps:\n\n1. Create a configuration file that:\n   - Imports the MoE Jetpack module\n   - Sets up the ImageNet dataset with appropriate data preprocessing and augmentations\n   - Configures a ViT model with dual-path MoE architecture, specifying parameters like number of experts, slots per expert, and which transformer layers should use MoE\n   - Loads pre-initialized model weights from checkpoint recycling\n   - Configures an AdamW optimizer with layer-wise learning rate decay\n   - Sets up a learning rate scheduler with warm-up phase and cosine annealing\n   - Defines training parameters (epochs, batch size, validation frequency)\n\n2. Create a distributed training script that:\n   - Takes the configuration file as input\n   - Launches the training process using PyTorch's distributed training capabilities\n   - Handles multi-GPU training\n\n3. The MoE model should have these key characteristics:\n   - Dual-path architecture with core experts and universal experts\n   - Mixture of Experts applied to specific transformer layers\n   - Layer-wise learning rate decay for different model components\n\nThe goal is to fine-tune the MoE model on ImageNet to achieve better accuracy than a comparable dense baseline model."
  }
]