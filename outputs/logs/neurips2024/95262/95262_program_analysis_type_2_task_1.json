{
  "requirements": [
    "Step 1: Set up the environment by importing necessary libraries (PyTorch, timm, etc.) and configuring paths for data, models, and saving checkpoints (/workspace/moejet/tools/gen_ViT_MoE_weight.py:1-44)",
    "Step 2: Prepare a dataset for feature analysis by loading a subset of ImageNet and creating a data loader (/workspace/moejet/tools/gen_ViT_MoE_weight.py:46-58)",
    "Step 3: Implement feature analysis functions to collect activation statistics from the dense model, including L1/L2 norms and activation patterns (/workspace/moejet/tools/gen_ViT_MoE_weight.py:61-162)",
    "Step 4: Implement graph partitioning methods (spectral clustering, METIS, Louvain) to analyze co-activation patterns (/workspace/moejet/tools/gen_ViT_MoE_weight.py:165-209)",
    "Step 5: Implement different weight selection strategies (random, uniform, feature-based, graph-based) to determine which weights from the dense model should be assigned to each expert (/workspace/moejet/tools/gen_ViT_MoE_weight.py:212-354)",
    "Step 6: Generate expert indices based on the chosen selection strategy (/workspace/moejet/tools/gen_ViT_MoE_weight.py:357-407)",
    "Step 7: Extract and organize the selected weights from the dense model, reshaping them appropriately for the MoE architecture (/workspace/moejet/tools/gen_ViT_MoE_weight.py:427-709)",
    "Step 8: Combine and stitch together the weights from different sources to create the complete MoE model (/workspace/moejet/tools/gen_ViT_MoE_weight.py:763-803)",
    "Step 9: Save the initialized MoE model checkpoint to disk (/workspace/moejet/tools/gen_ViT_MoE_weight.py:954-957)",
    "Final Step: Execute the main function that orchestrates the entire process of converting a dense ViT model to an MoE model (/workspace/moejet/tools/gen_ViT_MoE_weight.py:860-959)"
  ],
  "agent_instructions": "Your task is to create a script that converts a pre-trained dense Vision Transformer (ViT) model into a Mixture of Experts (MoE) model using the checkpoint recycling technique from the MoE Jetpack framework. This technique initializes MoE weights from a dense model to improve training efficiency.\n\nThe script should:\n\n1. Load a pre-trained dense ViT model (such as ViT-Tiny or ViT-Small) using a library like timm or a similar framework.\n\n2. Configure the MoE model parameters, including:\n   - Number of experts\n   - Which transformer layers to convert to MoE layers\n   - Whether to convert entire transformer blocks or just MLP layers\n   - Expert selection strategy\n\n3. Implement multiple strategies for selecting which weights from the dense model should be assigned to each expert:\n   - Simple strategies: random selection, uniform selection\n   - Feature-based strategies: selection based on L1/L2 norms or activation patterns\n   - Graph-based strategies: selection based on co-activation patterns\n\n4. For feature-based and graph-based strategies, implement a feature analysis process that:\n   - Loads a small dataset (e.g., subset of ImageNet)\n   - Passes data through the dense model\n   - Collects activation statistics at specified layers\n   - Uses these statistics to guide expert weight selection\n\n5. Extract and organize the selected weights from the dense model, reshaping them appropriately for the MoE architecture.\n\n6. Support dual-path MoE models with both core experts and universal experts if needed.\n\n7. Save the initialized MoE model checkpoint to disk.\n\nThe goal is to create a working implementation that can take any pre-trained dense ViT model and convert it into an MoE model with initialized weights, ready for further fine-tuning or evaluation."
}