{
    "source": [
        "/workspace/moejet/tools/gen_ViT_MoE_weight.py",
        "/workspace/tools/dist_train.sh",
        "/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py",
        "/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_scratch.py"
    ],
    "usage_instructions": "To evaluate the convergence speed of MoE Jetpack compared to a baseline model trained from scratch:\n\n1. First, initialize the MoE weights using checkpoint recycling:\n   ```bash\n   python moejet/tools/gen_ViT_MoE_weight.py\n   ```\n   This script transforms dense checkpoints into MoE model weights with SpheroMoE layers.\n\n2. Train the MoE Jetpack model (with checkpoint recycling) on ImageNet-1K:\n   ```bash\n   CUDA_VISIBLE_DEVICES=0,1,2,3 PORT=29500 ./tools/dist_train.sh moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py 4\n   ```\n\n3. Train the baseline MoE model from scratch on ImageNet-1K:\n   ```bash\n   CUDA_VISIBLE_DEVICES=0,1,2,3 PORT=29500 ./tools/dist_train.sh moejet/configs/timm/vit_tiny_dual_moe_timm_scratch.py 4\n   ```\n\n4. For CIFAR-100 experiments, modify the dataset configuration in both config files by changing the base dataset from:\n   ```python\n   _base_ = [\n       '../_base_/datasets/imagenet_bs64_swin_224.py',\n       '../_base_/default_runtime.py'\n   ]\n   ```\n   to:\n   ```python\n   _base_ = [\n       '../_base_/datasets/cifar100_bs16.py',\n       '../_base_/default_runtime.py'\n   ]\n   ```\n   And update the hyperparameters in the config files to match those specified in the experiment (batch size=512, warmup epochs=50, training epochs=300, drop path rate=0.1).\n\n5. Monitor the training logs to determine the number of epochs required for each model to reach the target accuracies (77% for ImageNet-1K, 76% for CIFAR-100).\n\nThe experiment will demonstrate that the MoE Jetpack-enabled model requires roughly half the number of epochs on ImageNet-1K and one-eighth the number of epochs on CIFAR-100 compared to the baseline model trained from scratch."
}