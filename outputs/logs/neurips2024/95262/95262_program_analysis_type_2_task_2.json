{
  "requirements": [
    "Step 1: Import the MoE Jetpack module to access the MoE model architecture and components (/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py:6-8)",
    "Step 2: Configure the ImageNet dataset with appropriate data preprocessing, augmentation, and loading parameters (/workspace/moejet/configs/_base_/datasets/imagenet_bs64_swin_224.py:1-86)",
    "Step 3: Set up the MoE model configuration with dual-path architecture, specifying core and universal experts (/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py:22-36, 38-77)",
    "Step 4: Load the pre-initialized MoE model weights from checkpoint recycling (/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py:35)",
    "Step 5: Configure the optimizer with AdamW and layer-wise learning rate decay for different model components (/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py:92-142)",
    "Step 6: Set up the learning rate scheduler with warm-up phase and cosine annealing (/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py:144-159)",
    "Step 7: Configure training parameters including epochs, validation interval, and checkpoint saving (/workspace/moejet/configs/timm/vit_tiny_dual_moe_timm_21k_ft.py:168-182)",
    "Step 8: Launch distributed training using the PyTorch distributed module with the specified configuration (/workspace/tools/dist_train.sh:1-19)"
  ],
  "agent_instructions": "Your task is to fine-tune a Mixture of Experts (MoE) model on the ImageNet dataset using the MoE Jetpack framework. This involves setting up a configuration for a Vision Transformer (ViT) model with a dual-path MoE architecture and training it in a distributed environment.\n\nFollow these steps:\n\n1. Create a configuration file that:\n   - Imports the MoE Jetpack module\n   - Sets up the ImageNet dataset with appropriate data preprocessing and augmentations\n   - Configures a ViT model with dual-path MoE architecture, specifying parameters like number of experts, slots per expert, and which transformer layers should use MoE\n   - Loads pre-initialized model weights from checkpoint recycling\n   - Configures an AdamW optimizer with layer-wise learning rate decay\n   - Sets up a learning rate scheduler with warm-up phase and cosine annealing\n   - Defines training parameters (epochs, batch size, validation frequency)\n\n2. Create a distributed training script that:\n   - Takes the configuration file as input\n   - Launches the training process using PyTorch's distributed training capabilities\n   - Handles multi-GPU training\n\n3. The MoE model should have these key characteristics:\n   - Dual-path architecture with core experts and universal experts\n   - Mixture of Experts applied to specific transformer layers\n   - Layer-wise learning rate decay for different model components\n\nThe goal is to fine-tune the MoE model on ImageNet to achieve better accuracy than a comparable dense baseline model."
}