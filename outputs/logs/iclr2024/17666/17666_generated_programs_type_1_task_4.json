{
    "source": [
        "/workspace/create_poisoned_set.py",
        "/workspace/train_on_poisoned_set.py",
        "/workspace/other_defense.py"
    ],
    "usage_instructions": "To evaluate BaDExpert's detection performance against adaptive backdoor attacks, follow these steps for each attack type:\n\n1. First, create a poisoned dataset with the specific adaptive attack:\n   - For Adap-Blend: `python create_poisoned_set.py -dataset=cifar10 -poison_type=adaptive_blend -poison_rate=0.003 -cover_rate=0.003 -alpha=0.15`\n   - For Adap-Patch: `python create_poisoned_set.py -dataset=cifar10 -poison_type=adaptive_patch -poison_rate=0.003 -cover_rate=0.006`\n   - For TaCT: `python create_poisoned_set.py -dataset=cifar10 -poison_type=TaCT -poison_rate=0.003 -cover_rate=0.003`\n   - For All-to-All: `python create_poisoned_set.py -dataset=cifar10 -poison_type=badnet_all_to_all -poison_rate=0.003`\n\n2. Train a model on the poisoned dataset:\n   - For Adap-Blend: `python train_on_poisoned_set.py -dataset=cifar10 -poison_type=adaptive_blend -poison_rate=0.003 -cover_rate=0.003 -alpha=0.15 -test_alpha=0.2`\n   - For Adap-Patch: `python train_on_poisoned_set.py -dataset=cifar10 -poison_type=adaptive_patch -poison_rate=0.003 -cover_rate=0.006`\n   - For TaCT: `python train_on_poisoned_set.py -dataset=cifar10 -poison_type=TaCT -poison_rate=0.003 -cover_rate=0.003`\n   - For All-to-All: `python train_on_poisoned_set.py -dataset=cifar10 -poison_type=badnet_all_to_all -poison_rate=0.003`\n\n3. Apply the BaDExpert defense and evaluate its performance:\n   - For Adap-Blend: `python other_defense.py -defense=BaDExpert -dataset=cifar10 -poison_type=adaptive_blend -poison_rate=0.003 -cover_rate=0.003 -alpha=0.15 -test_alpha=0.2`\n   - For Adap-Patch: `python other_defense.py -defense=BaDExpert -dataset=cifar10 -poison_type=adaptive_patch -poison_rate=0.003 -cover_rate=0.006`\n   - For TaCT: `python other_defense.py -defense=BaDExpert -dataset=cifar10 -poison_type=TaCT -poison_rate=0.003 -cover_rate=0.003`\n   - For All-to-All: `python other_defense.py -defense=BaDExpert -dataset=cifar10 -poison_type=badnet_all_to_all -poison_rate=0.003`\n\nThe output will include AUROC and ASR metrics that can be compared against the baseline performance to assess degradation under adaptive attacks."
}