{
  "questions": [
    {
      "hypothesis": "The size of the reserved clean set (Dc) affects BaDExpert's backdoor detection performance, with performance remaining robust (AUROC >98%) for moderately sized sets but potentially degrading when the number of clean samples is extremely limited.",
      "method": "Conduct a detailed ablation study by varying the reserved clean set size from 200 to 1800 samples, considering that the default is 2000 samples (roughly 5% of the training data). For a fixed attack type, such as the Blend attack on CIFAR10, run BaDExpert and record the AUROC for each Dc size. Plot the AUROC against the different Dc sizes with clear annotations marking performance trends. Additionally, compare these results against other detectors like SCALE-UP and STRIP under identical conditions to evaluate relative robustness. Include references to Table 1 (which provides results on post-development defense performance) and Table 2 (which details backdoor input detection metrics) to supplement the analysis.",
      "expected_outcome": "It is anticipated that BaDExpert will exhibit AUROC values above 98% for most settings with moderately sized clean sets, but for extremely limited clean sets (e.g., 200 or 400 samples), the AUROC may drop to around 95% or even lower (near 90%). Despite this, BaDExpert is expected to outperform the compared baselines (SCALE-UP and STRIP) under these constrained data conditions, supporting its overall robustness.",
      "subsection_source": "4.1 S ETUP",
      "source": [
        "/workspace/create_clean_set.py",
        "/workspace/other_defense.py"
      ],
      "usage_instructions": "To conduct the ablation study on how the size of the reserved clean set (Dc) affects BaDExpert's backdoor detection performance, follow these steps:\n\n1. First, create clean sets of different sizes (from 200 to 1800 samples) using the create_clean_set.py script:\n   ```\n   python create_clean_set.py -dataset=cifar10 -clean_budget 200\n   python create_clean_set.py -dataset=cifar10 -clean_budget 400\n   python create_clean_set.py -dataset=cifar10 -clean_budget 600\n   python create_clean_set.py -dataset=cifar10 -clean_budget 800\n   python create_clean_set.py -dataset=cifar10 -clean_budget 1000\n   python create_clean_set.py -dataset=cifar10 -clean_budget 1200\n   python create_clean_set.py -dataset=cifar10 -clean_budget 1400\n   python create_clean_set.py -dataset=cifar10 -clean_budget 1600\n   python create_clean_set.py -dataset=cifar10 -clean_budget 1800\n   ```\n\n2. For each clean set size, run the BaDExpert detection on a fixed attack type (e.g., Blend attack on CIFAR10):\n   ```\n   python other_defense.py -defense=BaDExpert -dataset=cifar10 -poison_type=blend -poison_rate=0.003 -cover_rate=0.003 -alpha=0.15\n   ```\n\n3. After each run, record the AUROC value from the output. The AUROC will be printed in the output as 'AUC: X.XXXX'.\n\n4. For comparison with other detectors, run SCALE-UP and STRIP under identical conditions:\n   ```\n   python other_defense.py -defense=ScaleUp -dataset=cifar10 -poison_type=blend -poison_rate=0.003 -cover_rate=0.003 -alpha=0.15\n   python other_defense.py -defense=STRIP -dataset=cifar10 -poison_type=blend -poison_rate=0.003 -cover_rate=0.003 -alpha=0.15\n   ```\n\n5. Plot the AUROC values against the different clean set sizes to visualize the performance trend.\n\nNote: Between each experiment with a different clean set size, you may need to reset the environment or delete the previously created clean set directory to avoid conflicts.",
      "requirements": [
        "Step 1: Set up the environment by importing necessary libraries (numpy, torch, torchvision, etc.) (/workspace/create_clean_set.py:1-9)",
        "Step 2: Parse command line arguments for dataset type and clean_budget (number of clean samples) (/workspace/create_clean_set.py:16-23)",
        "Step 3: Set random seed for reproducibility (/workspace/create_clean_set.py:25)",
        "Step 4: Load the appropriate dataset based on the dataset argument (CIFAR10, GTSRB, etc.) with proper transformations (/workspace/create_clean_set.py:32-99)",
        "Step 5: Create directory structure for storing clean samples (/workspace/create_clean_set.py:105-127)",
        "Step 6: Randomly shuffle and split the dataset into clean_split (for defense) and test_split (for evaluation) based on clean_budget (/workspace/create_clean_set.py:129-136)",
        "Step 7: Save the clean_split samples and their labels to disk (/workspace/create_clean_set.py:139-155)",
        "Step 8: Save the test_split samples and their labels to disk (/workspace/create_clean_set.py:158-173)",
        "Step 9: Import the appropriate backdoor detection method based on the defense argument (/workspace/other_defense.py:74-257)",
        "Step 10: Initialize the defense method with appropriate parameters (/workspace/other_defense.py:74-257)",
        "Step 11: Run the detection method and record performance metrics (AUROC) (/workspace/other_defense.py:74-257)",
        "Step 12: Compare detection performance across different clean set sizes and detection methods (/workspace/other_defense.py:74-257)"
      ],
      "agent_instructions": "Your task is to implement a system for evaluating how the size of a reserved clean dataset affects backdoor detection performance in neural networks. You need to create two main components:\n\n1. A script to create clean datasets of varying sizes (200 to 1800 samples) from standard datasets like CIFAR10. This script should:\n   - Accept parameters for dataset type and the number of clean samples (clean_budget)\n   - Load the specified dataset\n   - Randomly select the specified number of samples to create a clean set for defense purposes\n   - Save these samples and their labels to disk in an organized directory structure\n   - Save the remaining samples as a test set for evaluation\n\n2. A script to run backdoor detection methods on potentially backdoored models using the clean datasets. This script should:\n   - Support multiple backdoor detection methods including BaDExpert, SCALE-UP, and STRIP\n   - Accept parameters for dataset, poison type, poison rate, cover rate, alpha, and defense method\n   - Load the appropriate detection method\n   - Run the detection and output performance metrics, particularly AUROC values\n\nThe system should allow for running experiments with different clean set sizes to analyze how the amount of clean data affects detection performance. The output should include AUROC values that can be used to plot performance trends.",
      "masked_source": [
        "/workspace/create_clean_set.py",
        "/workspace/other_defense.py"
      ]
    }
  ]
}