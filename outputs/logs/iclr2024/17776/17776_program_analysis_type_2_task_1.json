{
  "requirements": [
    "Step 1: Load the RepoBench dataset from Hugging Face (/workspace/run.py:147-147)",
    "Step 2: Filter the dataset by specified levels (2k, 4k, 8k) (/workspace/run.py:153-153)",
    "Step 3: Load the model and tokenizer from Hugging Face (/workspace/run.py:156-162)",
    "Step 4: For each data point in the dataset, construct a prompt by combining cross-file and in-file context, respecting token length constraints (/workspace/data/utils.py:3-59)",
    "Step 5: Generate completions using the model with specified parameters (temperature, top_p, max_new_tokens) (/workspace/run.py:173-173)",
    "Step 6: Process the model outputs to extract the first non-comment line of code (/workspace/run.py:176-177)",
    "Step 7: Save the results (predictions and ground truth) to a file for each dataset level (/workspace/run.py:179-180)",
    "Step 8: Calculate evaluation metrics (Exact Match, Edit Similarity, and CodeBLEU) for the generated completions (/workspace/eval.py:43-45)",
    "Step 9: Report performance metrics for each level and calculate weighted averages across all levels (/workspace/eval.py:53-64)",
    "Final Step: Compare the performance metrics across different prompt lengths (2k, 4k, 8k) to analyze how the model's ability to leverage context changes with increasing context length (/workspace/eval.py:53-64)"
  ],
  "agent_instructions": "Your task is to create a system that evaluates how a code completion model's performance varies across different prompt lengths (2k, 4k, 8k tokens) in the RepoBench dataset. This will help understand how effectively the model leverages increasing context lengths.\n\nYou need to:\n\n1. Create a script to run a code completion model on the RepoBench dataset:\n   - Load the RepoBench Python dataset from Hugging Face\n   - Filter the dataset to include only specific levels (2k, 4k, 8k) that represent different prompt lengths\n   - Load a pretrained code completion model and its tokenizer\n   - For each data point, construct a prompt that combines repository context and file context\n   - Ensure prompts respect the token length constraints of the model\n   - Generate completions using the model\n   - Extract the first non-comment line from each completion\n   - Save the results (predictions and ground truth) for each level\n\n2. Create an evaluation script that:\n   - Calculates three key metrics for each level:\n     * Exact Match (EM): percentage of predictions that exactly match the ground truth\n     * Edit Similarity (ES): measure of how similar the predictions are to the ground truth\n     * CodeBLEU (CB): code-specific metric that evaluates the quality of generated code\n   - Reports the metrics for each level\n   - Calculates weighted averages across all levels\n\n3. Create a utility module that handles prompt construction:\n   - Combine cross-file context (information from other files in the repository) and in-file context\n   - Handle token length constraints by truncating if necessary\n\nThe system should allow you to run the model separately on each level (2k, 4k, 8k) and then compare the performance metrics to analyze how the model's ability to leverage context changes with increasing context length."
}