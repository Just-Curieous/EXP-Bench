{
  "questions": [
    {
      "hypothesis": "Larger language models and increased context lengths yield better performance for code completion tasks on RepoBench-C, as evidenced by enhanced performance metrics when processing extended in-file and cross-file contexts.",
      "method": "Replicate the RepoBench-C experiments by evaluating a range of autoregressive language models (e.g., CodeGen, CodeLlama, StarCoder, and Codex) for both Python and Java. Perform experiments under two context length settings: 2k and 8k. For the 2k setting, refer to the Level 2k token range (approximately 640\u20131,600 tokens), and for the 8k setting, refer to the token range approximations provided in RepoBench v1.1. For each model, execute the next-line prediction task by providing both the in-file context (Cin) and cross-file context (Cx) as inputs. Ensure data parity by adhering to the dataset constraints (e.g., a minimum token threshold of 12,000 for Python and 24,000 for Java, with the retrieval process requiring at least 10 candidates as detailed in Table 1). Measure performance using weighted average Exact Match (EM), Edit Similarity (Edit Sim), and CodeBLEU scores. Record additional metrics such as the mean number of candidates and tokens for each setting as reported in Table 1, and, where possible, include visual insights referenced in Figures 1\u20133 to reflect the impact of extended contexts.",
      "expected_outcome": "Based on previous findings, it is expected that larger models\u2014particularly CodeLlama-34B for Python and Codex for Java\u2014will demonstrate higher predictive accuracy. An increase in context length from 2k to 8k should further boost performance metrics (EM, Edit Sim, and CodeBLEU), although the magnitude of improvement may depend on the model architecture and language-specific characteristics. Detailed analysis across these metrics will provide insights into the benefits of extended context lengths and repository-level training.",
      "subsection_source": "4.2 REPOBENCH-C",
      "source": [
        "/workspace/run.py",
        "/workspace/eval.py"
      ],
      "usage_instructions": "To replicate the RepoBench-C experiments with different context lengths (2k and 8k) for both Python and Java, follow these steps:\n\n1. First, run the experiments for Python models at 2k context length:\n   ```bash\n   CUDA_VISIBLE_DEVICES=0 python run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n                  --dataset_name \"tianyang/repobench_python_v1.1\" \\\n                  --language \"python\" \\\n                  --max_token_nums 1600 \\\n                  --levels \"2k\" \\\n                  --temperature 0.2 \\\n                  --top_p 0.95 \\\n                  --max_new_tokens 128 \\\n                  --batch_size 1\n   ```\n\n2. Run the experiments for Python models at 8k context length:\n   ```bash\n   CUDA_VISIBLE_DEVICES=0 python run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n                  --dataset_name \"tianyang/repobench_python_v1.1\" \\\n                  --language \"python\" \\\n                  --max_token_nums 7200 \\\n                  --levels \"8k\" \\\n                  --temperature 0.2 \\\n                  --top_p 0.95 \\\n                  --max_new_tokens 128 \\\n                  --batch_size 1\n   ```\n\n3. Repeat steps 1-2 for Java models:\n   ```bash\n   CUDA_VISIBLE_DEVICES=0 python run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n                  --dataset_name \"tianyang/repobench_java_v1.1\" \\\n                  --language \"java\" \\\n                  --max_token_nums 1600 \\\n                  --levels \"2k\" \\\n                  --temperature 0.2 \\\n                  --top_p 0.95 \\\n                  --max_new_tokens 128 \\\n                  --batch_size 1\n   ```\n\n4. Repeat for 8k context length with Java:\n   ```bash\n   CUDA_VISIBLE_DEVICES=0 python run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n                  --dataset_name \"tianyang/repobench_java_v1.1\" \\\n                  --language \"java\" \\\n                  --max_token_nums 7200 \\\n                  --levels \"8k\" \\\n                  --temperature 0.2 \\\n                  --top_p 0.95 \\\n                  --max_new_tokens 128 \\\n                  --batch_size 1\n   ```\n\n5. Repeat steps 1-4 for each model you want to evaluate (e.g., CodeGen, StarCoder, Codex).\n\n6. After generating completions for all models and settings, evaluate the results:\n   ```bash\n   python eval.py --path \"results/CodeLlama-34b-hf-python\" --language \"python\"\n   python eval.py --path \"results/CodeLlama-34b-hf-java\" --language \"java\"\n   ```\n\n7. Repeat step 6 for each model's results directory.\n\nThe evaluation will output Exact Match (EM), Edit Similarity (ES), and CodeBLEU (CB) scores for each setting (cross_file_first, cross_file_random, in_file) as well as weighted averages across all settings, allowing you to compare performance across different models and context lengths.",
      "requirements": [
        "Step 1: Load the specified dataset from Hugging Face based on the language (Python or Java) (/workspace/run.py:146-147)",
        "Step 2: Filter the dataset by date range and context length levels (/workspace/run.py:149-153)",
        "Step 3: Load the language model and tokenizer with appropriate configuration (/workspace/run.py:155-161)",
        "Step 4: Create a directory structure to save results (/workspace/run.py:163-165)",
        "Step 5: For each data point in the dataset, construct a prompt that includes repository context and code snippets (/workspace/run.py:167-170, /workspace/data/utils.py:3-59)",
        "Step 6: Generate completions using the language model with specified parameters (temperature, top_p, etc.) (/workspace/run.py:172-173)",
        "Step 7: Extract the first non-comment line from the generated code (/workspace/run.py:175-177, /workspace/run.py:12-79)",
        "Step 8: Save the results (prediction, ground truth, metadata) to JSONL files (/workspace/run.py:179-180)",
        "Step 9: Load the generated results for evaluation (/workspace/eval.py:14-33)",
        "Step 10: Calculate Exact Match score by comparing predicted code with ground truth (/workspace/eval.py:40-43, /workspace/evaluation/metrics.py:4-27)",
        "Step 11: Calculate Edit Similarity score using fuzzy string matching (/workspace/eval.py:44, /workspace/evaluation/metrics.py:31-53)",
        "Step 12: Calculate CodeBLEU score for language-specific code quality evaluation (/workspace/eval.py:45, /workspace/evaluation/metrics.py:96-129)",
        "Step 13: Compute weighted averages of metrics across all settings (/workspace/eval.py:47-61)",
        "Final Step: Print evaluation results for each setting and the weighted averages (/workspace/eval.py:53-64)"
      ],
      "agent_instructions": "Create a system to evaluate large language models on code completion tasks using the RepoBench dataset. The system should consist of two main components:\n\n1. A script to run inference with language models:\n   - Load a code repository dataset (RepoBench) for either Python or Java\n   - Support filtering by context length (2k, 8k tokens)\n   - Construct prompts that include both cross-file context and in-file code\n   - Generate next-line completions using a specified language model\n   - Extract meaningful code (ignoring comments) from model outputs\n   - Save predictions alongside ground truth in a structured format\n\n2. An evaluation script that:\n   - Processes the generated completions\n   - Calculates three key metrics:\n     * Exact Match (EM): percentage of predictions that exactly match ground truth\n     * Edit Similarity (ES): string similarity between predictions and ground truth\n     * CodeBLEU (CB): code-specific BLEU score that considers syntax and semantics\n   - Reports results for different context settings (cross_file_first, cross_file_random, in_file)\n   - Computes weighted averages across all settings\n\nThe system should handle both Python and Java code, properly parse comments in both languages, and support various context lengths to evaluate how model performance changes with increasing context.",
      "masked_source": [
        "/workspace/run.py",
        "/workspace/eval.py"
      ]
    }
  ]
}