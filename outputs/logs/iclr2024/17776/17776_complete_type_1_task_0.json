{
  "questions": [
    {
      "hypothesis": "Will semantic retrieval methods (particularly InstructOR and UniXcoder) significantly outperform both lexical retrieval (Jaccard and Edit Similarity) and random retrieval in identifying the most relevant code snippets for next-line prediction?",
      "method": "Using the RepoBench-R dataset with down-sampled data to ensure parity between Java and Python (as detailed in Table 1), implement and compare three retrieval strategies: (a) Random Retrieval (execute 100 runs and average results), (b) Lexical Retrieval using Jaccard and Edit Similarity, and (c) Semantic Retrieval using encoder models such as InstructOR, UniXcoder, CodeBERT, among others. For each experiment, crop the last 3 lines (m = 3) from the in-file code context to form C[-m:] and compute similarity scores at the token level (using cosine similarity for semantic methods). Extend the evaluation by considering datasets with varied candidate statistics (e.g., Python Hard with ~17.8 candidates and Java Hard with ~25.5 candidates) and across related tasks (RepoBench-C and RepoBench-P). Use evaluation metrics including Exact Match, Edit Similarity, and CodeBLEU, and perform statistical significance testing to rigorously compare performance differences.",
      "expected_outcome": "It is expected that InstructOR will consistently outperform the other models due to its superior semantic encoding capabilities, with UniXcoder achieving competitive performance despite having fewer parameters. Lexical methods (especially using Jaccard similarity) are anticipated to serve as a strong baseline, whereas random retrieval is expected to produce the lowest performance. Furthermore, evaluations across different languages and task difficulties should reinforce the robustness of semantic retrieval methods.",
      "subsection_source": "4.1 REPOBENCH-R",
      "source": [
        "/workspace/run_repobench_r.py",
        "/workspace/evaluate_retrieval.py"
      ],
      "usage_instructions": "1. First, run the retrieval experiment using run_repobench_r.py to compare different retrieval methods. For random retrieval, run 100 times and average the results. For lexical retrieval (Jaccard and Edit Similarity), run without specifying a model. For semantic retrieval (InstructOR and UniXcoder), specify the appropriate model name.\n\nExample commands:\n- For random retrieval (will be evaluated in evaluate_retrieval.py with print_random=True)\n- For lexical retrieval with Jaccard:\n  `python run_repobench_r.py --language python --similarity jaccard --keep_lines [3]`\n- For lexical retrieval with Edit Similarity:\n  `python run_repobench_r.py --language python --similarity edit --keep_lines [3]`\n- For semantic retrieval with UniXcoder:\n  `python run_repobench_r.py --language python --similarity cosine --keep_lines [3] --model_name microsoft/unixcoder-base`\n- For semantic retrieval with InstructOR:\n  `python run_repobench_r.py --language python --similarity cosine --keep_lines [3] --model_name hkunlp/instructor-base`\n\n2. Then, evaluate the results using evaluate_retrieval.py to compare the performance of different retrieval methods:\n  `python evaluate_retrieval.py --dir results/retrieval/unixcoder-base --print_random True`\n\nThe scripts will handle the RepoBench-R dataset with both Python and Java languages, and will crop the last 3 lines from the in-file code context as specified in the experiment. The evaluation will compute accuracy metrics for both easy and hard datasets.",
      "requirements": [
        "Step 1: Load the RepoBench-R dataset for both 'cross_file_first' and 'cross_file_random' settings for the specified language (/workspace/run_repobench_r.py:19-23)",
        "Step 2: Set up the appropriate tokenizer based on the retrieval method (lexical or semantic) (/workspace/run_repobench_r.py:25-50)",
        "Step 3: For each dataset setting (first and random), process each data point by cropping the code to keep only the specified number of lines (/workspace/run_repobench_r.py:58-67)",
        "Step 4: Perform retrieval on the cropped code against candidate contexts using the specified similarity method (Jaccard, Edit, or Cosine) (/workspace/run_repobench_r.py:68-74)",
        "Step 5: Record the ground truth index for each retrieval task (/workspace/run_repobench_r.py:76-77)",
        "Step 6: Save the retrieval results to JSON files in the appropriate directory based on the retrieval method (/workspace/run_repobench_r.py:79-87)",
        "Step 7: Load the retrieval results from the specified directory for evaluation (/workspace/evaluate_retrieval.py:14-16)",
        "Step 8: Initialize a data structure to store accuracy metrics for both Python and Java languages, for both 'first' and 'random' settings, and for both 'easy' and 'hard' datasets (/workspace/evaluate_retrieval.py:18-39)",
        "Step 9: For random retrieval baseline (if requested), run 100 trials with shuffled candidate orders and calculate average accuracy (/workspace/evaluate_retrieval.py:54-87, 120-156)",
        "Step 10: Calculate accuracy@k metrics for each retrieval method on both easy and hard datasets (/workspace/evaluate_retrieval.py:89-116, 158-192)",
        "Step 11: Print the evaluation results in a formatted table for both Python and Java languages (/workspace/evaluate_retrieval.py:194-213)"
      ],
      "agent_instructions": "Your task is to implement a system for evaluating different code retrieval methods on the RepoBench-R dataset. The system consists of two main components:\n\n1. A retrieval script that:\n   - Loads the RepoBench-R dataset for both Python and Java languages\n   - Supports multiple retrieval methods:\n     - Random retrieval (baseline)\n     - Lexical retrieval (using Jaccard similarity and Edit similarity)\n     - Semantic retrieval (using embedding models like UniXcoder and InstructOR)\n   - Crops code snippets to keep only a specified number of lines\n   - Performs retrieval to find the most relevant context for each code snippet\n   - Saves the retrieval results to JSON files\n\n2. An evaluation script that:\n   - Loads the retrieval results\n   - Calculates accuracy metrics (accuracy@k) for both easy and hard datasets\n   - For random retrieval, runs multiple trials and averages the results\n   - Compares the performance of different retrieval methods\n   - Outputs the evaluation results in a formatted table\n\nThe system should handle both 'cross_file_first' and 'cross_file_random' settings, and evaluate performance on both easy and hard datasets. The evaluation should calculate accuracy@1, accuracy@3, and accuracy@5 metrics.",
      "masked_source": [
        "/workspace/run_repobench_r.py",
        "/workspace/evaluate_retrieval.py"
      ]
    }
  ]
}