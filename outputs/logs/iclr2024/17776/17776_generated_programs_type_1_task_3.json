{
    "source": ["/workspace/run.py", "/workspace/eval.py"],
    "usage_instructions": "To evaluate different model architectures on language-specific code completion tasks in RepoBench-C, follow these steps:\n\n1. Run the Python dataset evaluation for each model architecture (CodeGen, CodeLlama, StarCoder, Codex) using the run.py script with the 2k and 8k subsets:\n\n```bash\n# Example for CodeLlama-34B on Python 2k subset\npython run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n           --dataset_name \"tianyang/repobench_python_v1.1\" \\\n           --language \"python\" \\\n           --levels \"2k\" \\\n           --max_token_nums 15800 \\\n           --temperature 0.2 \\\n           --top_p 0.95 \\\n           --max_new_tokens 128 \\\n           --batch_size 1 \\\n           --res_dir \"./results_python_2k\"\n\n# Example for CodeLlama-34B on Python 8k subset\npython run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n           --dataset_name \"tianyang/repobench_python_v1.1\" \\\n           --language \"python\" \\\n           --levels \"8k\" \\\n           --max_token_nums 15800 \\\n           --temperature 0.2 \\\n           --top_p 0.95 \\\n           --max_new_tokens 128 \\\n           --batch_size 1 \\\n           --res_dir \"./results_python_8k\"\n```\n\n2. Run the Java dataset evaluation for each model architecture using the same run.py script:\n\n```bash\n# Example for CodeLlama-34B on Java 2k subset\npython run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n           --dataset_name \"tianyang/repobench_java_v1.1\" \\\n           --language \"java\" \\\n           --levels \"2k\" \\\n           --max_token_nums 15800 \\\n           --temperature 0.2 \\\n           --top_p 0.95 \\\n           --max_new_tokens 128 \\\n           --batch_size 1 \\\n           --res_dir \"./results_java_2k\"\n\n# Example for CodeLlama-34B on Java 8k subset\npython run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n           --dataset_name \"tianyang/repobench_java_v1.1\" \\\n           --language \"java\" \\\n           --levels \"8k\" \\\n           --max_token_nums 15800 \\\n           --temperature 0.2 \\\n           --top_p 0.95 \\\n           --max_new_tokens 128 \\\n           --batch_size 1 \\\n           --res_dir \"./results_java_8k\"\n```\n\n3. Evaluate the results for each model and language using the eval.py script:\n\n```bash\n# Evaluate Python results\npython eval.py --path \"results_python_2k/CodeLlama-34b-hf-python\" --language \"python\"\npython eval.py --path \"results_python_8k/CodeLlama-34b-hf-python\" --language \"python\"\n\n# Evaluate Java results\npython eval.py --path \"results_java_2k/CodeLlama-34b-hf-java\" --language \"java\"\npython eval.py --path \"results_java_8k/CodeLlama-34b-hf-java\" --language \"java\"\n```\n\n4. Repeat steps 1-3 for all model architectures (CodeGen, CodeLlama in various sizes, StarCoder, and Codex).\n\n5. After collecting all evaluation results, perform statistical comparisons (t-tests) on the reported scores to determine if specific models show significant performance advantages on one programming language over the other."
}