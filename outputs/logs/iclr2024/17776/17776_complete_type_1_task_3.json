{
  "questions": [
    {
      "hypothesis": "Different model architectures exhibit language-specific strengths in code completion tasks.",
      "method": "Design an experiment where the same set of code completion tasks in RepoBench-C are evaluated separately for Python and Java. Use two subsets from RepoBench-C (the 2k and 8k subsets) to capture different prompt lengths and complexities. For Python, the 2k subset contains 12,000 XF-F, 5,000 XF-R, and 7,000 IF data points with a mean of 1,035 tokens, and the 8k subset contains 18,000 XF-F, 7,500 XF-R, and 10,500 IF data points with a mean of 3,967 tokens; use analogous settings for Java. Evaluate each model\u2014CodeGen (in various sizes), CodeLlama (all available parameter sizes), StarCoder, and Codex\u2014using consistent evaluation metrics such as Exact Match (EM), Edit Similarity, and CodeBLEU. Perform statistical comparisons (e.g., t-tests) on the reported scores in both settings to determine if specific models show significant performance advantages on one programming language over the other. Incorporate the detailed dataset configurations provided in RepoBench-C to ensure a comprehensive assessment of the models across varying context lengths and candidate complexities.",
      "expected_outcome": "The experiment should reveal that, while some models perform competitively across both languages, particular models exhibit language-specific strengths. For instance, Codex is expected to maintain a performance edge in Java tasks, whereas models like CodeLlama-34B might excel in Python tasks. These outcomes would support claims regarding language-specific performance variations in code completion.",
      "subsection_source": "4.2 REPOBENCH-C",
      "source": [
        "/workspace/run.py",
        "/workspace/eval.py"
      ],
      "usage_instructions": "To evaluate different model architectures on language-specific code completion tasks in RepoBench-C, follow these steps:\n\n1. Run the Python dataset evaluation for each model architecture (CodeGen, CodeLlama, StarCoder, Codex) using the run.py script with the 2k and 8k subsets:\n\n```bash\n# Example for CodeLlama-34B on Python 2k subset\npython run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n           --dataset_name \"tianyang/repobench_python_v1.1\" \\\n           --language \"python\" \\\n           --levels \"2k\" \\\n           --max_token_nums 15800 \\\n           --temperature 0.2 \\\n           --top_p 0.95 \\\n           --max_new_tokens 128 \\\n           --batch_size 1 \\\n           --res_dir \"./results_python_2k\"\n\n# Example for CodeLlama-34B on Python 8k subset\npython run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n           --dataset_name \"tianyang/repobench_python_v1.1\" \\\n           --language \"python\" \\\n           --levels \"8k\" \\\n           --max_token_nums 15800 \\\n           --temperature 0.2 \\\n           --top_p 0.95 \\\n           --max_new_tokens 128 \\\n           --batch_size 1 \\\n           --res_dir \"./results_python_8k\"\n```\n\n2. Run the Java dataset evaluation for each model architecture using the same run.py script:\n\n```bash\n# Example for CodeLlama-34B on Java 2k subset\npython run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n           --dataset_name \"tianyang/repobench_java_v1.1\" \\\n           --language \"java\" \\\n           --levels \"2k\" \\\n           --max_token_nums 15800 \\\n           --temperature 0.2 \\\n           --top_p 0.95 \\\n           --max_new_tokens 128 \\\n           --batch_size 1 \\\n           --res_dir \"./results_java_2k\"\n\n# Example for CodeLlama-34B on Java 8k subset\npython run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n           --dataset_name \"tianyang/repobench_java_v1.1\" \\\n           --language \"java\" \\\n           --levels \"8k\" \\\n           --max_token_nums 15800 \\\n           --temperature 0.2 \\\n           --top_p 0.95 \\\n           --max_new_tokens 128 \\\n           --batch_size 1 \\\n           --res_dir \"./results_java_8k\"\n```\n\n3. Evaluate the results for each model and language using the eval.py script:\n\n```bash\n# Evaluate Python results\npython eval.py --path \"results_python_2k/CodeLlama-34b-hf-python\" --language \"python\"\npython eval.py --path \"results_python_8k/CodeLlama-34b-hf-python\" --language \"python\"\n\n# Evaluate Java results\npython eval.py --path \"results_java_2k/CodeLlama-34b-hf-java\" --language \"java\"\npython eval.py --path \"results_java_8k/CodeLlama-34b-hf-java\" --language \"java\"\n```\n\n4. Repeat steps 1-3 for all model architectures (CodeGen, CodeLlama in various sizes, StarCoder, and Codex).\n\n5. After collecting all evaluation results, perform statistical comparisons (t-tests) on the reported scores to determine if specific models show significant performance advantages on one programming language over the other.",
      "requirements": [
        "Step 1: Load a language-specific code completion dataset (Python or Java) from HuggingFace (/workspace/run.py:147)",
        "Step 2: Filter the dataset by date range and complexity levels (2k, 8k, etc.) (/workspace/run.py:150-153)",
        "Step 3: Load the language model and tokenizer (/workspace/run.py:156-161)",
        "Step 4: Create a directory to save results (/workspace/run.py:164-165)",
        "Step 5: For each example in the dataset, construct a prompt with repository context (/workspace/run.py:167-170, /workspace/data/utils.py:3-59)",
        "Step 6: Generate code completions using the model with specified parameters (temperature, top_p, etc.) (/workspace/run.py:172-173)",
        "Step 7: Extract the first non-comment line from the generated code (/workspace/run.py:176-177, /workspace/run.py:12-79)",
        "Step 8: Save the predictions to a results file with metadata (/workspace/run.py:179-180)",
        "Step 9: Load the prediction results for evaluation (/workspace/eval.py:14-33)",
        "Step 10: Calculate Exact Match score between predictions and ground truth (/workspace/eval.py:43, /workspace/evaluation/metrics.py:4-27)",
        "Step 11: Calculate Edit Similarity score between predictions and ground truth (/workspace/eval.py:44, /workspace/evaluation/metrics.py:31-53)",
        "Step 12: Calculate CodeBLEU score between predictions and ground truth (/workspace/eval.py:45, /workspace/evaluation/metrics.py:96-129)",
        "Step 13: Calculate weighted averages of all metrics across dataset levels (/workspace/eval.py:48-64)",
        "Final Step: Display evaluation results for each level and the weighted averages (/workspace/eval.py:53-64)"
      ],
      "agent_instructions": "Create a system for evaluating language models on code completion tasks using the RepoBench-C benchmark. The system should consist of two main components:\n\n1. A code generation component that:\n   - Loads a language-specific code completion dataset (Python or Java) from HuggingFace\n   - Filters the dataset by complexity levels (e.g., 2k, 8k tokens)\n   - Loads a language model (e.g., CodeLlama, CodeGen, StarCoder)\n   - Constructs prompts that include repository context for each example\n   - Generates code completions using the model with appropriate parameters\n   - Extracts the first non-comment line from the generated code\n   - Saves the predictions to a results file\n\n2. An evaluation component that:\n   - Loads the prediction results\n   - Evaluates the predictions using three metrics:\n     - Exact Match (EM): Percentage of predictions that exactly match ground truth\n     - Edit Similarity (ES): Fuzzy string matching score between predictions and ground truth\n     - CodeBLEU (CB): Code-specific BLEU score that considers syntax and semantics\n   - Calculates metrics for different dataset levels (cross_file_first, cross_file_random, in_file)\n   - Computes weighted averages across all levels\n   - Displays the evaluation results\n\nThe system should support command-line arguments for specifying model name, dataset, language (Python or Java), complexity levels, and other generation parameters.",
      "masked_source": [
        "/workspace/run.py",
        "/workspace/eval.py"
      ]
    }
  ]
}