{
    "questions": [
        {
            "hypothesis": "Will semantic retrieval methods (particularly InstructOR and UniXcoder) significantly outperform both lexical retrieval (Jaccard and Edit Similarity) and random retrieval in identifying the most relevant code snippets for next-line prediction?",
            "method": "Using the RepoBench-R dataset with down-sampled data to ensure parity between Java and Python (as detailed in Table 1), implement and compare three retrieval strategies: (a) Random Retrieval (execute 100 runs and average results), (b) Lexical Retrieval using Jaccard and Edit Similarity, and (c) Semantic Retrieval using encoder models such as InstructOR, UniXcoder, CodeBERT, among others. For each experiment, crop the last 3 lines (m = 3) from the in-file code context to form C[-m:] and compute similarity scores at the token level (using cosine similarity for semantic methods). Extend the evaluation by considering datasets with varied candidate statistics (e.g., Python Hard with ~17.8 candidates and Java Hard with ~25.5 candidates) and across related tasks (RepoBench-C and RepoBench-P). Use evaluation metrics including Exact Match, Edit Similarity, and CodeBLEU, and perform statistical significance testing to rigorously compare performance differences.",
            "expected_outcome": "It is expected that InstructOR will consistently outperform the other models due to its superior semantic encoding capabilities, with UniXcoder achieving competitive performance despite having fewer parameters. Lexical methods (especially using Jaccard similarity) are anticipated to serve as a strong baseline, whereas random retrieval is expected to produce the lowest performance. Furthermore, evaluations across different languages and task difficulties should reinforce the robustness of semantic retrieval methods.",
            "subsection_source": "4.1 REPOBENCH-R"
        },
        {
            "hypothesis": "Do language-specific coding practices lead to different retrieval accuracies, with Python code retrieval exhibiting higher accuracy than Java code retrieval on RepoBench-R?",
            "method": "Segment the RepoBench-R dataset by programming language (Python and Java) and further by the two subsets (Easy and Hard). Use a uniform experimental configuration with m = 3 and identical similarity calculations across all retrieval strategies. For each language and subset, run experiments using a variety of retrieval methods: baseline lexical strategies (Random, Lexical Jaccard, and Edit distance) and semantic methods (including models such as CodeGPT, UniXcoder, CodeBERT, CodeT5+, CodeGen, and InstructOR). Collect all relevant performance metrics as reported in Table 2, specifically acc@1 and acc@3 for the Easy subset and acc@1, acc@3, and acc@5 for the Hard subset. Analyze the results by comparing retrieval accuracies and other metrics between Python and Java, taking into account the differences in coding practices (for example, Python\u2019s tendency for proximate definition and use of function arguments versus Java\u2019s more complex class structures). Incorporate insights from the baseline weighted average results provided in Table 2 to contextualize the findings.",
            "expected_outcome": "Based on prior observations from the paper, Python retrieval tasks are anticipated to yield higher accuracy, likely due to the inherent closeness of context cues in Python code. In contrast, Java\u2019s use of more complex class structures is expected to introduce challenges, resulting in lower retrieval performance. These differences should be evident across both the Easy and Hard subsets and should be quantifiable via the specified metrics (acc@1, acc@3, and acc@5).",
            "subsection_source": "4.1 REPOBENCH-R"
        },
        {
            "hypothesis": "Larger language models and increased context lengths yield better performance for code completion tasks on RepoBench-C, as evidenced by enhanced performance metrics when processing extended in-file and cross-file contexts.",
            "method": "Replicate the RepoBench-C experiments by evaluating a range of autoregressive language models (e.g., CodeGen, CodeLlama, StarCoder, and Codex) for both Python and Java. Perform experiments under two context length settings: 2k and 8k. For the 2k setting, refer to the Level 2k token range (approximately 640\u20131,600 tokens), and for the 8k setting, refer to the token range approximations provided in RepoBench v1.1. For each model, execute the next-line prediction task by providing both the in-file context (Cin) and cross-file context (Cx) as inputs. Ensure data parity by adhering to the dataset constraints (e.g., a minimum token threshold of 12,000 for Python and 24,000 for Java, with the retrieval process requiring at least 10 candidates as detailed in Table 1). Measure performance using weighted average Exact Match (EM), Edit Similarity (Edit Sim), and CodeBLEU scores. Record additional metrics such as the mean number of candidates and tokens for each setting as reported in Table 1, and, where possible, include visual insights referenced in Figures 1\u20133 to reflect the impact of extended contexts.",
            "expected_outcome": "Based on previous findings, it is expected that larger models\u2014particularly CodeLlama-34B for Python and Codex for Java\u2014will demonstrate higher predictive accuracy. An increase in context length from 2k to 8k should further boost performance metrics (EM, Edit Sim, and CodeBLEU), although the magnitude of improvement may depend on the model architecture and language-specific characteristics. Detailed analysis across these metrics will provide insights into the benefits of extended context lengths and repository-level training.",
            "subsection_source": "4.2 REPOBENCH-C"
        },
        {
            "hypothesis": "Different model architectures exhibit language-specific strengths in code completion tasks.",
            "method": "Design an experiment where the same set of code completion tasks in RepoBench-C are evaluated separately for Python and Java. Use two subsets from RepoBench-C (the 2k and 8k subsets) to capture different prompt lengths and complexities. For Python, the 2k subset contains 12,000 XF-F, 5,000 XF-R, and 7,000 IF data points with a mean of 1,035 tokens, and the 8k subset contains 18,000 XF-F, 7,500 XF-R, and 10,500 IF data points with a mean of 3,967 tokens; use analogous settings for Java. Evaluate each model\u2014CodeGen (in various sizes), CodeLlama (all available parameter sizes), StarCoder, and Codex\u2014using consistent evaluation metrics such as Exact Match (EM), Edit Similarity, and CodeBLEU. Perform statistical comparisons (e.g., t-tests) on the reported scores in both settings to determine if specific models show significant performance advantages on one programming language over the other. Incorporate the detailed dataset configurations provided in RepoBench-C to ensure a comprehensive assessment of the models across varying context lengths and candidate complexities.",
            "expected_outcome": "The experiment should reveal that, while some models perform competitively across both languages, particular models exhibit language-specific strengths. For instance, Codex is expected to maintain a performance edge in Java tasks, whereas models like CodeLlama-34B might excel in Python tasks. These outcomes would support claims regarding language-specific performance variations in code completion.",
            "subsection_source": "4.2 REPOBENCH-C"
        },
        {
            "hypothesis": "Does incorporating cross-file context into the code completion pipeline improve performance over using only in-file context?",
            "method": "Set up a controlled experiment on the RepoBench-P task using the Codex (code-davinci-002) model with two configurations. In the Baseline configuration, reserve 1,600 tokens for the in-file context by cropping the 60 preceding lines, filling the remaining tokens with in-file content only up to a total prompt size of 6,400 tokens. In the Cross-file configuration, maintain the 1,600-token reservation for the in-file context (with the same 60-line crop) and append additional randomly selected cross-file code snippets until the total prompt reaches 6,400 tokens. The cross-file snippets can be selected following strategies similar to the 'gold-filled' approach where the gold snippet is included along with other random snippets. Run experiments on Python and Java test sets. Evaluate both configurations using Exact Match (EM), Edit Similarity (ES), and CodeBLEU metrics. Where appropriate, refer to the figures (e.g., Figure 1\u20133) and table entries reported in the ICLR 2024 publication to compare metric scores and provide visual insights into the performance differences.",
            "expected_outcome": "Based on the reported results in the paper, the inclusion of cross-file context is expected to enhance performance metrics compared to using only in-file context. The experimental configuration that integrates cross-file snippets should achieve higher Exact Match, Edit Similarity, and CodeBLEU scores, thereby demonstrating that even randomly selected cross-file snippets can contribute to a better contextual understanding and improved code completion accuracy.",
            "subsection_source": "4.3 REPOBENCH-P"
        },
        {
            "hypothesis": "Will retrieval methods based on semantic similarity using UniXcoder variants (i.e., UniXcoder-H2L and UniXcoder-L2H) outperform random retrieval in the combined retrieval and completion pipeline across both Python and Java datasets, in terms of next-line prediction accuracy measured by EM, Edit Similarity, and CodeBLEU metrics?",
            "method": "Design an experiment comparing two cross-file retrieval strategies. In the baseline setting, implement a Random retrieval strategy by selecting cross-file snippets randomly until reaching the overall prompt size limit of 6400 tokens. In the alternative setting, use semantic retrieval based on UniXcoder: implement two variants, High-to-Low (UniXcoder-H2L) and Low-to-High (UniXcoder-L2H), to select snippets based on their relevance to the three preceding in-file lines. Both experiments will use an in-file context limited to 1600 tokens with a cropping limit of the 60 preceding lines, and any remaining space up to 6400 tokens will be filled by cross-file context. Run these experiments on both Python and Java datasets as per the RepoBench-P configuration (e.g., Python: 10,867 examples with an average of 24 candidates and 6,399 tokens in the IF subset; Java: 10,599 examples with an average of 26 candidates and 6,196 tokens in the IF subset, as detailed in Table 1). Use Codex (code-davinci-002) to complete the next line of code and evaluate performance using Exact Match, Edit Similarity, and CodeBLEU metrics. Where applicable, incorporate visual references to Figures 1-3 to illustrate the experimental setup and retrieval pipeline, and refer to data distribution insights from Table 1 to contextualize the candidate and token statistics.",
            "expected_outcome": "It is expected that both UniXcoder variants will achieve improved performance compared to the random retrieval approach, as evidenced by higher EM, Edit Similarity, and CodeBLEU scores. This improvement would support the hypothesis that a relevance-based retrieval strategy, which leverages semantic similarity, enhances the effectiveness of next-line code completion. Additionally, improvements might be more pronounced in the Python dataset due to its localized function argument patterns, whereas Java's more complex class structures could present additional retrieval challenges.",
            "subsection_source": "4.3 R EPOBENCH -P"
        },
        {
            "hypothesis": "Does the ordering of the retrieved snippets in the cross-file context (placing the gold snippet at the head versus the tail) affect the code completion performance?",
            "method": "Set up an experiment using the RepoBench-P task where the cross-file context is composed of a gold snippet along with other randomly fetched snippets to fill the 6400-token capacity. Create two configurations: one where the gold snippet is placed at the beginning of the cross-file context (Gold-Filled-Head) and another where it is placed at the end (Gold-Filled-Tail). Use a fixed in-file context as described (1600 tokens with a limit of 60 preceding lines). Use Codex (code-davinci-002) for next-line prediction across both Python and Java datasets. Evaluate the performance of both configurations using Exact Match, Edit Similarity, and CodeBLEU metrics and analyze any statistically significant differences.",
            "expected_outcome": "The expected outcome is that positioning the gold snippet at the head (i.e., closer to the in-file context and the prediction point) will result in better code completion performance compared to positioning it at the tail. This supports the claim that the proximity of relevant code snippets in the context improves the autoregressive prediction accuracy.",
            "subsection_source": "4.3 R EPOBENCH -P"
        }
    ],
    "follow_up_work_ideas": [
        {
            "idea": "Investigate the effect of varying the number of preserved context lines (m) on retrieval performance.",
            "experiment_design": "Design an ablation study on the RepoBench-R dataset where m is varied (for example, using 1, 3, 5, 10, or more lines). For each setting, conduct retrieval experiments using both lexical and semantic methods, keeping all other variables constant. Evaluate using the same performance metrics (Exact Match, Edit Similarity, CodeBLEU) to determine the optimal number of context lines. Analyze whether increasing or decreasing m leads to improved accuracy or causes performance to plateau or degrade.",
            "subsection_source": "4.1 R EPOBENCH -R"
        },
        {
            "idea": "Examine the benefits of integrating multi-modal contrastive learning approaches with semantic retrieval models to further enhance performance.",
            "experiment_design": "Augment existing semantic retrieval models (e.g., UniXcoder) with an additional multi-modal contrastive learning component. Implement a modified training regime that leverages multi-modal data (such as code embeddings and auxiliary features) and retrain the model. Evaluate the modified model on RepoBench-R using the same cropping strategy (m = 3) and similarity computations (cosine similarity). Compare the performance against the baseline semantic retrieval outcomes to assess if multi-modal enhancements provide a measurable improvement.",
            "subsection_source": "4.1 R EPOBENCH -R"
        },
        {
            "idea": "Perform an ablation study on context components by separately evaluating the contributions of in-file context (Cin) and cross-file context (Cx) in code completion.",
            "experiment_design": "Design experiments where, for a fixed set of models, one condition uses only the in-file context (Cin) for next-line prediction while another condition uses both Cin and cross-file context (Cx). Evaluate on RepoBench-C using the same metrics (EM, Edit Sim, CodeBLEU) across both Python and Java. Compare performance differences between the ablation conditions to understand the relative importance of each context source.",
            "subsection_source": "4.2 R EPOBENCH -C"
        },
        {
            "idea": "Extend the evaluation to additional programming languages to assess the generalizability of models on RepoBench-C.",
            "experiment_design": "Collect and preprocess datasets for other popular programming languages that are not originally covered in RepoBench-C. Replicate the code completion task using the same autoregressive models and evaluation metrics. Perform cross-language analysis to investigate any shifts in performance trends and to determine if the observed strengths (e.g., Codex for Java, CodeLlama for Python) extend to other languages.",
            "subsection_source": "4.2 R EPOBENCH -C"
        },
        {
            "idea": "Investigate adaptive context allocation between in-file and cross-file snippets based on the complexity of the code file.",
            "experiment_design": "Develop a method that dynamically adjusts the token budget allocated to in-file and cross-file contexts based on metrics like code complexity or length. Utilize complexity analysis tools to determine optimal token splits and run experiments on the RepoBench-P task using the Codex model. Compare performance metrics (EM, ES, CodeBLEU) with the fixed token allocation approach described in the paper.",
            "subsection_source": "4.3 R EPOBENCH -P"
        },
        {
            "idea": "Extend the evaluation to additional programming languages to assess the generality of retrieval and completion improvements.",
            "experiment_design": "Select one or two new programming languages that differ structurally from Python and Java. Assemble appropriate datasets with similar characteristics (e.g., candidate snippet counts, context lengths) to the RepoBench datasets. Implement the best-performing retrieval strategies (e.g., UniXcoder-based retrieval) and run end-to-end pipeline experiments similar to RepoBench-P. Evaluate and compare performance metrics to see if the improvements observed for Python and Java hold for the new languages.",
            "subsection_source": "4.3 R EPOBENCH -P"
        }
    ],
    "main_takeaways": [
        "The paper presents a code auto-completion system that leverages a two-stage process: first retrieving relevant code snippets (using techniques such as lexical and semantic similarity measures) and then predicting the next line of code.",
        "It introduces two complementary tasks\u2014RepoBench-R for retrieval-based next-line prediction and RepoBench-P for an end-to-end pipeline that simulates systems like GitHub Copilot, incorporating both in-file and cross-file contexts.",
        "The experiments demonstrate that factors such as the number of context lines, inclusion of import statements, and choice of retrieval method (lexical vs. semantic) have significant impacts on next-line prediction performance, as measured by metrics including Exact Match, Edit Similarity, and CodeBLEU.",
        "Ablation studies on prompt construction and the number of lines kept during retrieval suggest that optimal configurations exist (e.g., short vs. long in-file contexts) and that these design choices directly influence retrieval and prediction accuracy.",
        "The paper also addresses practical concerns such as down-sampling to ensure parity between Java and Python datasets and discusses potential discrepancies due to quantized models and inference library issues."
    ]
}