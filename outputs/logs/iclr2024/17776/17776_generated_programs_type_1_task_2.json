{
    "source": ["/workspace/run.py", "/workspace/eval.py"],
    "usage_instructions": "To replicate the RepoBench-C experiments with different context lengths (2k and 8k) for both Python and Java, follow these steps:\n\n1. First, run the experiments for Python models at 2k context length:\n   ```bash\n   CUDA_VISIBLE_DEVICES=0 python run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n                  --dataset_name \"tianyang/repobench_python_v1.1\" \\\n                  --language \"python\" \\\n                  --max_token_nums 1600 \\\n                  --levels \"2k\" \\\n                  --temperature 0.2 \\\n                  --top_p 0.95 \\\n                  --max_new_tokens 128 \\\n                  --batch_size 1\n   ```\n\n2. Run the experiments for Python models at 8k context length:\n   ```bash\n   CUDA_VISIBLE_DEVICES=0 python run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n                  --dataset_name \"tianyang/repobench_python_v1.1\" \\\n                  --language \"python\" \\\n                  --max_token_nums 7200 \\\n                  --levels \"8k\" \\\n                  --temperature 0.2 \\\n                  --top_p 0.95 \\\n                  --max_new_tokens 128 \\\n                  --batch_size 1\n   ```\n\n3. Repeat steps 1-2 for Java models:\n   ```bash\n   CUDA_VISIBLE_DEVICES=0 python run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n                  --dataset_name \"tianyang/repobench_java_v1.1\" \\\n                  --language \"java\" \\\n                  --max_token_nums 1600 \\\n                  --levels \"2k\" \\\n                  --temperature 0.2 \\\n                  --top_p 0.95 \\\n                  --max_new_tokens 128 \\\n                  --batch_size 1\n   ```\n\n4. Repeat for 8k context length with Java:\n   ```bash\n   CUDA_VISIBLE_DEVICES=0 python run.py --model_name \"codellama/CodeLlama-34b-hf\" \\\n                  --dataset_name \"tianyang/repobench_java_v1.1\" \\\n                  --language \"java\" \\\n                  --max_token_nums 7200 \\\n                  --levels \"8k\" \\\n                  --temperature 0.2 \\\n                  --top_p 0.95 \\\n                  --max_new_tokens 128 \\\n                  --batch_size 1\n   ```\n\n5. Repeat steps 1-4 for each model you want to evaluate (e.g., CodeGen, StarCoder, Codex).\n\n6. After generating completions for all models and settings, evaluate the results:\n   ```bash\n   python eval.py --path \"results/CodeLlama-34b-hf-python\" --language \"python\"\n   python eval.py --path \"results/CodeLlama-34b-hf-java\" --language \"java\"\n   ```\n\n7. Repeat step 6 for each model's results directory.\n\nThe evaluation will output Exact Match (EM), Edit Similarity (ES), and CodeBLEU (CB) scores for each setting (cross_file_first, cross_file_random, in_file) as well as weighted averages across all settings, allowing you to compare performance across different models and context lengths."
}