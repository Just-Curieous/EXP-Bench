{
    "source": [
        "/workspace/run_repobench_r.py",
        "/workspace/evaluate_retrieval.py"
    ],
    "usage_instructions": "1. First, run the retrieval experiment using run_repobench_r.py to compare different retrieval methods. For random retrieval, run 100 times and average the results. For lexical retrieval (Jaccard and Edit Similarity), run without specifying a model. For semantic retrieval (InstructOR and UniXcoder), specify the appropriate model name.\n\nExample commands:\n- For random retrieval (will be evaluated in evaluate_retrieval.py with print_random=True)\n- For lexical retrieval with Jaccard:\n  `python run_repobench_r.py --language python --similarity jaccard --keep_lines [3]`\n- For lexical retrieval with Edit Similarity:\n  `python run_repobench_r.py --language python --similarity edit --keep_lines [3]`\n- For semantic retrieval with UniXcoder:\n  `python run_repobench_r.py --language python --similarity cosine --keep_lines [3] --model_name microsoft/unixcoder-base`\n- For semantic retrieval with InstructOR:\n  `python run_repobench_r.py --language python --similarity cosine --keep_lines [3] --model_name hkunlp/instructor-base`\n\n2. Then, evaluate the results using evaluate_retrieval.py to compare the performance of different retrieval methods:\n  `python evaluate_retrieval.py --dir results/retrieval/unixcoder-base --print_random True`\n\nThe scripts will handle the RepoBench-R dataset with both Python and Java languages, and will crop the last 3 lines from the in-file code context as specified in the experiment. The evaluation will compute accuracy metrics for both easy and hard datasets."
}