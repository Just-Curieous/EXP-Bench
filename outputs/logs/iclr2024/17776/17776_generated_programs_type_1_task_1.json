{
    "source": [
        "/workspace/run_repobench_r.py",
        "/workspace/evaluate_retrieval.py"
    ],
    "usage_instructions": "To compare retrieval accuracies between Python and Java code on RepoBench-R, follow these steps:\n\n1. First, run the retrieval experiments for both Python and Java using various retrieval methods with m=3:\n\n   For Python:\n   ```\n   # Run baseline lexical strategies\n   python run_repobench_r.py --language python --similarity jaccard --keep_lines [3] \n   python run_repobench_r.py --language python --similarity edit --keep_lines [3]\n   \n   # Run semantic methods\n   python run_repobench_r.py --language python --similarity cosine --keep_lines [3] --model_name microsoft/unixcoder-base\n   python run_repobench_r.py --language python --similarity cosine --keep_lines [3] --model_name microsoft/codebert-base\n   python run_repobench_r.py --language python --similarity cosine --keep_lines [3] --model_name microsoft/CodeGPT-small-py-adaptedGPT2\n   python run_repobench_r.py --language python --similarity cosine --keep_lines [3] --model_name Salesforce/codegen-350M-mono\n   ```\n\n   For Java:\n   ```\n   # Run baseline lexical strategies\n   python run_repobench_r.py --language java --similarity jaccard --keep_lines [3]\n   python run_repobench_r.py --language java --similarity edit --keep_lines [3]\n   \n   # Run semantic methods\n   python run_repobench_r.py --language java --similarity cosine --keep_lines [3] --model_name microsoft/unixcoder-base\n   python run_repobench_r.py --language java --similarity cosine --keep_lines [3] --model_name microsoft/codebert-base\n   python run_repobench_r.py --language java --similarity cosine --keep_lines [3] --model_name microsoft/CodeGPT-small-java-adaptedGPT2\n   python run_repobench_r.py --language java --similarity cosine --keep_lines [3] --model_name Salesforce/codegen-350M-multi\n   ```\n\n2. After running the retrieval experiments, evaluate the results for each model using the evaluate_retrieval.py script:\n\n   ```\n   # For each model directory in results/retrieval/\n   python evaluate_retrieval.py --dir results/retrieval/unixcoder-base\n   python evaluate_retrieval.py --dir results/retrieval/codebert-base\n   python evaluate_retrieval.py --dir results/retrieval/CodeGPT-small-py-adaptedGPT2\n   python evaluate_retrieval.py --dir results/retrieval/codegen-350M-mono\n   python evaluate_retrieval.py --dir results/retrieval/jaccard\n   python evaluate_retrieval.py --dir results/retrieval/edit\n   ```\n\n3. The evaluation script will output the acc@1, acc@3, and acc@5 metrics for both Python and Java across the Easy and Hard subsets, allowing you to compare the retrieval accuracies between the two languages."
}