{
    "source": ["/workspace/inference_repobench_p.py", "/workspace/evaluate_completion.py"],
    "usage_instructions": "To compare the performance of code completion with and without cross-file context on the RepoBench-P task using Codex (code-davinci-002), run the following commands:\n\n1. First, run the baseline configuration (in-file context only):\n   ```\n   python inference_repobench_p.py --language python --mode baseline --model_name codex\n   ```\n   This will use only in-file context with up to 6400 tokens, cropping to the last 60 lines for the in-file context.\n\n2. Then, run the cross-file configuration with random snippets (similar to gold-filled approach):\n   ```\n   python inference_repobench_p.py --language python --mode random --model_name codex\n   ```\n   This will use 1600 tokens for in-file context (cropped to 60 lines) and fill the remaining tokens with randomly selected cross-file snippets up to a total of 6400 tokens.\n\n3. Evaluate the results for both configurations:\n   ```\n   python evaluate_completion.py --path results_new/pipeline/codex/baseline/python\n   python evaluate_completion.py --path results_new/pipeline/codex/random/python\n   ```\n\n4. Repeat steps 1-3 for Java by changing the language parameter to 'java'.\n\nThe evaluation will output Exact Match (EM), Edit Similarity (ES), and CodeBLEU metrics for both configurations, allowing you to compare the performance with and without cross-file context."
}