[
  {
    "mode": "B",
    "question": "How can you create a simple animation with different motion patterns using AnimateDiff's core concepts?",
    "method": "Create a Python script that generates animations with three different motion patterns: circular motion, zoom effect, and wave pattern.",
    "expected_outcome": "Three separate GIF animations showing different motion patterns and a combined animation showing all three patterns together.",
    "source": [
      "/workspace/motion_module_example.py"
    ],
    "usage_instructions": "1. Import necessary libraries (torch, numpy, PIL, etc.)\n2. Define a function to save video frames as GIF animations\n3. Create a function that generates different motion patterns (circle, zoom, wave)\n4. For the circle pattern, animate a circle moving in a circular path\n5. For the zoom pattern, animate an expanding circle\n6. For the wave pattern, animate a sine wave moving across the frame\n7. Generate separate animations for each pattern\n8. Combine all animations into a single output file\n9. Save all animations as GIF files",
    "requirements": [
      "Step 1: Import necessary libraries including torch, numpy, PIL, and matplotlib for creating and manipulating images (motion_module_example.py:1-10)",
      "Step 2: Define a function to convert a sequence of frames into a GIF animation (motion_module_example.py:12-25)",
      "Step 3: Create a base function that generates a blank canvas/frame for animations (motion_module_example.py:27-35)",
      "Step 4: Implement a function that generates circular motion pattern by animating an object moving in a circular path (motion_module_example.py:37-55)",
      "Step 5: Implement a function that generates zoom effect pattern by animating an expanding/contracting circle (motion_module_example.py:57-75)",
      "Step 6: Implement a function that generates wave pattern by animating a sine wave moving across the frame (motion_module_example.py:77-95)",
      "Step 7: Generate separate animations for each motion pattern (circle, zoom, wave) (motion_module_example.py:97-115)",
      "Step 8: Combine all three motion patterns into a single animation (motion_module_example.py:117-130)",
      "Step 9: Save all animations (individual patterns and combined) as GIF files (motion_module_example.py:132-145)"
    ],
    "agent_instructions": "Create a Python script that demonstrates different motion patterns using animation techniques. The script should:\n\n1. Create three distinct motion pattern animations:\n   - A circular motion pattern where an object moves in a circular path\n   - A zoom effect where a circle expands and contracts\n   - A wave pattern where a sine wave moves across the frame\n\n2. Generate each animation as a sequence of frames, with each frame showing the progression of the motion pattern\n\n3. Save each motion pattern as a separate GIF animation\n\n4. Create a combined animation that showcases all three motion patterns together\n\n5. Save all animations (the three individual patterns and the combined animation) as GIF files\n\nYou'll need to:\n- Use libraries like PIL, numpy, and matplotlib for image creation and manipulation\n- Create functions to generate each type of motion pattern\n- Implement a utility function to convert sequences of frames into GIF animations\n- Ensure the animations clearly demonstrate the distinct characteristics of each motion pattern\n\nThe final output should be four GIF files: one for each individual motion pattern (circle, zoom, wave) and one combined animation."
  },
  {
    "mode": "B",
    "question": "How can you implement advanced motion transformations on static images using AnimateDiff's motion concepts?",
    "method": "Create a Python script that applies different motion transformations (zoom, pan, rotate) to static images to create animations.",
    "expected_outcome": "Multiple GIF animations showing zoom, pan, and rotation effects applied to different static images, plus a combined animation showing all transformations.",
    "source": [
      "/workspace/advanced_motion_example.py"
    ],
    "usage_instructions": "1. Import necessary libraries (torch, numpy, PIL, etc.)\n2. Define a function to save video frames as GIF animations\n3. Create a SimpleMotionModel class that can apply different motion patterns\n4. Implement the zoom transformation that creates a zoom-in effect\n5. Implement the pan transformation that creates a panning effect\n6. Implement the rotate transformation that creates a rotation effect\n7. Create sample static images with different patterns (gradient, checkerboard, circles)\n8. Apply each motion transformation to a different static image\n9. Save individual animations and a combined animation as GIF files",
    "requirements": [
      "Step 1: Import necessary libraries (torch, numpy, PIL, etc.) for image processing and animation creation (advanced_motion_example.py:1-8)",
      "Step 2: Define a function to save video frames as GIF animations using PIL's save method with duration and loop parameters (advanced_motion_example.py:10-18)",
      "Step 3: Create a SimpleMotionModel base class that defines the interface for applying motion patterns to images (advanced_motion_example.py:20-30)",
      "Step 4: Implement the ZoomMotion class that inherits from SimpleMotionModel and creates a zoom-in/out effect by scaling the image (advanced_motion_example.py:32-50)",
      "Step 5: Implement the PanMotion class that inherits from SimpleMotionModel and creates a panning effect by translating the image (advanced_motion_example.py:52-70)",
      "Step 6: Implement the RotateMotion class that inherits from SimpleMotionModel and creates a rotation effect by rotating the image around its center (advanced_motion_example.py:72-90)",
      "Step 7: Create a function to generate sample static images with different patterns (gradient, checkerboard, circles) (advanced_motion_example.py:92-120)",
      "Step 8: In the main function, create instances of each motion class and apply them to different static images (advanced_motion_example.py:122-140)",
      "Step 9: Save individual animations for each motion type applied to different images (advanced_motion_example.py:142-155)",
      "Step 10: Create and save a combined animation showing all transformations sequentially (advanced_motion_example.py:157-170)"
    ],
    "agent_instructions": "Create a Python script that applies different motion transformations to static images to create animations. The script should:\n\n1. Import necessary libraries for image processing (torch, numpy, PIL, etc.)\n\n2. Create a function to save sequences of frames as GIF animations\n\n3. Implement a SimpleMotionModel class hierarchy:\n   - A base class that defines the interface for applying motion patterns\n   - ZoomMotion class that creates zoom-in/out effects by scaling images\n   - PanMotion class that creates panning effects by translating images\n   - RotateMotion class that creates rotation effects by rotating images around their centers\n\n4. Create a function to generate sample static images with different patterns (such as gradient, checkerboard, circles)\n\n5. In the main function:\n   - Generate several sample static images\n   - Create instances of each motion transformation class\n   - Apply each motion transformation to a different static image\n   - Save individual animations for each transformation\n   - Create and save a combined animation showing all transformations\n\nThe output should be multiple GIF animations showing zoom, pan, and rotation effects applied to different static images, plus a combined animation showing all transformations."
  },
  {
    "mode": "A",
    "question": "How can you use AnimateDiff to generate animations from text prompts with different motion styles?",
    "method": "Use AnimateDiff's motion module and MotionLoRA to generate animations from text prompts with different camera movements.",
    "expected_outcome": "Multiple GIF animations showing different camera movements (zoom in, zoom out, pan left, pan right, etc.) applied to the same text prompt.",
    "source": [
      "/workspace/configs/prompts/2_motionlora/2_motionlora_RealisticVision.yaml",
      "/workspace/scripts/animate.py"
    ],
    "usage_instructions": "1. Set up the AnimateDiff environment with required dependencies\n2. Download the necessary model files (motion module, DreamBooth model, MotionLoRA models)\n3. Use the animate.py script with the 2_motionlora_RealisticVision.yaml configuration\n4. The configuration applies different MotionLoRA models (ZoomIn, ZoomOut, PanLeft, PanRight, etc.) to the same text prompt\n5. Each section in the configuration specifies a different motion style\n6. The script will generate multiple animations with the same content but different camera movements\n7. Compare the resulting animations to understand how different MotionLoRA models affect the motion",
    "requirements": [
      "Step 1: Set up the environment by importing necessary libraries including torch, diffusers, transformers, and AnimateDiff-specific modules (/workspace/scripts/animate.py:1-29)",
      "Step 2: Create a function to handle the animation generation process with command-line arguments (/workspace/scripts/animate.py:31-35)",
      "Step 3: Create a directory to save the generated animations with a timestamp (/workspace/scripts/animate.py:36-38)",
      "Step 4: Load the configuration file that specifies different motion styles (/workspace/scripts/animate.py:40-41)",
      "Step 5: Initialize the base models (tokenizer, text encoder, VAE) from the pretrained model path (/workspace/scripts/animate.py:44-46)",
      "Step 6: For each motion style configuration in the config file, process it separately (/workspace/scripts/animate.py:48-49)",
      "Step 7: Set up dimensions (width, height, length) for the animation (/workspace/scripts/animate.py:50-52)",
      "Step 8: Load the UNet model with appropriate configuration (/workspace/scripts/animate.py:54-55)",
      "Step 9: Optionally load and configure ControlNet if specified in the config (/workspace/scripts/animate.py:58-115)",
      "Step 10: Enable memory optimization with xformers if available (/workspace/scripts/animate.py:117-119)",
      "Step 11: Create the AnimationPipeline with all the loaded components (/workspace/scripts/animate.py:121-125)",
      "Step 12: Load the motion module, MotionLoRA weights, and other model components (/workspace/scripts/animate.py:127-139)",
      "Step 13: Prepare prompts, negative prompts, and random seeds for generation (/workspace/scripts/animate.py:141-146)",
      "Step 14: For each prompt, generate an animation using the pipeline with the specified motion style (/workspace/scripts/animate.py:148-170)",
      "Step 15: Save each generated animation as a GIF file with appropriate naming (/workspace/scripts/animate.py:172-174)",
      "Step 16: Combine all generated animations into a grid and save as a single GIF (/workspace/scripts/animate.py:178-179)",
      "Step 17: Save the final configuration for reproducibility (/workspace/scripts/animate.py:181)",
      "Final Step: Set up command-line argument parsing to allow users to specify configuration files and parameters (/workspace/scripts/animate.py:184-197)"
    ],
    "agent_instructions": "Your task is to create a system that generates animations from text prompts using AnimateDiff with different motion styles. You'll need to implement a script that does the following:\n\n1. Create a script that uses AnimateDiff's motion module and MotionLoRA to generate animations from text prompts\n2. The script should accept a configuration file that defines different motion styles to apply to the same text prompt\n3. For each motion style (like ZoomIn, ZoomOut, PanLeft, PanRight, TiltUp, TiltDown, etc.), the script should:\n   - Load the appropriate motion module and MotionLoRA model\n   - Apply the same text prompt with the different motion style\n   - Generate an animation and save it as a GIF\n4. The configuration should allow specifying:\n   - The text prompt and negative prompt\n   - The motion module path\n   - The MotionLoRA model path and its strength (alpha)\n   - The base model (DreamBooth model)\n   - Generation parameters like seed, steps, and guidance scale\n5. The script should save all generated animations in a timestamped directory\n6. Additionally, create a sample configuration file that demonstrates how to apply different camera movements (ZoomIn, ZoomOut, PanLeft, PanRight, TiltUp, TiltDown, RollingClockwise, RollingAnticlockwise) to the same text prompt\n\nThe goal is to demonstrate how AnimateDiff's MotionLoRA can be used to control camera movements in generated animations while keeping the content (defined by the text prompt) the same across all animations."
  },
  {
    "mode": "A",
    "question": "How can you use AnimateDiff's SparseCtrl to control animation generation with reference images?",
    "method": "Use AnimateDiff's SparseCtrl feature to generate animations controlled by reference RGB images.",
    "expected_outcome": "Animations that follow the visual content of the provided reference images while maintaining temporal consistency.",
    "source": [
      "/workspace/configs/prompts/3_sparsectrl/3_1_sparsectrl_i2v.yaml",
      "/workspace/scripts/animate.py"
    ],
    "usage_instructions": "1. Set up the AnimateDiff environment with required dependencies\n2. Download the necessary model files (motion module, adapter, SparseCtrl encoder)\n3. Use the animate.py script with the 3_1_sparsectrl_i2v.yaml configuration\n4. The configuration specifies reference images in the controlnet_images field\n5. The controlnet_image_indexs field determines which frames the reference images control\n6. For animation, a single reference image controls the entire animation\n7. For interpolation, two reference images control the start and end frames\n8. For more complex animations, multiple reference images can control specific frames\n9. Compare the resulting animations to see how the reference images guide the generation process",
    "requirements": [
      "Step 1: Set up the necessary imports for the animation pipeline, including torch, diffusers, transformers, and custom AnimateDiff modules (/workspace/scripts/animate.py:1-28)",
      "Step 2: Create a function to load and process configuration files that specify animation parameters (/workspace/scripts/animate.py:31-40)",
      "Step 3: Load the base models (tokenizer, text encoder, VAE) from the pretrained model path (/workspace/scripts/animate.py:43-46)",
      "Step 4: For each configuration in the config file, extract parameters like dimensions, prompts, and seeds (/workspace/scripts/animate.py:48-53)",
      "Step 5: Load the UNet model with appropriate configuration (/workspace/scripts/animate.py:54-55)",
      "Step 6: If controlnet_path is specified, load the SparseControlNetModel for reference image control (/workspace/scripts/animate.py:58-76)",
      "Step 7: Process reference images by loading, transforming, and encoding them (/workspace/scripts/animate.py:78-115)",
      "Step 8: Set up the AnimationPipeline with all required components (VAE, text encoder, UNet, controlnet) (/workspace/scripts/animate.py:121-125)",
      "Step 9: Load additional weights for motion module, adapter, and other components (/workspace/scripts/animate.py:127-139)",
      "Step 10: For each prompt, generate animations using the pipeline with the specified parameters and reference image indices (/workspace/scripts/animate.py:141-169)",
      "Step 11: Save the generated animations as GIFs (/workspace/scripts/animate.py:172-181)",
      "Final Step: Provide command-line interface to specify model paths, configuration file, and other parameters (/workspace/scripts/animate.py:184-197)"
    ],
    "agent_instructions": "Your task is to implement a system that uses AnimateDiff's SparseCtrl feature to control animation generation with reference images. This system should allow users to generate animations that follow the visual content of provided reference images while maintaining temporal consistency.\n\nYou need to create:\n\n1. A script that can generate animations using AnimateDiff with SparseCtrl control. The script should:\n   - Accept a configuration file that specifies animation parameters\n   - Load necessary models (Stable Diffusion, motion module, adapter, SparseCtrl encoder)\n   - Process reference images for control\n   - Generate animations based on text prompts and reference images\n   - Save the resulting animations as GIFs\n\n2. Support different control modes:\n   - Animation mode: A single reference image controls the entire animation\n   - Interpolation mode: Two reference images control the start and end frames\n   - Complex control: Multiple reference images control specific frames throughout the animation\n\nThe key components you need to implement are:\n\n- Loading and processing reference images\n- Setting up the AnimationPipeline with SparseControlNetModel\n- Applying reference images to specific frames using a control index mechanism\n- Generating animations with the pipeline\n\nThe system should allow users to specify:\n- Text prompts for the animation\n- Reference images for visual control\n- Which frames each reference image controls\n- Animation parameters (dimensions, steps, guidance scale)\n\nThe output should be animations that visually align with the reference images at the specified frames while creating smooth transitions between controlled frames."
  }
]