{
    "questions": [
        {
            "question": "Does the tensor-based RL model achieve higher success rates on minitasks with immediate rewards compared to those with sparse, delayed rewards?",
            "method": "Select a representative set of minitasks from the evaluation, clearly categorizing tasks into two groups: (1) tasks that provide explicit, immediate rewards (e.g., the 'CityTileWonder' minitask that has demonstrated \u223c90% success rate) and (2) tasks that provide sparse or delayed rewards, requiring longer-term strategic planning. For each task in both groups, train the tensor baseline model from scratch using the same architecture and training hyperparameters for 40,000 steps over 3 random seeds. Record the success rate for each training run. Additionally, analyze training curves and convergence trends as illustrated in Figure 6. Compute the average success rate and standard deviation for both groups, and perform a statistical analysis (e.g., a t-test or ANOVA) to determine if the observed differences are statistically significant. Incorporate summary statistics from Table 1 if applicable, and detail any observed trends or anomalies in performance across training steps.",
            "expected_outcome": "It is anticipated that minitasks offering immediate rewards will yield significantly higher success rates, as the tensor-based RL model benefits from clear and frequent feedback. In contrast, tasks with sparse, delayed rewards are expected to show markedly lower success rates due to the difficulty in aligning long-term strategic planning with reinforcement signals. The statistical analysis should confirm the significance of these differences, thereby validating the model's relative strengths and weaknesses as depicted in the experimental results (e.g., Figure 6).",
            "subsection_source": "5.1 T ENSOR -BASED REINFORCEMENT LEARNING"
        },
        {
            "question": "Does the current unit production-focused strategy in full games result in a myopic performance that achieves high immediate scores at the expense of long-term development? Can a modified reward scheme that adds a bonus for early city expansion\u2014despite an initial dip in population\u2014lead to improved overall game scores when considering multi-faceted metrics such as population, technology researched, and economic growth?",
            "method": "Conduct an experiment on the full game scenario under two experimental conditions as depicted in Figure 7. For the baseline condition, train the tensor RL model for 2 million steps using the original reward structure that heavily weights unit production. In the modified condition, adjust the reward configuration to incorporate a bonus for the early establishment of cities, aiming to stimulate long-term strategic planning even if it causes a short-term decline in population scores. Both conditions should use the same architecture, hyperparameters, and at least 3 random seeds to ensure robust and statistically significant comparisons. Record raw data on the overall game scores along with sub-metrics such as population, technology researched, and units built/killed over the training period. Analyze and compare the evolution of the overall scores and the detailed breakdown of game metrics using appropriate statistical methods to determine if the modified reward scheme effectively promotes exponential long-term growth.",
            "expected_outcome": "The baseline condition is expected to produce high immediate scores driven predominantly by unit production; however, it may lack long-term strategic growth. In contrast, the modified reward scheme is anticipated to eventually outperform the baseline by fostering early investment in city expansion, leading to exponential growth in population, technology, and economic metrics. This would validate that incentivizing early city expansion can mitigate the myopic tendencies of the original reinforcement learning strategy.",
            "subsection_source": "5.1 TENSOR-BASED REINFORCEMENT LEARNING"
        },
        {
            "question": "Does incorporating a hierarchical decision architecture improve strategic expansion and overall game performance in CivRealm?",
            "method": "Design an experiment comparing two language-based agents: one using a hierarchical decision architecture (Mastaba) and one relying on local observations without global oversight (BaseLang). Run 5 full games on each of 10 maps with distinct and diverse geographic features (including coastal, inland, and mixed terrains as illustrated in Figures 1 and 2). For each game, record detailed metrics such as population size, number of military units, overall game score, and extent of map expansion (e.g., area settled, with specific attention to turn milestones when significant expansion events occur). Additionally, document the impact of specific game-mechanic triggers (e.g., pirate invasions, diplomatic events) on each agent's performance. Supplement the quantitative metrics with timeline analyses by comparing progress at specified turns (for example, preliminary evaluations indicate that Mastaba may reach extensive map expansion by turn 113 compared to BaseLang\u2019s progress around turn 210). Moreover, consider and record in-game events and indicators as provided in Table 1 that outline performance metric benchmarks. Analyze the collected data using appropriate statistical tests (such as t-tests or ANOVA) to assess differences in performance across all measured outcomes. This refined setup leverages both the strategic overviews depicted in the figures and the structured metrics from the full game environment to comprehensively evaluate the benefits of a hierarchical decision architecture.",
            "expected_outcome": "It is expected that the hierarchical agent (Mastaba) will demonstrate significantly improved performance compared to BaseLang. Specifically, Mastaba should achieve a larger population, produce more military units, secure higher overall game scores, and exhibit faster map expansion. These findings would confirm that adopting a global perspective through a hierarchical decision architecture leads to improved strategic decisions and overall game performance in complex environments like CivRealm.",
            "subsection_source": "5.2 LANGUAGE-BASED AGENTS: BASELANG AND MASTABA"
        },
        {
            "question": "Does the triggering of built-in game mechanics, such as pirate invasions when the score threshold is reached, affect the survival durations of hierarchical versus non-hierarchical agents?",
            "method": "Using the same experimental setup as above, record not only the primary performance metrics but also the timing and consequences of game-mechanic triggers. For each game, log the following details using a data structure similar to that in Table 1 (with three columns: Game Score at Trigger, Trigger Turn, and Post-Trigger Survival Duration): (a) the game score at which the pirate invasion is triggered, (b) the turn number of the trigger, and (c) the number of additional turns (i.e., survival duration) played after the invasion. Additionally, record detailed metrics for phase transitions and common indicators, such as rewards, scores, success rate, policy loss, and value loss, as illustrated in Figures 18 to 24. Perform statistical correlation and survival analysis to compare the timing and severity of these invasions for both Mastaba and BaseLang. In particular, assess whether an earlier triggering of the pirate invasion correlates with a reduction in survival time, and determine if the hierarchical decision-making framework of Mastaba, which may rapidly increase scores and trigger events earlier, yields overall strategic gains at the expense of shorter sustained play post-event.",
            "expected_outcome": "It is anticipated that Mastaba\u2019s hierarchical approach will lead to a rapid increase in game score and consequently earlier triggering of built-in game mechanics such as pirate invasions. This earlier triggering may lead to a shorter survival duration (i.e., fewer turns played after the invasion) compared to BaseLang. However, even with a reduction in survival time post-trigger, the overall strategic performance and rapid expansion gains of Mastaba may validate its hierarchical decision framework. The trade-off between quick expansion and vulnerability to in-game penalties will be quantified using survival analysis and correlation statistics based on the logged metrics.",
            "subsection_source": "5.2 LANGUAGE-BASED AGENTS : BASELANG AND MASTABA"
        }
    ],
    "follow_up_work_ideas": [
        {
            "idea": "Integrate hierarchical reinforcement learning (HRL) components to address long-term strategic planning challenges in tasks with delayed rewards.",
            "experiment_design": "Develop an augmented version of the tensor-based RL model that introduces a hierarchical decision-making layer operating over multiple timescales. Train both the hierarchical model and the original flat model on minitasks with sparse/delayed rewards as well as full game environments. Use identical training durations (e.g., 40,000 steps for minitasks and 2 million steps for full games) and track comparable metrics such as success rate, policy loss, and overall game score. Perform statistical comparisons to determine if the HRL approach offers statistically significant improvements in handling long-term strategic planning.",
            "subsection_source": "5.1 T ENSOR -BASED REINFORCEMENT LEARNING"
        },
        {
            "idea": "Systematically explore different reward shaping configurations to balance immediate rewards with long-term strategic gains.",
            "experiment_design": "Design a grid search over reward parameters that adjust the relative weight between short-term achievements (like unit production) and long-term goals (such as establishing cities for exponential growth). For each configuration, train the tensor-based RL model on full game tasks for 2 million steps using multiple seeds. Collect detailed performance metrics including the weighted game score, sub-component stats (population, technology, units built/killed), and the evolution of these metrics over time. Analyze which configurations lead to lower policy loss and higher cumulative scores, thereby identifying optimal reward shaping strategies that foster balanced strategic behavior.",
            "subsection_source": "5.1 T ENSOR -BASED REINFORCEMENT LEARNING"
        },
        {
            "idea": "Integrate an explicit defensive module into the hierarchical decision architecture to mitigate the vulnerability to in-game mechanics like pirate invasions.",
            "experiment_design": "Develop an enhanced variant of Mastaba that combines the current hierarchical framework with a specialized sub-module focused on defensive strategies. Run the same set of experiments (5 full games on 10 maps) and record the same metrics: population, military units, scores, expansion progress, invasion trigger timing, and survival duration. Compare the performance of this enhanced agent against the original Mastaba and BaseLang to evaluate if the added defensive capability can maintain strategic advantages while improving survival outcomes.",
            "subsection_source": "5.2 L ANGUAGE -BASED AGENTS : BASELANG AND MASTABA"
        },
        {
            "idea": "Study the robustness of hierarchical versus non-hierarchical decision-making under varying game-mechanic conditions.",
            "experiment_design": "Conduct experiments where the built-in game mechanics\u2014such as the score threshold that triggers pirate invasions\u2014are systematically varied. Maintain the experimental framework (using both Mastaba and BaseLang on multiple maps) while altering the invasion trigger thresholds across different game runs. Analyze how changes in these thresholds affect the agents\u2019 expansion rate, overall score, and survival time. This variation will help assess the sensitivity of each agent to environmental changes and further elucidate the strengths and limitations of a hierarchical decision architecture in dynamic settings.",
            "subsection_source": "5.2 L ANGUAGE -BASED AGENTS : BASELANG AND MASTABA"
        }
    ],
    "main_takeaways": [
        "The paper presents a novel approach that integrates zero\u2010shot planning and language-based reasoning with reinforcement learning for embodied agents.",
        "It demonstrates that combining tensor\u2010based reinforcement learning with language-informed decision making can lead to improvements in key performance metrics such as rewards, scores, and success rates across a variety of simulated environments (including battle and development scenarios).",
        "The evaluation highlights that metrics such as policy loss, value loss, and step reward are informative signals that closely correspond with the agent\u2019s overall performance and learning stability.",
        "Comparisons with a baseline language-based agent (referred to as BaseLang) show that the proposed method can achieve faster convergence and better overall outcomes in complex tasks.",
        "The experiments conducted\u2014spanning scenarios like battle modern era, battle naval, development build city, and others\u2014provide granular evidence that configuration choices and loss function tuning can substantially affect performance."
    ]
}