{
  "requirements": [
    "Step 1: Import necessary libraries including gymnasium, civrealm, ray, and other utilities for logging and configuration (/workspace/src/civrealm/random_game_minitask.py:16-22)",
    "Step 2: Set up the experiment configuration with parameters for minitask patterns, batch size, and maximum training steps (/workspace/src/civrealm/runners/parallel_tensor_runner.py:12-24)",
    "Step 3: Initialize the TensorBoard logger to track experiment metrics (/workspace/src/civrealm/runners/parallel_tensor_runner.py:13-17)",
    "Step 4: Create the tensor-based minitask environment with appropriate wrappers for reward handling (/workspace/src/civrealm/envs/freeciv_tensor_minitask_env.py:26-51)",
    "Step 5: Configure the environment to use either immediate rewards (MinitaskDenseReward) or delayed/sparse rewards (MinitaskDelayedReward) based on the minitask type (/workspace/src/civrealm/envs/freeciv_wrapper/reward_wrapper.py:37-72)",
    "Step 6: Run training for each minitask configuration with multiple random seeds (/workspace/src/civrealm/runners/parallel_tensor_runner.py:32-43)",
    "Step 7: Collect and store metrics including success rates for each minitask type (/workspace/src/civrealm/envs/freeciv_minitask_env.py:233-245)",
    "Step 8: Parse the TensorBoard logs to extract performance metrics for each minitask type (/workspace/src/civrealm/envs/freeciv_minitask_env.py:123-135)",
    "Step 9: Generate visualizations comparing performance between immediate reward minitasks and sparse/delayed reward minitasks (/workspace/src/civrealm/random_game_minitask.py:46-49)",
    "Final Step: Analyze the results to determine the impact of reward structure (immediate vs. delayed) on learning performance (/workspace/src/civrealm/envs/freeciv_minitask_env.py:216-225)"
  ],
  "agent_instructions": "Create a system to compare the performance of tensor-based reinforcement learning models on different types of minitasks in the Freeciv game environment. The system should:\n\n1. Set up an experiment that compares performance on two categories of minitasks:\n   - Minitasks with immediate rewards (like development_citytile_wonder)\n   - Minitasks with sparse/delayed rewards (like battle_* minitasks)\n\n2. Create a script that:\n   - Configures the Freeciv environment with appropriate parameters\n   - Sets up TensorBoard logging to track metrics\n   - Runs training on both types of minitasks with multiple random seeds\n   - Uses a consistent training duration (around 40,000 steps)\n\n3. Create a second script that:\n   - Parses the TensorBoard logs after training is complete\n   - Extracts success rates and other relevant metrics for each minitask type\n   - Generates visualizations comparing performance between immediate reward tasks and delayed reward tasks\n   - Saves the plots to a designated directory\n\nThe goal is to analyze whether the RL model performs better on tasks with immediate feedback versus those with delayed rewards.",
  "masked_source": [
    "/workspace/src/civrealm/random_game_minitask.py",
    "/workspace/src/civrealm/envs/freeciv_tensor_minitask_env.py",
    "/workspace/src/civrealm/envs/freeciv_wrapper/reward_wrapper.py",
    "/workspace/src/civrealm/envs/freeciv_minitask_env.py",
    "/workspace/src/civrealm/runners/parallel_tensor_runner.py"
  ]
}