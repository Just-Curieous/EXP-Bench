[
  {
    "mode": "B",
    "question": "Implement a Time Delay Neural Network (TDNN) model for speech recognition on the yesno dataset that achieves a Word Error Rate (WER) of less than 1%.",
    "method": "Create a TDNN model that processes audio features and outputs log probabilities for phoneme classes. The model should be trained using CTC loss on the yesno dataset, which consists of recordings of someone saying 'yes' or 'no' in various combinations.",
    "expected_outcome": "A trained model that achieves a WER of less than 1% on the test set. The expected output should be similar to: [test_set] %WER 0.42% [1 / 240, 0 ins, 1 del, 0 sub]",
    "source": [
      "/workspace/egs/yesno/ASR/tdnn/model.py",
      "/workspace/egs/yesno/ASR/tdnn/train.py",
      "/workspace/egs/yesno/ASR/tdnn/decode.py",
      "/workspace/egs/yesno/ASR/tdnn/asr_datamodule.py"
    ],
    "usage_instructions": "1. Implement a TDNN model with a structure similar to the one in model.py, consisting of three Conv1d layers with ReLU activations and BatchNorm, followed by a linear output layer.\n2. Set up the training process using CTC loss as shown in train.py, with appropriate hyperparameters (learning rate, batch size, etc.).\n3. Create a data module to load and preprocess the yesno dataset, extracting fbank features from the audio.\n4. Train the model for a sufficient number of epochs (around 15) to achieve convergence.\n5. Implement a decoding function that converts the model's output probabilities to word sequences.\n6. Evaluate the model on the test set and report the Word Error Rate (WER)."
  },
  {
    "mode": "B",
    "question": "Create a speech recognition inference script that can transcribe audio files containing 'yes' and 'no' utterances using a pre-trained model.",
    "method": "Develop a script that loads a pre-trained TDNN model, processes input audio files to extract features, runs inference to get log probabilities, and decodes the output to produce transcriptions of 'yes' and 'no' sequences.",
    "expected_outcome": "A script that can take audio files as input and output the transcribed text. For example, given an audio file with the sequence 'no no no yes no no no yes', the script should output exactly that text.",
    "source": [
      "/workspace/egs/yesno/ASR/tdnn/pretrained.py",
      "/workspace/egs/yesno/ASR/tdnn/model.py"
    ],
    "usage_instructions": "1. Create a script that accepts command-line arguments for the checkpoint path, HLG graph, words file, and input audio files.\n2. Load the pre-trained TDNN model from the checkpoint.\n3. Implement audio processing to convert input audio files to features (using kaldifeat for Fbank extraction).\n4. Run the model on the extracted features to get log probabilities.\n5. Use the HLG graph and k2 library functions to decode the output probabilities into word sequences.\n6. Print the transcription results for each input audio file."
  },
  {
    "mode": "A",
    "question": "Implement a speech recognition system using the Transducer architecture on the yesno dataset and compare its performance with the TDNN-CTC model.",
    "method": "Use the existing Transducer framework in icefall to build a speech recognition system for the yesno dataset. The system should consist of an encoder network, a decoder (prediction) network, and a joiner network. Train the model using the RNN-T loss function and evaluate its performance.",
    "expected_outcome": "A trained Transducer model that achieves comparable or better performance than the TDNN-CTC model on the yesno dataset. The evaluation should include WER metrics and a comparison with the TDNN-CTC approach.",
    "source": [
      "/workspace/egs/yesno/ASR/transducer/train.py",
      "/workspace/egs/yesno/ASR/transducer/decode.py",
      "/workspace/egs/yesno/ASR/transducer/model.py"
    ],
    "usage_instructions": "1. Use the existing Transducer class from model.py which combines an encoder, decoder, and joiner.\n2. Configure the training process with appropriate hyperparameters for the Transducer model.\n3. Train the model on the yesno dataset using the RNN-T loss function from torchaudio.\n4. Implement beam search decoding for the Transducer model to convert its outputs to word sequences.\n5. Evaluate the model on the test set and calculate the WER.\n6. Compare the results with the TDNN-CTC model in terms of accuracy and efficiency."
  }
]