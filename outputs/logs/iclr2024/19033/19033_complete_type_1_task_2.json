{
  "questions": [
    {
      "hypothesis": "LLMs fine-tuned on instruction formats conforming to a specific order (e.g., 'Answer <question> with <answer>') will achieve high performance when the evaluation prompt matches that order, yet will perform poorly if the evaluation prompt order is reversed (e.g., 'Q: <question> A:').",
      "method": "Prepare two datasets of 1100 question-answer pairs each. One dataset (QuestionToAnswer) uses instructions in the form 'Answer <question> with <answer>' and the other (AnswerToQuestion) uses instructions in the form 'Answer with <answer> when you see <question>'. Additionally, include a subset of direct Q-A examples in the format 'Q: <question> A: <answer>' to facilitate generalization from instruction to example. Finetune separate model instances (e.g., Llama-7b, Llama-13b, Llama-30b) for 20 epochs using the best hyperparameters obtained from a GPT-3-350M hyperparameter sweep, with five different seeds per configuration. At evaluation, use a fixed prompt formatted as 'Q: <question> A:' and include three correct in-context examples (3-shot prompting) with temperature set to zero. Compare performance on held-out test sets when the training instruction order matches the test prompt (same direction) versus when it is reversed. Relevant performance details (e.g., same direction accuracy above 80% and reverse direction near chance level around 7%) can be cross-referenced with results reported in similar tasks (see, for instance, Figure 6 and Table 3 in the related document).",
      "expected_outcome": "Models are expected to achieve high exact-match accuracy (above 80%) on the test examples when the instruction order used during training aligns with the order in the evaluated prompt. When the training and evaluation orders are reversed, the accuracy is anticipated to drop to near chance levels (around or below 7%), emphasizing the crucial role of matching instruction and example order during both training and inference.",
      "subsection_source": "2 EXPERIMENTS AND RESULTS",
      "source": [
        "/workspace/scripts/instructions/create_qa_dataset.py",
        "/workspace/scripts/instructions/start_finetunes.py",
        "/workspace/scripts/evaluate_quickly.py"
      ],
      "usage_instructions": "1. First, create two datasets: one with the QuestionToAnswer format using the default guidance phrasings file (qa_guidance_simple.txt) and another with the AnswerToQuestion format using the reverse guidance phrasings file (qa_guidance_reverse.txt).\n\nFor QuestionToAnswer format:\n```\npython scripts/instructions/create_qa_dataset.py --task copypaste --realized-guidance-size 1000 --unrealized-guidance-size 100 --guidance-size-range 2,5 --n-unrealized-guidance-phrasings 0 --upsample-examples-factor 1 --upsample-guidances-factor 1 --suffix questiontoanswer --subdir instructions --guidance-phrasings-filename qa_guidance_simple.txt\n```\n\nFor AnswerToQuestion format:\n```\npython scripts/instructions/create_qa_dataset.py --task copypaste --realized-guidance-size 1000 --unrealized-guidance-size 100 --guidance-size-range 2,5 --n-unrealized-guidance-phrasings 0 --upsample-examples-factor 1 --upsample-guidances-factor 1 --suffix answertoquestion --subdir instructions --guidance-phrasings-filename qa_guidance_reverse.txt\n```\n\n2. Fine-tune models on each dataset with multiple seeds (e.g., 5 seeds per configuration) using the best hyperparameters. For each dataset:\n```\npython scripts/instructions/start_finetunes.py --model_name llama-7b --learning_rate_multiplier 0.2 --batch_size 2 --n_epochs 20 --dataset_name copypaste_ug100_rg1000_questiontoanswer --num_finetunes 5\n```\n\nRepeat for llama-13b and llama-30b models, and for both the questiontoanswer and answertoquestion datasets.\n\n3. Evaluate the fine-tuned models using the evaluation script with the 'qa' evaluator. This will test the models on the held-out test sets with the fixed prompt format 'Q: <question> A:':\n```\npython scripts/evaluate_quickly.py --wandb-entity your_wandb_username --wandb-project your_project --evaluator qa\n```\n\nThe evaluation will show the performance difference between models trained on matching instruction order versus reversed order, demonstrating the reversal curse phenomenon described in the experiment question.",
      "requirements": [
        "Step 1: Load and parse command line arguments for dataset creation, including task type, guidance sizes, suffix, and guidance phrasings filename (/workspace/scripts/instructions/create_qa_dataset.py:138-144)",
        "Step 2: Load guidance phrasings from the specified file (simple or reverse format) (/workspace/src/tasks/qa/qa.py:110-118)",
        "Step 3: Load question-answer pairs from source data file (/workspace/src/tasks/qa/qa_copypaste.py:140-145)",
        "Step 4: Split data into realized (training) and unrealized (testing) sets based on specified sizes (/workspace/src/tasks/qa/qa_copypaste.py:149-151)",
        "Step 5: Create QA items with question-answer pairs (/workspace/src/tasks/qa/qa_copypaste.py:156-158)",
        "Step 6: Format guidance documents and examples using the loaded phrasings (/workspace/src/tasks/qa/qa_copypaste.py:161-166)",
        "Step 7: Create guidance documents by grouping multiple guidances together (/workspace/src/tasks/qa/qa_copypaste.py:70-89)",
        "Step 8: Create example documents for both realized and unrealized examples (/workspace/src/tasks/qa/qa_copypaste.py:168-170)",
        "Step 9: Save the dataset files (training, testing, and debug data) (/workspace/src/tasks/qa/qa_copypaste.py:102-128)",
        "Step 10: Parse command line arguments for fine-tuning, including model name, learning rate, batch size, epochs, dataset name, and number of fine-tunes (/workspace/scripts/instructions/start_finetunes.py:13-23)",
        "Step 11: Calculate training cost and confirm with user before proceeding (/workspace/src/openai_finetune.py:46-47)",
        "Step 12: Submit multiple fine-tuning jobs with the specified parameters (/workspace/src/openai_finetune.py:50-59)",
        "Step 13: Parse command line arguments for evaluation, including wandb entity, project, and evaluator type (/workspace/scripts/evaluate_quickly.py:20-27)",
        "Step 14: Initialize the QA evaluator with the specified parameters (/workspace/scripts/evaluate_quickly.py:10)",
        "Step 15: Fetch model IDs from wandb or use the specified model ID (/workspace/scripts/evaluate_quickly.py:34-39)",
        "Final Step: Run evaluation on each model and report results (/workspace/scripts/evaluate_quickly.py:41-43)"
      ],
      "agent_instructions": "You need to implement a system to demonstrate the reversal curse phenomenon in language models through a question-answering experiment. The experiment involves three main components:\n\n1. Dataset Creation:\n   - Create a dataset generator that produces two formats of question-answer pairs:\n     a) QuestionToAnswer format: Standard format where models are trained to answer questions\n     b) AnswerToQuestion format: Reversed format where the order is flipped\n   - The dataset should include:\n     - Training data (realized examples) with guidance phrasings\n     - Testing data (unrealized examples) for evaluation\n   - Support command-line arguments for customizing dataset parameters (guidance sizes, phrasings file, etc.)\n\n2. Model Fine-tuning:\n   - Implement a script to fine-tune language models on the created datasets\n   - Support multiple fine-tuning runs with different random seeds\n   - Allow specification of model size, learning rate, batch size, and number of epochs\n   - Calculate and display the estimated cost before proceeding with fine-tuning\n\n3. Evaluation:\n   - Create an evaluation script that tests models on the held-out test sets\n   - Use a fixed prompt format 'Q: <question> A:' for evaluation\n   - Track and report performance metrics using Weights & Biases\n   - Support evaluating multiple models in a single run\n\nThe system should demonstrate that models trained on the standard QuestionToAnswer format perform well on test questions, while models trained on the AnswerToQuestion format struggle with the standard question format, illustrating the reversal curse.",
      "masked_source": [
        "/workspace/scripts/instructions/create_qa_dataset.py",
        "/workspace/scripts/instructions/start_finetunes.py",
        "/workspace/scripts/evaluate_quickly.py"
      ]
    }
  ]
}