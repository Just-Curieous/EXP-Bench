{
    "source": [
        "/workspace/scripts/instructions/create_qa_dataset.py",
        "/workspace/scripts/instructions/start_finetunes.py",
        "/workspace/scripts/evaluate_quickly.py"
    ],
    "usage_instructions": "1. First, create two datasets: one with the QuestionToAnswer format using the default guidance phrasings file (qa_guidance_simple.txt) and another with the AnswerToQuestion format using the reverse guidance phrasings file (qa_guidance_reverse.txt).\n\nFor QuestionToAnswer format:\n```\npython scripts/instructions/create_qa_dataset.py --task copypaste --realized-guidance-size 1000 --unrealized-guidance-size 100 --guidance-size-range 2,5 --n-unrealized-guidance-phrasings 0 --upsample-examples-factor 1 --upsample-guidances-factor 1 --suffix questiontoanswer --subdir instructions --guidance-phrasings-filename qa_guidance_simple.txt\n```\n\nFor AnswerToQuestion format:\n```\npython scripts/instructions/create_qa_dataset.py --task copypaste --realized-guidance-size 1000 --unrealized-guidance-size 100 --guidance-size-range 2,5 --n-unrealized-guidance-phrasings 0 --upsample-examples-factor 1 --upsample-guidances-factor 1 --suffix answertoquestion --subdir instructions --guidance-phrasings-filename qa_guidance_reverse.txt\n```\n\n2. Fine-tune models on each dataset with multiple seeds (e.g., 5 seeds per configuration) using the best hyperparameters. For each dataset:\n```\npython scripts/instructions/start_finetunes.py --model_name llama-7b --learning_rate_multiplier 0.2 --batch_size 2 --n_epochs 20 --dataset_name copypaste_ug100_rg1000_questiontoanswer --num_finetunes 5\n```\n\nRepeat for llama-13b and llama-30b models, and for both the questiontoanswer and answertoquestion datasets.\n\n3. Evaluate the fine-tuned models using the evaluation script with the 'qa' evaluator. This will test the models on the held-out test sets with the fixed prompt format 'Q: <question> A:':\n```\npython scripts/evaluate_quickly.py --wandb-entity your_wandb_username --wandb-project your_project --evaluator qa\n```\n\nThe evaluation will show the performance difference between models trained on matching instruction order versus reversed order, demonstrating the reversal curse phenomenon described in the experiment question."
}