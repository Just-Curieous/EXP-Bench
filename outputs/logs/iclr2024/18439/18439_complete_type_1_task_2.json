{
  "questions": [
    {
      "question": "Can the smooth reliability diagram reliably reflect calibration quality across diverse domains such as deep learning (using ImageNet validation data), solar flare forecasting (731 days, 25.7% event rate), and precipitation prediction (346 days of daily forecasts with discrete probabilities in 10% increments) when using a principled kernel smoothing method, as implemented in relplot? In your analysis, consider whether the automatic kernel bandwidth selection effectively adapts to the prediction distributions encountered in these domains.",
      "method": "Select three datasets: (i) the ImageNet validation set for deep networks, (ii) the solar flare forecasting dataset with 731 days and a known event occurrence rate of 25.7%, and (iii) the daily rain forecasts dataset from the Finnish Meteorological Institute covering 346 days with discrete forecasts in 10% increments. For each dataset, generate two reliability diagrams using the relplot package: one using the traditional binned approach (via relplot.rel_diagram_binned) and one using the smooth reliability diagram (via relplot.rel_diagram) which employs automatic kernel bandwidth selection. When generating the smooth reliability diagrams, either use the default automatic bandwidth or override it with a domain-appropriate kernel bandwidth \u03c3* if necessary, ensuring that the kernel density estimates of predictions are clearly visualized. For the solar flare and precipitation datasets, verify that the plotting functions correctly handle the unique prediction distributions (e.g., the discrete probability increments in the precipitation dataset). Additionally, if desired, compute calibration error metrics using rp.smECE and assess the consistency between the conditional mean estimates and density estimates. Evaluate the diagrams in terms of their visual clarity, consistency, and adherence to the [0,1] boundaries.",
      "expected_outcome": "It is expected that across the three diverse domains, the smooth reliability diagrams will consistently display improved visual interpretability and robust calibration error assessment, with clear density estimates and proper handling of boundary constraints, while the binned methods may show instability or less interpretable aggregation, particularly in cases with non-uniform or discrete probability outputs.",
      "subsection_source": "4 E XPERIMENTS",
      "source": [
        "/workspace/notebooks/paper_experiments.ipynb"
      ],
      "usage_instructions": "To analyze how smooth reliability diagrams reflect calibration quality across diverse domains (deep learning with ImageNet, solar flare forecasting, and precipitation prediction), run the paper_experiments.ipynb notebook. The notebook contains three sections that address the experiment question:\n\n1. 'DNN calibration' section (cell 10) - Uses ImageNet validation data for deep networks\n2. 'Solar Flares' section (cell 13) - Uses solar flare forecasting dataset with 731 days and 25.7% event rate\n3. 'Rain in Finland' section (cell 15) - Uses daily rain forecasts with discrete probabilities in 10% increments\n\nEach section uses the plot_compare() function (defined in cell 8) which generates two reliability diagrams side by side: one using the traditional binned approach (via relplot.rel_diagram_binned) and one using the smooth reliability diagram (via relplot.rel_diagram) with automatic kernel bandwidth selection. The function also computes calibration error metrics.\n\nThe notebook demonstrates that the smooth reliability diagram effectively adapts to different prediction distributions across these diverse domains, providing clearer visual interpretability compared to binned methods.",
      "requirements": [
        "Step 1: Import necessary libraries including matplotlib, pandas, numpy, scipy, and sklearn for data manipulation and visualization (/workspace/notebooks/paper_experiments.ipynb:9-22)",
        "Step 2: Configure the plotting style for reliability diagrams (/workspace/notebooks/paper_experiments.ipynb:32-33)",
        "Step 3: Define a plot_compare function that creates two side-by-side reliability diagrams (binned vs. smooth) for a given set of predictions and ground truth labels (/workspace/notebooks/paper_experiments.ipynb:43-62)",
        "Step 4: For the DNN calibration experiment, load ImageNet validation data from external URLs, extract prediction confidences and binary correctness labels, and generate reliability diagrams (/workspace/notebooks/paper_experiments.ipynb:80-86, 114)",
        "Step 5: For the Solar Flares experiment, load solar flare forecasting data from an external URL, extract DAFFS predictions and ground truth labels, and generate reliability diagrams (/workspace/notebooks/paper_experiments.ipynb:8-13 in cell 8)",
        "Step 6: For the Rain in Finland experiment, load precipitation data from an external URL, process it to extract binary rain events and forecast probabilities, and generate reliability diagrams (/workspace/notebooks/paper_experiments.ipynb:10-14 in cell 10)",
        "Step 7: For each experiment, use the plot_compare function which automatically determines the optimal number of bins for the binned diagram and uses automatic kernel bandwidth selection for the smooth diagram (/workspace/notebooks/paper_experiments.ipynb:52-57)",
        "Step 8: For each experiment, display two reliability diagrams side by side - one using traditional binning (rel_diagram_binned) and one using smooth reliability diagram (rel_diagram) (/workspace/notebooks/paper_experiments.ipynb:57-61)",
        "Step 9: For each reliability diagram, compute and display calibration error metrics (/workspace/src/relplot/diagrams.py:101, 194-264)"
      ],
      "agent_instructions": "Create a notebook that demonstrates how smooth reliability diagrams reflect calibration quality across three diverse domains compared to traditional binned reliability diagrams. The notebook should include the following experiments:\n\n1. Deep Neural Network (DNN) calibration using ImageNet validation data\n2. Solar flare forecasting using a dataset with 731 days and 25.7% event rate\n3. Daily rain forecasts in Finland with discrete probabilities in 10% increments\n\nFor each experiment, you should:\n- Load the appropriate dataset (ImageNet validation data, solar flare forecasting data, or rain forecast data)\n- Extract prediction probabilities/confidences and ground truth labels\n- Create a function that generates two reliability diagrams side by side:\n  - A traditional binned reliability diagram on the left\n  - A smooth reliability diagram with automatic kernel bandwidth selection on the right\n- Compute calibration error metrics for both approaches\n\nThe goal is to demonstrate that the smooth reliability diagram effectively adapts to different prediction distributions across these diverse domains, providing clearer visual interpretability compared to binned methods. Your implementation should handle the automatic selection of appropriate binning and smoothing parameters based on the data characteristics.",
      "masked_source": [
        "/workspace/notebooks/paper_experiments.ipynb",
        "/workspace/src/relplot/__init__.py",
        "/workspace/src/relplot/diagrams.py",
        "/workspace/src/relplot/metrics.py",
        "/workspace/src/relplot/estimators.py",
        "/workspace/src/relplot/kernels.py",
        "/workspace/src/relplot/config.py"
      ]
    }
  ]
}