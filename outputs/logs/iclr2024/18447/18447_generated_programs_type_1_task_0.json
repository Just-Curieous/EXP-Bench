{
    "source": ["/workspace/train.py", "/workspace/validate.py"],
    "usage_instructions": "To train MogaNet variants on ImageNet-1K for 300 epochs with AdamW optimizer and cosine scheduler, run the following command for MogaNet-T at 224x224 resolution:\n\npython -m torch.distributed.launch --nproc_per_node=8 train.py \\\n--model moganet_tiny --img_size 224 --drop_path 0.1 \\\n--epochs 300 --batch_size 128 --lr 1e-3 --weight_decay 0.04 \\\n--aa rand-m7-mstd0.5-inc1 --crop_pct 0.9 --mixup 0.1 \\\n--amp --native_amp \\\n--data_dir /path/to/imagenet-1k \\\n--experiment /path/to/save_results\n\nFor MogaNet-T at 256x256 resolution, change --img_size to 256.\n\nFor MogaNet-XT, use --model moganet_xtiny --drop_path 0.05 --weight_decay 0.03.\n\nAfter training, validate the model with:\n\npython validate.py \\\n--model moganet_tiny --img_size 224 --crop_pct 0.9 \\\n--data_dir /path/to/imagenet-1k \\\n--checkpoint /path/to/checkpoint.tar.gz\n\nThe training script uses AdamW optimizer with a base learning rate of 1e-3 and a cosine scheduler by default, matching the experiment requirements."
}