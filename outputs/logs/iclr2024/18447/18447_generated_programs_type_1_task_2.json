{
    "no_answer": "After thoroughly examining the repository, I could not find specific scripts that directly implement the experiment described in the question. The repository contains code for training MogaNet-XL on ImageNet-1K for 300 epochs (as seen in the model configurations and training scripts), but there are no specific scripts for pre-training MogaNet-XL on ImageNet-21K for 90 epochs followed by fine-tuning on ImageNet-1K for 30 epochs. The TRAINING.md file mentions that ImageNet-21K training is available in the OpenMixup repository, but the specific implementation for this experiment is not present in the current repository. The repository seems to focus on the ImageNet-1K training scenario mentioned in the experiment question, but not the pre-training and fine-tuning workflow."
}