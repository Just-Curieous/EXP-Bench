[
  {
    "mode": "A",
    "question": "How can you use the FuseLLM-7B model to generate text completions?",
    "method": "Load the FuseLLM-7B model using the Hugging Face Transformers library and generate text completions for a given input.",
    "expected_outcome": "The model should successfully load and generate a coherent text completion for the provided input.",
    "source": ["/workspace/FuseLLM/README.md"],
    "usage_instructions": "1. Import the necessary libraries from transformers (AutoTokenizer, AutoModelForCausalLM).\n2. Load the tokenizer for 'Wanfq/FuseLLM-7B' with use_fast=False.\n3. Load the model with torch_dtype='auto'.\n4. Move the model to CUDA.\n5. Tokenize the input text and move it to the model's device.\n6. Generate tokens with the specified parameters (max_new_tokens=512, temperature=0.6, top_p=0.9, do_sample=True).\n7. Decode the generated tokens and print the result."
  },
  {
    "mode": "A",
    "question": "How can you use the FuseChat-7B-v2.0 model for single-turn and multi-turn conversations?",
    "method": "Load the FuseChat-7B-v2.0 model using the Transformers library and demonstrate how to format inputs for both single-turn and multi-turn conversations.",
    "expected_outcome": "The model should correctly tokenize both single-turn and multi-turn conversation formats, producing the expected token IDs.",
    "source": ["/workspace/FuseChat/README.md"],
    "usage_instructions": "1. Import the transformers library.\n2. Load the tokenizer for 'FuseAI/FuseChat-7B-v2.0'.\n3. Demonstrate single-turn conversation by tokenizing a string with the format 'GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant:'.\n4. Verify the token IDs match the expected output.\n5. Demonstrate multi-turn conversation by tokenizing a string with multiple turns.\n6. Verify the token IDs match the expected output.\n7. Show how to use the chat_template feature by creating a messages list with user and assistant roles.\n8. Apply the chat template with add_generation_prompt=True and verify the tokens match the expected output."
  },
  {
    "mode": "A",
    "question": "How can you use the FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview model to solve a mathematical reasoning problem?",
    "method": "Load the FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview model using VLLM and generate a step-by-step solution for a mathematical problem.",
    "expected_outcome": "The model should provide a detailed step-by-step solution to the quadratic polynomial problem, with the final answer enclosed in \\boxed{}.",
    "source": ["/workspace/FuseO1-Preview/README.md"],
    "usage_instructions": "1. Import the necessary libraries from vllm (LLM, SamplingParams).\n2. Initialize the LLM with the model 'FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview' and tensor_parallel_size=8.\n3. Set up sampling parameters with max_tokens=32768, temperature=0.7, and appropriate stop tokens.\n4. Create a conversation list with a system message instructing to reason step by step and put the final answer in \\boxed{}.\n5. Add a user message containing the quadratic polynomial problem.\n6. Call the chat method on the LLM with the conversation and sampling parameters.\n7. Print the model's response."
  },
  {
    "mode": "A",
    "question": "How can you evaluate a language model on the MT-Bench benchmark using VLLM?",
    "method": "Use the MT-Bench evaluation script to generate answers from a language model using VLLM and evaluate its performance.",
    "expected_outcome": "The script should successfully generate model answers for the MT-Bench questions and save them to a JSON file for later evaluation.",
    "source": ["/workspace/FuseChat-3.0/FuseEval/IF-Eval/MT-Bench/gen_model_answer_vllm.py", "/workspace/FuseChat-3.0/FuseEval/IF-Eval/MT-Bench/generate_vllm.sh"],
    "usage_instructions": "1. Set the VLLM_WORKER_MULTIPROC_METHOD environment variable to 'spawn'.\n2. Specify the model directory, model name, total number of GPUs, and GPUs per model.\n3. Run the gen_model_answer_vllm.py script with the appropriate parameters.\n4. The script will load the questions from the question file.\n5. For each question, it will generate responses using the specified model.\n6. The script handles both single-turn and multi-turn conversations by applying the appropriate chat template.\n7. The generated answers are saved to a JSON file in the specified output directory.\n8. The answers are organized by question ID for later evaluation."
  }
]