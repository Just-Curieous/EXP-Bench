{
    "questions": [
        {
            "question": "Does the iterative prompt optimization process improve training accuracy and stabilize instruction quality on GSM8K? (This experiment uses the GSM8K dataset as implemented in the repository's prompt optimization pipeline.)",
            "method": "Using the GSM8K dataset, randomly sample 3.5% of the training examples to create a fixed subset throughout the optimization process. Begin with the initial instruction 'Let\u2019s solve the problem' (approximate training accuracy ~60.5%). Use the PaLM 2-L-IT as the optimizer and a pre-trained PaLM 2-L as the scorer with a temperature setting of 0 for evaluations. At each optimization step, generate 8 candidate instructions using a meta-prompt that includes the best 20 instructions so far along with 3 randomly sampled exemplars from the task. Record the training accuracy and the variance among the generated instructions at every step. Visualize the optimization process by plotting the accuracy curve to capture the overall upward trend in accuracy and a decrease in variance over time. (Note: Refer to the repository\u2019s guidelines regarding potential API costs and consider starting with lighter configurations for initial exploratory experiments.)",
            "expected_outcome": "It is expected that the iterative prompt optimization will progressively increase the training accuracy from the initial ~60.5% through several leaps, while also reducing the variance among the generated instructions. This would confirm that the iterative process effectively refines the instructions, ultimately stabilizing instruction quality.",
            "subsection_source": "5.2 MAIN RESULTS"
        },
        {
            "question": "Do the optimized instructions discovered by OPRO outperform the baseline instructions ('Let's think step by step.' and the empty prompt) on BBH tasks?",
            "method": "For the BBH evaluation, select 23 distinct tasks. Use 20% of each task\u2019s examples for prompt optimization (starting from an empty instruction) and reserve the remaining 80% for testing. Evaluate prompt optimization by employing both the PaLM 2-L-IT optimizer and text-bison as scorer in different configurations. Compare the final training accuracy of the optimized instructions against baseline prompts\u2014specifically 'Let's think step by step.' and the empty instruction\u2014across all tasks. Analyze the per-task accuracy differences, expecting that in at least 19 or 20 out of the 23 tasks, the optimized instructions outperform the baselines by more than 5%.",
            "expected_outcome": "Based on the reported results, the optimized instructions should exceed the performance of the baselines by over 5% in the majority of the tasks, confirming that OPRO produces superior prompts compared to standard or naive starting instructions.",
            "subsection_source": "5.2 M AINRESULTS"
        },
        {
            "question": "Are the optimized prompts found on GSM8K transferable to similar tasks in the same domain, such as MultiArith and AQuA? Specifically, will applying these GSM8K-optimized prompt instructions lead to performance improvements on these benchmarks compared to their baseline prompts?",
            "method": "Use the prompt optimization pipeline as described for GSM8K (for instance, by running the optimization script with options similar to those in the repository, such as using a scorer like text-bison) to obtain the top prompt instructions on GSM8K. Then, directly apply these optimized prompts to the MultiArith and AQuA datasets. Evaluate performance using pre-trained scoring models (e.g., text-bison as supported by the repository) with the same evaluation methodology as used for GSM8K (for example, via the evaluation script). Record detailed accuracy scores for both the training and test splits, and perform a comparative analysis against the baseline prompts originally used on MultiArith and AQuA. Analyze percentage improvements and convergence behavior to assess the transferability of the prompt optimization.",
            "expected_outcome": "It is expected that the optimized prompts will maintain or exceed the improvements observed on GSM8K by outperforming the baseline prompts on both MultiArith and AQuA. The transferability of the prompt optimization process should be demonstrated through measurable gains in accuracy and consistent performance across these mathematical reasoning benchmarks, in line with the results presented in Table 4.",
            "subsection_source": "5.2 MAIN RESULTS"
        },
        {
            "question": "Does the order of instructions in the meta-prompt (i.e., lowest to highest vs. highest to lowest vs. random) affect the final accuracy and convergence speed of prompt optimization for tasks such as GSM8K and BBH sports_understanding?",
            "method": "Use text-bison as the scorer and PaLM 2-L as the optimizer. For each task, construct three meta-prompts with different instruction orderings: (1) ascending order (lowest to highest, the default), (2) descending order (highest to lowest), and (3) random order. For GSM8K, run optimizations over 200 steps and record performance at steps 0, 100, and 200; for BBH sports_understanding, run the optimization over 1600 steps and record performance at predetermined intervals (e.g., 0, 800, and 1600 steps). Each experiment should be repeated three times to compute average accuracies and standard deviations. Additionally, leverage insights from previous studies (e.g., recency bias indicating that later instructions in the meta-prompt have a greater influence) and compare the results with baseline performance comparisons as described in the paper. To implement the experiments, you may use the prompt optimization setup described in the README by running the appropriate script (e.g., the one for prompt optimization) and ensuring that you have installed the necessary dependencies along with valid API keys.",
            "expected_outcome": "It is expected that the default incremental ordering (ascending - lowest to highest) will result in better final accuracy and faster convergence for both GSM8K and BBH sports_understanding tasks. This performance advantage is hypothesized to stem from the recency bias in the optimizer LLM, as instructions positioned later in the meta-prompt exert more influence. The results should display a clear upward trend in optimization performance when using the default ordering, with performance metrics closely matching or exceeding those shown in the baseline comparisons (e.g., as seen in Table 4).",
            "subsection_source": "5.3 A BLATION STUDIES"
        },
        {
            "question": "How does the number of generated instructions per optimization step influence the performance and stability of the prompt optimization process for datasets like GSM8K and BBH sports_understanding? In this study you will use a setup similar to the one provided by the repository (which supports text-bison as the scorer and allows use of models like PaLM 2-L as the optimizer) to investigate this relationship.",
            "method": "Employ text-bison as the scorer and PaLM 2-L as the optimizer, following a protocol similar to that used in the repository\u2019s prompt optimization pipeline (e.g., via optimize_instructions.py). Design experiments where the only variable is the number of instructions generated per optimization step by testing with values 1, 2, 4, 8, and 16. For each setting, adjust the number of optimization steps so that the overall number of evaluated instructions stays roughly constant (for example, if using 8 instructions per step, run 200 steps; scale accordingly for the other settings). For GSM8K, sample approximately 3.5% of the training set as per prior protocol and record training accuracy at intervals that reflect the cumulative number of evaluated instructions (e.g., every 50,000 instructions). Conduct parallel experiments on the BBH sports_understanding task. Analyze both the convergence speed and the final accuracy across configurations, and examine qualitative patterns in the generated prompt instructions relative to the patterns observed in related studies. Aggregate results over three independent runs to reduce random variance and assess stability.",
            "expected_outcome": "It is anticipated that generating 8 instructions per step will yield the optimal balance between reducing gradient variance and permitting a larger number of optimization iterations, thereby leading to improved convergence speed and higher final accuracy. Sampling too few instructions may cause high variance and unstable performance, whereas sampling too many may limit the number of optimization steps available, potentially leading to suboptimal integration of iterative improvements. The experimental results should align with observations from the literature (e.g., as seen in the improvement trends in Figures and tables of related studies) and provide clear quantification of both convergence behavior and overall stability.",
            "subsection_source": "5.3 A BLATION STUDIES"
        }
    ],
    "follow_up_work_ideas": [
        {
            "idea": "Investigate the impact of different optimizer LLMs on convergence speed and final prompt quality.",
            "experiment_design": "Conduct a systematic comparison across several optimizer LLMs (e.g., PaLM 2-L-IT, gpt-3.5-turbo, GPT-4) while keeping the scorer (e.g., pre-trained PaLM 2-L) constant. Run the prompt optimization process on a fixed subset of GSM8K or BBH tasks and record the number of optimization steps required to reach predefined accuracy milestones. Additionally, analyze the semantic quality and conciseness of the resulting instructions. This experiment will help understand if certain optimizers enable faster convergence or produce qualitatively different prompts.",
            "subsection_source": "5.2 M AINRESULTS"
        },
        {
            "idea": "Evaluate the robustness of optimized instructions against changes in dataset characteristics or adversarial inputs.",
            "experiment_design": "Use one or more BBH tasks and introduce controlled perturbations or noise into the test examples (such as altering phrasing or adding irrelevant context). Compare the performance drop (in training accuracy) of the optimized instructions against the baseline (e.g., 'Let's think step by step.') under these perturbed conditions. This setup should reveal how robust the optimized prompts are when exposed to data that deviates from the training distribution or follows adversarial patterns.",
            "subsection_source": "5.2 M AINRESULTS"
        },
        {
            "idea": "Investigate adaptive meta-prompt ordering strategies that dynamically adjust the instruction order based on feedback from previous optimization iterations.",
            "experiment_design": "Develop a dynamic meta-prompt that reorders instructions based on their observed contribution to the improvement of training accuracy. Compare the dynamic ordering with the fixed ascending ordering over the same tasks (GSM8K and BBH sports_understanding) using the same evaluation metrics. Assess whether adaptive ordering could further accelerate convergence and improve final accuracy.",
            "subsection_source": "5.3 A BLATION STUDIES"
        },
        {
            "idea": "Extend the ablation studies to other aspects of the optimization framework, such as the impact of different temperature settings for the optimizer and scorer LLMs or the composition of the meta-prompt (e.g., number of past best instructions included).",
            "experiment_design": "Set up experiments on GSM8K and BBH sports_understanding with varying temperature settings for the optimizer (e.g., 0.8, 1.0, 1.2) and test different sizes of the meta-prompt (e.g., top 10 versus top 20 instructions). Measure the resulting training accuracy and convergence rate. This would help in understanding sensitivity to hyperparameters and potentially in further stabilizing or enhancing performance.",
            "subsection_source": "5.3 A BLATION STUDIES"
        }
    ],
    "main_takeaways": [
        "Prompt optimization starting from an empty string can effectively discover task-specific instructions that drive high training accuracy across a diverse set of BBH tasks.",
        "Different scoring and optimization strategies (e.g., using PaLM 2-L, GPT-3.5-turbo, and text-bison scorers) yield distinct refined instructions for tasks such as boolean_expressions, causal_judgement, and date_understanding, suggesting that the choice of scorer influences the final prompts.",
        "The training accuracy curves across 23 BBH tasks show that some tasks (like disambiguation_qa and logical_deduction_seven_objects) achieve near-perfect training accuracy rapidly, whereas others require more steps, highlighting variable task difficulties and learning dynamics.",
        "The study provides concrete numerical evidence (e.g., accuracy progressions over steps for tasks such as ruin_names, sports_understanding, and formal_fallacies) that prompt optimization can be a systematic approach to tailoring prompts for improved performance.",
        "The method demonstrates the potential to generalize the optimization framework across different tasks, opening pathways to further analyze how prompt design choices correlate with downstream performance metrics."
    ]
}