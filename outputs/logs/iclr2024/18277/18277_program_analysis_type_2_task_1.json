{
  "requirements": [
    "Step 1: Import necessary libraries including cv2, llama, torch, and PIL.Image (/workspace/llama_adapter_v2_multimodal7b/demo.py:1-4)",
    "Step 2: Set up the device for computation, using CUDA if available, otherwise CPU (/workspace/llama_adapter_v2_multimodal7b/demo.py:6)",
    "Step 3: Define the path to the LLaMA directory containing model weights (/workspace/llama_adapter_v2_multimodal7b/demo.py:8)",
    "Step 4: Load the LLaMA-Adapter V2 multimodal model and its preprocessing function, specifying the model variant (BIAS-7B) and device (/workspace/llama_adapter_v2_multimodal7b/demo.py:11)",
    "Step 5: Set the model to evaluation mode (/workspace/llama_adapter_v2_multimodal7b/demo.py:12)",
    "Step 6: Format a text prompt that asks about the image using the llama.format_prompt function (/workspace/llama_adapter_v2_multimodal7b/demo.py:14)",
    "Step 7: Load an image from file using OpenCV and convert it to a PIL Image object (/workspace/llama_adapter_v2_multimodal7b/demo.py:15)",
    "Step 8: Preprocess the image using the model's preprocessing function, convert it to a tensor, add a batch dimension, and move it to the appropriate device (/workspace/llama_adapter_v2_multimodal7b/demo.py:16)",
    "Step 9: Generate a response by passing both the preprocessed image and formatted prompt to the model's generate method (/workspace/llama_adapter_v2_multimodal7b/demo.py:18)",
    "Step 10: Print the generated response that describes the image based on the prompt (/workspace/llama_adapter_v2_multimodal7b/demo.py:20)"
  ],
  "agent_instructions": "Create a Python program that demonstrates how to use the LLaMA-Adapter V2 framework to build a multimodal system that processes both images and text. Your program should:\n\n1. Set up the necessary environment and import required libraries\n2. Load the LLaMA-Adapter V2 multimodal model with appropriate parameters\n3. Prepare an image input by loading and preprocessing it\n4. Create a text prompt that asks a question about the image\n5. Pass both the image and text prompt to the model to generate a response\n6. Display the generated response\n\nThe final program should be able to take any image and text prompt as input and generate a descriptive response about the image based on the prompt."
}