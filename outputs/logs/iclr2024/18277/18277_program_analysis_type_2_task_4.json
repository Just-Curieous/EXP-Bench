{
  "requirements": [
    "Step 1: Import necessary libraries including ImageBind data module and llama module (/workspace/imagebind_LLM/demo.py:1-2, /workspace/imagebind_LLM/demo_3d.py:1-2, /workspace/imagebind_LLM/gradio_app.py:1-13)",
    "Step 2: Set up the path to the LLaMA model directory (/workspace/imagebind_LLM/demo.py:5, /workspace/imagebind_LLM/demo_3d.py:5, /workspace/imagebind_LLM/gradio_app.py:15-28)",
    "Step 3: Load the ImageBind-LLM model with the specified parameters, enabling KNN functionality for retrieval (/workspace/imagebind_LLM/demo.py:7-8, /workspace/imagebind_LLM/demo_3d.py:7-8, /workspace/imagebind_LLM/gradio_app.py:30-31)",
    "Step 4: Initialize a StableUnCLIP image generation pipeline for creating images from multimodal embeddings (/workspace/imagebind_LLM/gradio_app.py:33-34)",
    "Step 5: Create a dictionary to store inputs from different modalities (/workspace/imagebind_LLM/demo.py:10, /workspace/imagebind_LLM/demo_3d.py:10, /workspace/imagebind_LLM/gradio_app.py:60)",
    "Step 6: Load and transform inputs from different modalities (image, text, video, audio, point cloud) using the ImageBind data module, and assign weights to each modality (/workspace/imagebind_LLM/demo.py:11-14, /workspace/imagebind_LLM/demo_3d.py:11-12, /workspace/imagebind_LLM/gradio_app.py:61-95)",
    "Step 7: Format a prompt that asks a question about the multimodal inputs (/workspace/imagebind_LLM/demo.py:18, /workspace/imagebind_LLM/demo_3d.py:16, /workspace/imagebind_LLM/gradio_app.py:97-103)",
    "Step 8: Generate a response by passing both the multimodal inputs and the prompt to the model, with parameters for controlling generation length, temperature, and top-p sampling (/workspace/imagebind_LLM/demo.py:16-20, /workspace/imagebind_LLM/demo_3d.py:14-18, /workspace/imagebind_LLM/gradio_app.py:106-108)",
    "Step 9: For text output, process and display the generated text response (/workspace/imagebind_LLM/demo.py:21-22, /workspace/imagebind_LLM/demo_3d.py:19-20, /workspace/imagebind_LLM/gradio_app.py:109-110)",
    "Step 10: For image output, extract embeddings from each modality input, normalize and combine them based on their weights (/workspace/imagebind_LLM/image_generate.py:9-23)",
    "Step 11: If KNN is enabled, retrieve similar embeddings from the model's index, apply temperature scaling, and combine with the original embedding using a weighted average (/workspace/imagebind_LLM/image_generate.py:25-46)",
    "Step 12: Generate an image using the StableUnCLIP pipeline with the combined embedding and the provided prompt (/workspace/imagebind_LLM/image_generate.py:48-49, /workspace/imagebind_LLM/gradio_app.py:114)",
    "Step 13: Create a Gradio interface with input components for each modality (image, text, video, audio, point cloud) and their weights (/workspace/imagebind_LLM/gradio_app.py:142-168)",
    "Step 14: Add controls for output type selection (text or image), prompt input, and generation parameters (/workspace/imagebind_LLM/gradio_app.py:169-187)",
    "Step 15: Set up event handlers to update the interface based on user interactions and to process the multimodal inputs when the Run button is clicked (/workspace/imagebind_LLM/gradio_app.py:195-250)",
    "Final Step: Launch the Gradio demo with a queue for handling concurrent requests (/workspace/imagebind_LLM/gradio_app.py:266-271)"
  ],
  "agent_instructions": "Your task is to build a multimodal system using ImageBind-LLM that can process and reason about multiple input types (image, audio, video, text, point cloud) simultaneously. Follow these steps:\n\n1. Set up the necessary libraries including ImageBind and LLaMA modules\n2. Configure the path to the LLaMA model directory\n3. Load the ImageBind-LLM model with KNN functionality enabled\n4. Initialize a StableUnCLIP pipeline for image generation capabilities\n5. Create a system that can:\n   - Accept inputs from different modalities (image, text, video, audio, point cloud)\n   - Allow weighting of different modalities\n   - Process these inputs through the ImageBind-LLM model\n   - Generate either text responses or images based on the multimodal inputs\n6. Implement a user interface that allows:\n   - Selection of input modalities\n   - Uploading/entering content for each modality\n   - Setting weights for each modality\n   - Entering prompts/questions about the multimodal inputs\n   - Choosing between text or image output\n   - Adjusting generation parameters (length, temperature, etc.)\n\nThe final system should demonstrate ImageBind-LLM's ability to understand relationships across different modalities and generate coherent responses or images that incorporate information from all provided inputs."
}