{
    "source": ["/workspace/alpaca_finetuning_v1/finetuning.py"],
    "usage_instructions": "To run the experiment with different numbers of layers (L) for inserting adaptation prompts, use the following command for each configuration:\n\ntorchrun --nproc_per_node 8 /workspace/alpaca_finetuning_v1/finetuning.py \\\n    --model Llama7B_adapter \\\n    --llama_model_path $TARGET_FOLDER/ \\\n    --data_path $DATA_PATH/alpaca_data.json \\\n    --adapter_layer 10 \\\n    --adapter_len 10 \\\n    --max_seq_len 512 \\\n    --batch_size 4 \\\n    --epochs 5 \\\n    --warmup_epochs 2 \\\n    --blr 9e-3 \\\n    --weight_decay 0.02 \\\n    --output_dir ./checkpoint_L10/\n\nRepeat the command for L = 20, 30, and 32 by changing the --adapter_layer parameter and the output directory accordingly. After training, evaluate each model on the ScienceQA validation set to measure the answering accuracy. The paper's ablation study (Table 4) confirms that increasing the layer numbers from 10 to 30 leads to significant improvement in accuracy, with L = 30 being optimal."
}