{
    "source": ["/workspace/llama_adapter_v2_multimodal7b/util/evaluate_mme.py"],
    "usage_instructions": "1. Download the MME benchmark dataset and evaluation tool from the MME repository (https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation). 2. Run the evaluation script with: `python util/evaluate_mme.py --pretrained_path [MODEL_PATH] --llama_path [LLAMA_DIR] --output_path [RESULT_FILE_PATH]` 3. Evaluate the results using the MME evaluation tool: `python MME_Benchmark_release_version/eval_tool/calculation.py --results_dir [RESULT_FILE_PATH]` 4. Compare the results with LLaVA's scores from the MME leaderboard (https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation). According to the leaderboard, LLaMA-Adapter V2.1 achieves 1328.39 in Perception and 356.43 in Cognition, while LLaVA achieves 1531.31 in Perception and 295.36 in Cognition, showing that LLaMA-Adapter performs better in Cognition but worse in Perception compared to LLaVA."
}