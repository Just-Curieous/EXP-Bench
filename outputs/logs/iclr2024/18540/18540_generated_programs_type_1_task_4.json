{
    "source": [
        "/workspace/scripts/ppo-lag.sh",
        "/workspace/scripts/ppo-reward-shaping.sh"
    ],
    "usage_instructions": "To compare dynamic balancing with Lagrange multiplier versus fixed reward shaping coefficients in Safe RLHF training, follow these steps:\n\n1. First, train the SFT model (if not already available):\n   ```bash\n   bash scripts/sft.sh --model_name_or_path PKU-Alignment/alpaca-7b-reproduced --output_dir output/sft\n   ```\n\n2. Train the reward and cost models:\n   ```bash\n   bash scripts/reward-model.sh --model_name_or_path output/sft --output_dir output/rm\n   bash scripts/cost-model.sh --model_name_or_path output/sft --output_dir output/cm\n   ```\n\n3. For the dynamic balancing approach with Lagrange multiplier, run the following for each of the three fine-tuning rounds (replacing previous_round with the appropriate path for rounds 2 and 3):\n   ```bash\n   bash scripts/ppo-lag.sh \\\n       --actor_model_name_or_path output/sft \\\n       --reward_model_name_or_path output/rm \\\n       --cost_model_name_or_path output/cm \\\n       --output_dir output/ppo-lag-round1\n   ```\n\n4. For the fixed reward shaping coefficients approach, run the following for each coefficient value (0.01, 0.5, 1, 2, 5, 10, and 100) and for each of the three fine-tuning rounds:\n   ```bash\n   bash scripts/ppo-reward-shaping.sh \\\n       --actor_model_name_or_path output/sft \\\n       --reward_model_name_or_path output/rm \\\n       --cost_model_name_or_path output/cm \\\n       --lambda_init 1.0 \\\n       --output_dir output/ppo-rs-coef1-round1\n   ```\n   Note: To change the fixed coefficient, modify the `--lambda_init` parameter to the desired value (e.g., 0.01, 0.5, etc.).\n\n5. For subsequent rounds, use the output from the previous round as the input for the next round:\n   ```bash\n   bash scripts/ppo-lag.sh \\\n       --actor_model_name_or_path output/ppo-lag-round1 \\\n       --reward_model_name_or_path output/rm \\\n       --cost_model_name_or_path output/cm \\\n       --output_dir output/ppo-lag-round2\n   ```\n\n6. The training curves and win rates for helpfulness and harmlessness will be logged to wandb (if configured) or to the output directories. Compare these metrics across the different approaches to analyze the trade-offs."
}