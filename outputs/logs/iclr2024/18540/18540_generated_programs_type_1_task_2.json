{
    "source": [
        "/workspace/scripts/arena-evaluation.sh",
        "/workspace/safe_rlhf/evaluate/gpt4/__main__.py"
    ],
    "usage_instructions": "To evaluate whether iterative Safe RLHF training improves helpfulness and harmlessness across successive iterations, run the following steps:\n\n1. First, use the arena-evaluation.sh script to compare the base model (Alpaca-7B) with each version of the Beaver model (v1, v2, v3) to see the progression of improvements:\n\n   ```bash\n   # Compare base model (Alpaca-7B) with Beaver-v1\n   bash scripts/arena-evaluation.sh \\\n       --red_corner_model_name_or_path PKU-Alignment/alpaca-7b-reproduced \\\n       --blue_corner_model_name_or_path PKU-Alignment/beaver-7b-v1.0 \\\n       --reward_model_name_or_path PKU-Alignment/beaver-7b-unified-reward \\\n       --cost_model_name_or_path PKU-Alignment/beaver-7b-unified-cost \\\n       --output_dir output/arena-evaluation-v1\n\n   # Compare base model (Alpaca-7B) with Beaver-v2\n   bash scripts/arena-evaluation.sh \\\n       --red_corner_model_name_or_path PKU-Alignment/alpaca-7b-reproduced \\\n       --blue_corner_model_name_or_path PKU-Alignment/beaver-7b-v2.0 \\\n       --reward_model_name_or_path PKU-Alignment/beaver-7b-unified-reward \\\n       --cost_model_name_or_path PKU-Alignment/beaver-7b-unified-cost \\\n       --output_dir output/arena-evaluation-v2\n\n   # Compare base model (Alpaca-7B) with Beaver-v3\n   bash scripts/arena-evaluation.sh \\\n       --red_corner_model_name_or_path PKU-Alignment/alpaca-7b-reproduced \\\n       --blue_corner_model_name_or_path PKU-Alignment/beaver-7b-v3.0 \\\n       --reward_model_name_or_path PKU-Alignment/beaver-7b-unified-reward \\\n       --cost_model_name_or_path PKU-Alignment/beaver-7b-unified-cost \\\n       --output_dir output/arena-evaluation-v3\n   ```\n\n2. Then, use the GPT-4 evaluation to get a more comprehensive assessment of the models' performance:\n\n   ```bash\n   # Install OpenAI Python API if not already installed\n   pip install openai\n   export OPENAI_API_KEY=\"your_openai_api_key_here\"\n\n   # Compare base model (Alpaca-7B) with Beaver-v3 using GPT-4\n   python -m safe_rlhf.evaluate.gpt4 \\\n       --red_corner_model_name_or_path PKU-Alignment/alpaca-7b-reproduced \\\n       --blue_corner_model_name_or_path PKU-Alignment/beaver-7b-v3.0\n   ```\n\n3. The arena evaluation will output metrics showing both helpfulness (reward scores) and harmlessness (cost scores) for each model, allowing you to compare how these metrics improve across iterations. The GPT-4 evaluation will provide additional human-like assessment of the models' performance.\n\nThe results should show that later iterations, particularly Beaver-v3, record substantially higher Elo scores in both harmlessness and helpfulness compared to the base model, with improvements like an increase of approximately +244.91 in helpfulness (as measured by GPT-4) and a reduction in harmful response ratios from around 53.08% for the base model to about 2.45% for Beaver-v3."
}