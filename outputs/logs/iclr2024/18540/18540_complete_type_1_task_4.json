{
  "questions": [
    {
      "question": "Does using a dynamic balancing approach with a Lagrange multiplier outperform fixed reward shaping coefficients in achieving a balance between helpfulness and harmlessness during Safe RLHF training? Investigate this by comparing the trade-offs in win rates and cost signal trends across multiple fine-tuning rounds starting from an Alpaca-7B SFT checkpoint.",
      "method": "Train two sets of models using the Safe RLHF pipeline. In the first set, employ the dynamic balancing approach where the Lagrange multiplier (\u03bb) is updated based on the moving average of the cost signal (as described in Equation (10) and Appendix F.3 of the paper). In the second set, apply fixed reward shaping coefficients (specifically, 0.01, 0.5, 1, 2, 5, 10, and 100). Record detailed training curves capturing both the moving average of the cost signal and the win rates for helpfulness and harmlessness. Perform the evaluation over three iterative fine-tuning rounds starting from the initial Alpaca-7B SFT model. The experiment should leverage the available Safe RLHF training scripts (e.g., using the ppo-lag.sh command) to ensure reproducibility and standardized comparisons.",
      "expected_outcome": "The dynamic balancing method is expected to demonstrate a superior trade-off by dynamically modulating the penalty for harmfulness, thereby preventing over-optimization of one objective at the expense of the other. Quantitative results, as reflected in win rates and training curves (e.g., those in Table 7), should show that the dynamic Lagrange multiplier update adjusts more effectively to fluctuations in the cost signal compared to static fixed coefficients. Moderate fixed coefficients may partially address the tension but are anticipated to yield less consistent results, highlighting the benefits of the dynamic approach in maintaining both helpfulness and safety.",
      "subsection_source": "5.2 EXPERIMENT RESULTS",
      "source": [
        "/workspace/scripts/ppo-lag.sh",
        "/workspace/scripts/ppo-reward-shaping.sh"
      ],
      "usage_instructions": "To compare dynamic balancing with Lagrange multiplier versus fixed reward shaping coefficients in Safe RLHF training, follow these steps:\n\n1. First, train the SFT model (if not already available):\n   ```bash\n   bash scripts/sft.sh --model_name_or_path PKU-Alignment/alpaca-7b-reproduced --output_dir output/sft\n   ```\n\n2. Train the reward and cost models:\n   ```bash\n   bash scripts/reward-model.sh --model_name_or_path output/sft --output_dir output/rm\n   bash scripts/cost-model.sh --model_name_or_path output/sft --output_dir output/cm\n   ```\n\n3. For the dynamic balancing approach with Lagrange multiplier, run the following for each of the three fine-tuning rounds (replacing previous_round with the appropriate path for rounds 2 and 3):\n   ```bash\n   bash scripts/ppo-lag.sh \\\n       --actor_model_name_or_path output/sft \\\n       --reward_model_name_or_path output/rm \\\n       --cost_model_name_or_path output/cm \\\n       --output_dir output/ppo-lag-round1\n   ```\n\n4. For the fixed reward shaping coefficients approach, run the following for each coefficient value (0.01, 0.5, 1, 2, 5, 10, and 100) and for each of the three fine-tuning rounds:\n   ```bash\n   bash scripts/ppo-reward-shaping.sh \\\n       --actor_model_name_or_path output/sft \\\n       --reward_model_name_or_path output/rm \\\n       --cost_model_name_or_path output/cm \\\n       --lambda_init 1.0 \\\n       --output_dir output/ppo-rs-coef1-round1\n   ```\n   Note: To change the fixed coefficient, modify the `--lambda_init` parameter to the desired value (e.g., 0.01, 0.5, etc.).\n\n5. For subsequent rounds, use the output from the previous round as the input for the next round:\n   ```bash\n   bash scripts/ppo-lag.sh \\\n       --actor_model_name_or_path output/ppo-lag-round1 \\\n       --reward_model_name_or_path output/rm \\\n       --cost_model_name_or_path output/cm \\\n       --output_dir output/ppo-lag-round2\n   ```\n\n6. The training curves and win rates for helpfulness and harmlessness will be logged to wandb (if configured) or to the output directories. Compare these metrics across the different approaches to analyze the trade-offs.",
      "requirements": [
        "Step 1: Set up the environment and parse command-line arguments for both PPO-LAG and PPO-Reward-Shaping scripts (/workspace/scripts/ppo-lag.sh:18-110, /workspace/scripts/ppo-reward-shaping.sh:18-103)",
        "Step 2: Initialize the Lagrange multiplier for PPO-LAG as a learnable parameter with specified initial value, learning rate, and optional maximum value (/workspace/safe_rlhf/algorithms/ppo_lag/trainer.py:59-67)",
        "Step 3: Initialize a fixed lambda value for PPO-Reward-Shaping based on the lambda_init parameter (/workspace/safe_rlhf/algorithms/ppo_reward_shaping/trainer.py:55-56)",
        "Step 4: Load and initialize actor, reward, cost, and critic models for both approaches (/workspace/safe_rlhf/algorithms/ppo_lag/trainer.py:69-138, /workspace/safe_rlhf/algorithms/ppo_reward_shaping/trainer.py:58-82)",
        "Step 5: Implement post-rollout processing to collect rewards and costs from respective models (/workspace/safe_rlhf/algorithms/ppo_lag/trainer.py:146-209, /workspace/safe_rlhf/algorithms/ppo_reward_shaping/trainer.py:84-147)",
        "Step 6: For PPO-LAG, implement the dynamic lambda update mechanism that adjusts the Lagrange multiplier based on whether the average episode cost exceeds the threshold (/workspace/safe_rlhf/algorithms/ppo_lag/trainer.py:311-326)",
        "Step 7: For PPO-LAG, implement the actor loss function that combines reward and cost advantages using the current lambda value (/workspace/safe_rlhf/algorithms/ppo_lag/trainer.py:289-309)",
        "Step 8: For PPO-Reward-Shaping, directly combine reward and cost using the fixed lambda value: shaped_reward = reward - lambda_value * cost (/workspace/safe_rlhf/algorithms/ppo_reward_shaping/trainer.py:214)",
        "Step 9: Implement the RL training step for both approaches, including actor and critic model updates (/workspace/safe_rlhf/algorithms/ppo_lag/trainer.py:311-442, /workspace/safe_rlhf/algorithms/ppo_reward_shaping/trainer.py:196-303)",
        "Step 10: Set up logging for training metrics including rewards, costs, and lambda values (/workspace/safe_rlhf/algorithms/ppo_lag/trainer.py:414-442, /workspace/safe_rlhf/algorithms/ppo_reward_shaping/trainer.py:255-303)",
        "Final Step: Execute the training process with DeepSpeed for distributed training (/workspace/scripts/ppo-lag.sh:148-198, /workspace/scripts/ppo-reward-shaping.sh:138-182)"
      ],
      "agent_instructions": "Your task is to implement two different approaches for Safe Reinforcement Learning from Human Feedback (Safe RLHF) that balance reward maximization with safety constraints: (1) a dynamic Lagrangian approach and (2) a fixed reward shaping approach.\n\nFor the Lagrangian approach:\n- Implement a training script that uses Proximal Policy Optimization (PPO) with a Lagrange multiplier\n- The Lagrange multiplier should be initialized to a specified value and dynamically updated during training\n- The multiplier should increase when the average cost exceeds a threshold and decrease otherwise\n- Implement a mechanism to cap the maximum value of the multiplier if specified\n\nFor the reward shaping approach:\n- Implement a training script that uses PPO with a fixed coefficient to combine reward and cost\n- The shaped reward should be calculated as: reward - coefficient * cost\n- The coefficient should remain constant throughout training\n\nBoth approaches should:\n- Load pretrained actor, reward, and cost models\n- Use the PKU-SafeRLHF/train dataset for training\n- Apply KL divergence regularization to prevent the policy from deviating too much from the reference policy\n- Track and log metrics including rewards, costs, and training losses\n- Support distributed training using DeepSpeed\n\nThe implementation should allow for comparing the two approaches by running multiple training rounds with different configurations.",
      "masked_source": [
        "/workspace/scripts/ppo-lag.sh",
        "/workspace/scripts/ppo-reward-shaping.sh",
        "/workspace/safe_rlhf/algorithms/ppo_lag/__init__.py",
        "/workspace/safe_rlhf/algorithms/ppo_lag/__main__.py",
        "/workspace/safe_rlhf/algorithms/ppo_lag/main.py",
        "/workspace/safe_rlhf/algorithms/ppo_lag/trainer.py",
        "/workspace/safe_rlhf/algorithms/ppo_reward_shaping/__init__.py",
        "/workspace/safe_rlhf/algorithms/ppo_reward_shaping/__main__.py",
        "/workspace/safe_rlhf/algorithms/ppo_reward_shaping/main.py",
        "/workspace/safe_rlhf/algorithms/ppo_reward_shaping/trainer.py"
      ]
    }
  ]
}