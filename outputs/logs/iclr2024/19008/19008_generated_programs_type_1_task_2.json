{
    "source": ["/workspace/run_psychobench.py"],
    "usage_instructions": "To answer the question about language models' fairness in interpersonal perception as measured by ICB scores, run the following commands for each model:\n\n1. For LLaMA-2 7B:\n   ```\n   python run_psychobench.py --model llama-2-7b --questionnaire ICB --shuffle-count 0 --test-count 10\n   ```\n\n2. For LLaMA-2 13B:\n   ```\n   python run_psychobench.py --model llama-2-13b --questionnaire ICB --shuffle-count 0 --test-count 10\n   ```\n\n3. For text-davinci-003:\n   ```\n   python run_psychobench.py --model text-davinci-003 --questionnaire ICB --shuffle-count 0 --test-count 10 --openai-key \"YOUR_OPENAI_API_KEY\"\n   ```\n\n4. For GPT-3.5-turbo:\n   ```\n   python run_psychobench.py --model gpt-3.5-turbo --questionnaire ICB --shuffle-count 0 --test-count 10 --openai-key \"YOUR_OPENAI_API_KEY\"\n   ```\n\n5. For GPT-4 (default configuration):\n   ```\n   python run_psychobench.py --model gpt-4 --questionnaire ICB --shuffle-count 0 --test-count 10 --openai-key \"YOUR_OPENAI_API_KEY\"\n   ```\n\nFor jailbroken configurations, you would need to modify the prompts in the questionnaires.json file or create custom generators. The results will be saved in the 'results' directory with ICB scores in Mean Â± Standard Deviation format, which can be compared to the human reference baseline (Crowd ICB score of approximately 3.7 as indicated in the questionnaires.json file)."
}