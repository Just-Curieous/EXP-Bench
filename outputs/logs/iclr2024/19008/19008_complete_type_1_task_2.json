{
  "questions": [
    {
      "question": "Do language models display higher fairness in interpersonal perception, as measured by lower ICB scores, compared to the human average?",
      "method": "Perform an experiment using the ICB (Implicit Culture Belief) questionnaire. Evaluate several LLMs\u2014including LLaMA-2 (7B and 13B), text-davinci-003, GPT-3.5-turbo, and GPT-4 in both default and jailbroken configurations\u2014under identical prompt conditions. For each model, conduct 10 independent runs using controlled generation parameters (e.g., fixed temperature, max tokens, and seed settings) to ensure consistency. When running the experiment with a framework like PsychoBench, ensure you specify the ICB questionnaire (e.g., using the '--questionnaire ICB' argument) and set the test count appropriately to replicate the 10 runs. Record the overall ICB scores in Mean \u00b1 Standard Deviation format, and compare these results to the human reference baseline (e.g., a Crowd ICB score of approximately 3.7).",
      "expected_outcome": "The results should indicate that LLMs, particularly high-performing models such as GPT-4 (default configuration displaying an ICB around 1.9 \u00b1 0.4), exhibit significantly lower ICB scores than the human average. This outcome would suggest that these language models demonstrate higher fairness in interpersonal attributions and show a reduced emphasis on ethnic determinism.",
      "subsection_source": "3.2 E XPERIMENTAL RESULTS",
      "source": [
        "/workspace/run_psychobench.py"
      ],
      "usage_instructions": "To answer the question about language models' fairness in interpersonal perception as measured by ICB scores, run the following commands for each model:\n\n1. For LLaMA-2 7B:\n   ```\n   python run_psychobench.py --model llama-2-7b --questionnaire ICB --shuffle-count 0 --test-count 10\n   ```\n\n2. For LLaMA-2 13B:\n   ```\n   python run_psychobench.py --model llama-2-13b --questionnaire ICB --shuffle-count 0 --test-count 10\n   ```\n\n3. For text-davinci-003:\n   ```\n   python run_psychobench.py --model text-davinci-003 --questionnaire ICB --shuffle-count 0 --test-count 10 --openai-key \"YOUR_OPENAI_API_KEY\"\n   ```\n\n4. For GPT-3.5-turbo:\n   ```\n   python run_psychobench.py --model gpt-3.5-turbo --questionnaire ICB --shuffle-count 0 --test-count 10 --openai-key \"YOUR_OPENAI_API_KEY\"\n   ```\n\n5. For GPT-4 (default configuration):\n   ```\n   python run_psychobench.py --model gpt-4 --questionnaire ICB --shuffle-count 0 --test-count 10 --openai-key \"YOUR_OPENAI_API_KEY\"\n   ```\n\nFor jailbroken configurations, you would need to modify the prompts in the questionnaires.json file or create custom generators. The results will be saved in the 'results' directory with ICB scores in Mean \u00b1 Standard Deviation format, which can be compared to the human reference baseline (Crowd ICB score of approximately 3.7 as indicated in the questionnaires.json file).",
      "requirements": [
        "Step 1: Parse command line arguments including model name, questionnaire name, shuffle count, test count, experiment name, significance level, and OpenAI API key (run_psychobench.py:7-25)",
        "Step 2: Load the specified questionnaire (ICB) from questionnaires.json file, which contains questions, scoring scales, and human baseline data (utils.py:13-29)",
        "Step 3: Create results directories if they don't exist (utils.py:491-492)",
        "Step 4: Generate test files with questions in specified order, including original order and optional shuffled orders (utils.py:61-88)",
        "Step 5: Set up API credentials for the language model (example_generator.py:78)",
        "Step 6: For each question order (original and shuffled), run the language model multiple times according to test_count (example_generator.py:102-157)",
        "Step 7: Format prompts with appropriate instructions for the ICB questionnaire and send to the language model (example_generator.py:113-126)",
        "Step 8: Parse model responses to extract numerical scores (example_generator.py:62-70)",
        "Step 9: Save prompts and responses to files for reference (example_generator.py:132-138)",
        "Step 10: Convert raw responses into structured data for analysis (utils.py:92-137)",
        "Step 11: Compute statistics (mean and standard deviation) for each category in the questionnaire (utils.py:141-164)",
        "Step 12: Perform hypothesis testing to compare model results with human baseline data (utils.py:168-239)",
        "Step 13: Generate visualizations of the results (utils.py:33-57)",
        "Step 14: Save results in markdown format with statistical analysis (utils.py:449-474)"
      ],
      "agent_instructions": "Create a system to evaluate language models on the Implicit Person Theory of Ethnocultural Beliefs (ICB) questionnaire, which measures beliefs about whether ethnocultural characteristics are fixed or malleable. The system should:\n\n1. Accept command line arguments for model selection (e.g., llama-2-7b, gpt-3.5-turbo), questionnaire type (ICB), number of test runs, and API credentials when needed.\n\n2. Load the ICB questionnaire, which contains 8 statements about ethnocultural beliefs that are rated on a 6-point scale from 'strongly disagree' to 'strongly agree'.\n\n3. Generate prompts for the language model that instruct it to respond with numerical scores (1-6) for each statement.\n\n4. Send the prompts to the specified language model and collect responses, handling different model types appropriately (OpenAI API for GPT models, etc.).\n\n5. Parse the responses to extract numerical scores, handling potential formatting issues.\n\n6. Calculate statistics (mean and standard deviation) from the collected scores, applying reverse scoring where needed (statements 5-8 in ICB are reverse-scored).\n\n7. Compare the model's results with human baseline data (the human average ICB score is approximately 3.7).\n\n8. Perform statistical analysis (t-tests) to determine if the model's responses differ significantly from human responses.\n\n9. Generate visualizations and save results in a readable format.\n\nThe system should be able to run multiple tests with the same or different question orders to ensure reliability of the results.",
      "masked_source": [
        "/workspace/run_psychobench.py",
        "/workspace/utils.py",
        "/workspace/example_generator.py"
      ]
    }
  ]
}