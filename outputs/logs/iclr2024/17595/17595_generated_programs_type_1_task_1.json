{
    "source": [
        "/workspace/examples/test_all_schedules.sh",
        "/workspace/examples/pretrain_zero_bubble.sh"
    ],
    "usage_instructions": "To compare ZB-2p and 1F1B under identical memory constraints, you can use the provided scripts as follows:\n\n1. First, set up the environment variables to configure the experiment:\n   ```bash\n   export PIPELINE_SIZE=8  # Set to the number of pipeline stages (e.g., 8 GPUs)\n   export LAYERS=30  # Set the number of layers for your model\n   export MICRO_BATCH_SIZE=1  # For ZB-2p\n   export GLOBAL_BATCH_SIZE=16  # Adjust as needed\n   export HIDDEN_SIZE=4096  # For a 6.2B model as mentioned in the question\n   export ATTENTION_HEADS=32\n   ```\n\n2. For ZB-2p, run:\n   ```bash\n   export ENABLE_ZERO_BUBBLE=1\n   export ZERO_BUBBLE_MEM_LIMIT=$((2 * $PIPELINE_SIZE))  # This activates ZB-2p mode\n   export ZERO_BUBBLE_TIMER_START=100\n   export ZERO_BUBBLE_TIMER_END=110\n   bash /workspace/examples/pretrain_zero_bubble.sh --enable-optimizer-post-validation --allow-padding-num-layers\n   ```\n\n3. For 1F1B with equivalent memory budget (which requires doubling the microbatch size), run:\n   ```bash\n   export ENABLE_ZERO_BUBBLE=  # Unset this variable\n   export MICRO_BATCH_SIZE=2  # Double the microbatch size for equivalent memory\n   bash /workspace/examples/pretrain_zero_bubble.sh --allow-padding-num-layers\n   ```\n\n4. Compare the throughput (samples per GPU per second) reported in the logs for both runs.\n\nAlternatively, you can use the test_all_schedules.sh script which automates testing multiple scheduling strategies:\n   ```bash\n   bash /workspace/examples/test_all_schedules.sh\n   ```\n   This will run tests for various scheduling strategies including 1F1B and ZB (ZB-2p) and output the results to the logs directory."
}