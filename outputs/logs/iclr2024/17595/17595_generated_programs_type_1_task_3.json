{
    "source": ["/workspace/examples/pretrain_zero_bubble.sh"],
    "usage_instructions": "To run the experiment comparing ZB-2p with other scheduling methods (1F1B, 1F1B-I, and ZB-1p) for a 28.3B model on 32 GPUs with varying microbatch counts (96, 128, and 256), execute the pretrain_zero_bubble.sh script with the following configurations for each method:\n\n1. For 1F1B (baseline):\n   ```\n   export PIPELINE_SIZE=32\n   export LAYERS=94  # 32*3-2 to compensate for embedding layers\n   export HIDDEN_SIZE=7168  # Parameters for 28.3B model\n   export ATTENTION_HEADS=56\n   export MICRO_BATCH_SIZE=1\n   export GLOBAL_BATCH_SIZE=96  # Or 128, or 256 for different experiments\n   export ENABLE_ZERO_BUBBLE=  # Unset this variable\n   export INTERLEAVED_1F1B=  # Unset this variable\n   bash examples/pretrain_zero_bubble.sh\n   ```\n\n2. For 1F1B-I (Interleaved):\n   ```\n   export PIPELINE_SIZE=32\n   export LAYERS=94\n   export HIDDEN_SIZE=7168\n   export ATTENTION_HEADS=56\n   export MICRO_BATCH_SIZE=1\n   export GLOBAL_BATCH_SIZE=96  # Or 128, or 256 for different experiments\n   export ENABLE_ZERO_BUBBLE=  # Unset this variable\n   export INTERLEAVED_1F1B=1  # Set this to enable interleaved 1F1B\n   bash examples/pretrain_zero_bubble.sh\n   ```\n\n3. For ZB-1p:\n   ```\n   export PIPELINE_SIZE=32\n   export LAYERS=94\n   export HIDDEN_SIZE=7168\n   export ATTENTION_HEADS=56\n   export MICRO_BATCH_SIZE=1\n   export GLOBAL_BATCH_SIZE=96  # Or 128, or 256 for different experiments\n   export ENABLE_ZERO_BUBBLE=1\n   export ZERO_BUBBLE_MEM_LIMIT=32  # 1x number of pipelines\n   bash examples/pretrain_zero_bubble.sh\n   ```\n\n4. For ZB-2p:\n   ```\n   export PIPELINE_SIZE=32\n   export LAYERS=94\n   export HIDDEN_SIZE=7168\n   export ATTENTION_HEADS=56\n   export MICRO_BATCH_SIZE=1\n   export GLOBAL_BATCH_SIZE=96  # Or 128, or 256 for different experiments\n   export ENABLE_ZERO_BUBBLE=1\n   export ZERO_BUBBLE_MEM_LIMIT=64  # 2x number of pipelines\n   bash examples/pretrain_zero_bubble.sh\n   ```\n\nEnsure that you have access to 4 nodes with 8 NVIDIA A100 SXM 80G GPUs each (for a total of 32 GPUs) connected via a RoCE RDMA network as specified in the experiment question. The script will automatically collect the necessary metrics to compare throughput and memory usage across the different scheduling methods and microbatch counts."
}