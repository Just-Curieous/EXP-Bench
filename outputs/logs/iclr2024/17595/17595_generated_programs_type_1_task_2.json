{
    "source": ["/workspace/examples/test_all_schedules.sh", "/workspace/examples/pretrain_zero_bubble.sh"],
    "usage_instructions": "To run the experiment comparing throughput across different scheduling methods with varying pipeline stages for a 14.6B model on 16 GPUs, follow these steps:\n\n1. Modify the `/workspace/examples/test_all_schedules.sh` script with the following changes:\n   - Update the `methods` array to include only the required scheduling methods: \"1f1b\", \"1f1b-interleaved\", \"zb\" (for ZB-1p), and a second \"zb\" configuration (for ZB-2p)\n   - Set `PIPELINE_SIZE=16` to use all 16 GPUs\n   - Set `LAYERS=46` for the 14.6B model\n   - Set `HIDDEN_SIZE=5120` and `ATTENTION_HEADS=40` as specified\n   - Set `SEQ_LENGTH=1024` as specified\n   - Add a loop to vary the microbatch size with values 48, 64, and 128\n   - For the ZB-1p configuration, set `ZERO_BUBBLE_MEM_LIMIT=$PIPELINE_SIZE` (1x pipeline size)\n   - For the ZB-2p configuration, set `ZERO_BUBBLE_MEM_LIMIT=$((2 * $PIPELINE_SIZE))` (2x pipeline size)\n\n2. Execute the modified script, which will run all the specified scheduling methods with different microbatch configurations and collect throughput measurements.\n\nThe script will automatically run the experiments using the `pretrain_zero_bubble.sh` script, which handles the actual training process with the specified configuration. The results will be logged to the `./logs` directory, where you can analyze the throughput measurements for each configuration."
}