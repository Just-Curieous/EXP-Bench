{
  "questions": [
    {
      "question": "Under identical memory consumption constraints\u2014using the appropriate zero bubble scheduling flags such as --enable-zero-bubble along with setting --zero-bubble-max-pending-backward appropriately for ZB-2p\u2014does ZB-2p achieve a higher throughput than 1F1B even when using fewer microbatches?",
      "method": "Set up experiments using a chosen model configuration (for example, a 6.2B model on 8 GPUs). Enable zero bubble scheduling using flags like --enable-zero-bubble and set --zero-bubble-max-pending-backward to 2\u00d7 the number of pipeline stages to activate the ZB-2p mode. Adjust the configuration so that both ZB-2p and 1F1B operate under the same peak memory budget; this may involve doubling the microbatch size for 1F1B as suggested. Keep all other conditions constant (e.g., warm-up iterations, random seeds, optimizer post validation using --enable-optimizer-post-validation, network and hardware conditions). Record the samples per GPU per second for each method, and compare how throughput scales with different microbatch counts as indicated in the repository documentation.",
      "expected_outcome": "It is expected that ZB-2p will demonstrate higher throughput compared to 1F1B under identical memory consumption, even with a lower microbatch count. The efficient scheduling in ZB-2p, which nearly eliminates pipeline bubbles, should allow its throughput to approach the upper bound (e.g., throughput values around 4.32\u20134.39 samples per second for the 6.2B model on 8 GPUs as observed in Table 4) whereas 1F1B is expected to yield lower throughput. Detailed experimental data from key measurements (Tables 4 and 9, and Figure 5) should support this conclusion.",
      "subsection_source": "5.2 MAIN RESULTS",
      "source": [
        "/workspace/examples/test_all_schedules.sh",
        "/workspace/examples/pretrain_zero_bubble.sh"
      ],
      "usage_instructions": "To compare ZB-2p and 1F1B under identical memory constraints, you can use the provided scripts as follows:\n\n1. First, set up the environment variables to configure the experiment:\n   ```bash\n   export PIPELINE_SIZE=8  # Set to the number of pipeline stages (e.g., 8 GPUs)\n   export LAYERS=30  # Set the number of layers for your model\n   export MICRO_BATCH_SIZE=1  # For ZB-2p\n   export GLOBAL_BATCH_SIZE=16  # Adjust as needed\n   export HIDDEN_SIZE=4096  # For a 6.2B model as mentioned in the question\n   export ATTENTION_HEADS=32\n   ```\n\n2. For ZB-2p, run:\n   ```bash\n   export ENABLE_ZERO_BUBBLE=1\n   export ZERO_BUBBLE_MEM_LIMIT=$((2 * $PIPELINE_SIZE))  # This activates ZB-2p mode\n   export ZERO_BUBBLE_TIMER_START=100\n   export ZERO_BUBBLE_TIMER_END=110\n   bash /workspace/examples/pretrain_zero_bubble.sh --enable-optimizer-post-validation --allow-padding-num-layers\n   ```\n\n3. For 1F1B with equivalent memory budget (which requires doubling the microbatch size), run:\n   ```bash\n   export ENABLE_ZERO_BUBBLE=  # Unset this variable\n   export MICRO_BATCH_SIZE=2  # Double the microbatch size for equivalent memory\n   bash /workspace/examples/pretrain_zero_bubble.sh --allow-padding-num-layers\n   ```\n\n4. Compare the throughput (samples per GPU per second) reported in the logs for both runs.\n\nAlternatively, you can use the test_all_schedules.sh script which automates testing multiple scheduling strategies:\n   ```bash\n   bash /workspace/examples/test_all_schedules.sh\n   ```\n   This will run tests for various scheduling strategies including 1F1B and ZB (ZB-2p) and output the results to the logs directory.",
      "requirements": [
        "Step 1: Check for required libraries (libibverbs) to ensure proper networking support (/workspace/examples/test_all_schedules.sh:3-8)",
        "Step 2: Define the pipeline scheduling strategies to be tested, including 1F1B and Zero Bubble variants (/workspace/examples/test_all_schedules.sh:10-20)",
        "Step 3: Set up distributed training environment variables (world size, rank, master address, port) (/workspace/examples/test_all_schedules.sh:22-34)",
        "Step 4: Wait until GPUs are available for use by checking memory usage (/workspace/examples/test_all_schedules.sh:38-59)",
        "Step 5: Configure common training parameters (CUDA settings, logging intervals) (/workspace/examples/test_all_schedules.sh:61-65)",
        "Step 6: For each scheduling strategy, set up specific environment variables (/workspace/examples/test_all_schedules.sh:71-102)",
        "Step 7: Configure model architecture parameters (layers, hidden size, attention heads) based on pipeline size (/workspace/examples/test_all_schedules.sh:78-86)",
        "Step 8: For Zero Bubble (ZB) strategy, enable zero bubble and set memory limit to 2x pipeline size (/workspace/examples/test_all_schedules.sh:105-107)",
        "Step 9: For 1F1B strategy, ensure zero bubble is disabled (/workspace/examples/test_all_schedules.sh:124-125)",
        "Step 10: Download and prepare the training dataset if not already available (/workspace/examples/pretrain_zero_bubble.sh:17-20)",
        "Step 11: Configure model training parameters including sequence length, batch sizes, and learning rates (/workspace/examples/pretrain_zero_bubble.sh:72-114)",
        "Step 12: Set up model architecture options (transformer implementation, flash attention, etc.) (/workspace/examples/pretrain_zero_bubble.sh:115-114)",
        "Step 13: Launch distributed training using torchrun with the configured parameters (/workspace/examples/pretrain_zero_bubble.sh:151-155)",
        "Step 14: Collect and log training metrics for comparison between scheduling strategies (/workspace/examples/test_all_schedules.sh:152-153)",
        "Final Step: Compare the throughput and memory usage between different scheduling strategies from the logs (/workspace/examples/test_all_schedules.sh:173-174)"
      ],
      "agent_instructions": "Your task is to implement scripts for comparing different pipeline parallelism scheduling strategies for large language model training, specifically focusing on Zero Bubble (ZB-2p) and 1F1B (one-forward-one-backward) under identical memory constraints.\n\nYou need to create:\n\n1. A main script that tests multiple scheduling strategies by:\n   - Setting up distributed training environment variables\n   - Configuring model parameters based on available GPUs\n   - Running each strategy sequentially and logging results\n   - Ensuring proper cleanup between runs\n\n2. A training script that:\n   - Downloads a sample dataset if not available\n   - Configures model architecture (layers, hidden size, attention heads)\n   - Sets up appropriate batch sizes and learning rates\n   - Launches distributed training with the specified scheduling strategy\n\nThe key scheduling strategies to implement are:\n- Zero Bubble (ZB-2p): Uses memory-aware scheduling with a memory limit of 2x pipeline size\n- 1F1B: Standard one-forward-one-backward pipeline scheduling\n\nFor fair comparison, when using ZB-2p, set MICRO_BATCH_SIZE=1, and when using 1F1B, set MICRO_BATCH_SIZE=2 to maintain equivalent memory usage.\n\nThe scripts should automatically detect the number of available GPUs, configure the pipeline size accordingly, and scale the number of model layers based on pipeline size.\n\nEnsure the output logs contain throughput metrics (samples per second) for comparing the efficiency of different scheduling strategies.",
      "masked_source": [
        "/workspace/examples/test_all_schedules.sh",
        "/workspace/examples/pretrain_zero_bubble.sh"
      ]
    }
  ]
}