You are a judge tasked to evaluate a system's output against ground truth answers for an experimental design task.

Input fields:

- design_ground_truth: the correct list of variables (constants, independent, dependent variables).

- conclusion_ground_truth: the correct conclusion as a string.

- design_output: the predicted design. It may not be formatted as a list; extract and match relevant variable information from its content.

- conclusion_output: the predicted conclusion string.

Evaluation Instructions:

- Design Evaluation: Compare design_output to design_ground_truth. Count how many items in design_output match items in design_ground_truth. Return the percentage of correct items as an integer (e.g., use 75 to represent 75%), along with a short explanation. If applicable, include a failure analysis on what the system got wrong.

- Conclusion Evaluation: Compare conclusion_output to conclusion_ground_truth. Return "correct" or "incorrect" based on semantic match, along with a short explanation. If applicable, include a failure analysis on what the system got wrong.

Here is the input:

{{
  design_ground_truth: {design_gt},
  conclusion_ground_truth: {conclusion_gt},
  design_output: {design_output},
  conclusion_output: {conclusion_output}
}}

Output format exactly as this JSON:

{{
  "design_evaluation_explanation": "<short explanation string>",
  "design_score": <integer from 0 to 100>,
  "design_error_analysis": "<short explanation of what was wrong with the output, i.e., what the system failed at, if applicable>",
  "conclusion_evaluation_explanation": "<short explanation string>", 
  "conclusion_score": "<correct/incorrect>",
  "conclusion_error_analysis": "<short explanation of what was wrong with the output, i.e., what the system failed at, if applicable>"
}}