{"id": "93022", "url": "https://nips.cc/virtual/2024/poster/93022", "title": "Generative Modeling of Molecular Dynamics Trajectories", "authors": [], "abstract": "Abstract:Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show preliminary results on scaling to protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself. Code is available at https://github.com/bjing2016/mdgen.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/bjing2016/mdgen"}, "reproduce_difficulty": {"environment_setup": "Requires pip for package installation with specific versioning.", "resource_requirements": "GPUs not specified, but PyTorch and related libraries suggest a need for a CUDA-compatible GPU. Memory requirements unspecified.", "time_requirements": "Estimated 10 hours for training depending on dataset size and hardware.", "code_quality": "Well-documented code with clear installation and usage instructions.", "difficulty": 3, "stars": 144}, "research_ideas": {"problem_statements": ["Molecular dynamics (MD) simulation is computationally demanding due to the large separation in timescales between integration steps and relevant molecular phenomena.", "Existing deep learning-based surrogate models fail to fully leverage the rich dynamical information in MD training data, restricting their applicability to a limited set of downstream problems.", "There is a need for a fast, general-purpose surrogate modeling of MD that can address diverse tasks such as forward simulation, transition path sampling, trajectory upsampling, and dynamics-conditioned molecular design."], "main_takeaways": ["MDGEN introduces a generative modeling paradigm for learning flexible multi-task surrogate models of MD from data.", "Generative models can be conditioned on different parts of a trajectory to perform tasks like forward simulation, interpolation, upsampling, and inpainting.", "MDGEN demonstrates the capability to reproduce free energy surfaces, torsional relaxation, and Markov state fluxes accurately for tetrapeptide simulations.", "The framework provides proof-of-concept results for scaling to larger systems like protein monomers and opens up new possibilities for dynamics-conditioned molecular design."], "testable_hypotheses": [{"hypothesis": "Does the generative model accurately reproduce the free energy surfaces of molecular systems when compared to traditional MD simulations?", "method": "Evaluate the free energy surfaces generated by MDGEN against those obtained from traditional MD simulations using metrics such as Jensen-Shannon divergence.", "expected_outcome": "MDGEN is expected to achieve close distributional similarity to ground truth free energy surfaces, approaching the accuracy of replicate 100-ns simulations."}, {"hypothesis": "Can the generative model accurately capture the dynamical content of MD, such as torsional relaxation times?", "method": "Compare the predicted relaxation timescales of torsion angles from MDGEN simulations with those from traditional MD simulations.", "expected_outcome": "MDGEN should show excellent agreement for sidechain torsions and reasonable agreement for backbone torsions with ground truth simulations."}, {"hypothesis": "Does the upsampling capability of the generative model recover fast dynamics that are missed in subsampled trajectories?", "method": "Evaluate the autocorrelation functions of torsion angles in upsampled trajectories and compare them with those from high-frequency ground truth trajectories.", "expected_outcome": "MDGEN should accurately recover fast dynamics, including sub-picosecond oscillations, that are not captured in subsampled trajectories."}, {"hypothesis": "Is the generative model capable of zero-shot sampling of transition paths for unseen peptides?", "method": "Sample transition paths between well-separated metastable states and evaluate their likelihood and validity under reference MSM transition path distribution.", "expected_outcome": "MDGEN should produce transition paths with higher likelihoods and validity rates compared to paths sampled from MSMs built from shorter MD simulations."}, {"hypothesis": "Can the generative model outperform existing models like Timewarp and ITO on the forward simulation task?", "method": "Compare the distributional similarity of free energy surfaces and torsion angle distributions generated by MDGEN, Timewarp, and ITO with ground truth MD data.", "expected_outcome": "MDGEN is expected to achieve better consistency with ground truth distributions compared to Timewarp and ITO."}], "follow_up_work_ideas": ["Explore the application of MDGEN to more complex molecular systems beyond peptides, such as large protein complexes or drug-like molecules.", "Investigate the integration of additional conditioning information, such as experimental descriptors, to further enhance the generative capabilities of the model.", "Develop new tokenization strategies for modeling trajectories in systems with atoms entering and exiting a predefined space, expanding the applicability of MDGEN.", "Experiment with fine-tuning single-structure models for co-generation of key frames and trajectory tokens to enhance the scalability of the framework.", "Study the theoretical implications of modeling equilibrium vs. non-equilibrium processes in molecular dynamics using generative models."]}}
{"id": "93431", "url": "https://nips.cc/virtual/2024/poster/93431", "title": "Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs", "authors": [], "abstract": "Abstract:We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. AutoDiff frameworks, like PyTorch, enable efficient end-to-end optimization of differentiable systems. However, general computational workflows can be non-differentiable and involve rich feedback (e.g. console output or user\u2019s responses), heterogeneous parameters (e.g. prompts, codes), and intricate objectives (beyond maximizing a score). We investigate end-to-end generative optimization \u2013 using generative models such as LLMs within the optimizer for automatic updating of general computational workflows. We discover that workflow execution traces are akin to back-propagated gradients in AutoDiff and can provide key information to interpret feedback for efficient optimization. Formally, we frame a new mathematical setup, Optimization with Trace Oracle (OPTO). In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. We provide a Python library, Trace, that efficiently converts a workflow optimization problem into an OPTO instance using PyTorch-like syntax. Using Trace, we develop a general LLM-based generative optimizer called OptoPrime. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We envision Trace as an open research platform for devising novel generative optimizers and developing the next generation of interactive learning agents. Website: https://microsoft.github.io/Trace/.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/microsoft/Trace"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 x NVIDIA GPU recommended", "memory": "8 GB RAM minimum", "API_call": "OpenAI API key required"}, "time_requirements": "1-2", "code_quality": "well-documented code", "difficulty": 2, "stars": 492}, "research_ideas": {"problem_statements": ["How can we automate the design and update of AI systems like coding assistants, robots, and copilots when dealing with non-differentiable workflows with rich feedback and heterogeneous parameters?", "What are the limits of current optimization techniques like Bayesian Optimization, Evolutionary Algorithms, and Reinforcement Learning when applied to large parameter spaces in computational workflows?", "Can generative models like LLMs be used within optimizers to improve the efficiency of workflow optimization?", "How can execution traces be used as a form of feedback to guide optimization in a manner similar to back-propagation in differentiable systems?"], "main_takeaways": ["Trace is a framework that treats computational workflows as graphs and propagates execution traces to optimize heterogeneous parameters.", "The new framework, OPTO, allows optimization of workflows using execution traces and feedback, enabling the use of generative models for optimization.", "OptoPrime, an LLM-based generative optimizer, was developed and shown to be competitive with specialized optimizers across various domains such as prompt optimization and robot controller design.", "Trace's approach subsumes back-propagation for differentiable workflows and extends optimization capabilities to non-differentiable workflows."], "testable_hypotheses": [{"hypothesis": "Does using execution traces as feedback improve optimization efficiency over traditional scalar feedback?", "method": "Compare optimization performance (convergence speed, accuracy) of Trace with scalar feedback-based optimizers on tasks with large parameter spaces.", "expected_outcome": "Trace will show faster convergence and better final performance due to richer feedback from execution traces."}, {"hypothesis": "Can OptoPrime optimize a broader range of tasks compared to specialized optimizers?", "method": "Benchmark OptoPrime against specialized optimizers in diverse domains such as hyper-parameter tuning, prompt optimization, and robot control.", "expected_outcome": "OptoPrime performs competitively or better across multiple tasks, demonstrating its generality."}, {"hypothesis": "Will the inclusion of memory in OptoPrime improve its performance on tasks with non-trivial dependencies?", "method": "Evaluate OptoPrime with and without memory on complex tasks like robot manipulator control.", "expected_outcome": "OptoPrime with memory will show improved performance due to better handling of dependencies and historical context."}, {"hypothesis": "Can execution errors in workflows be effectively used as feedback to optimize parameters?", "method": "Intentionally introduce errors in a workflow and use Trace to optimize parameters using error messages as feedback.", "expected_outcome": "Trace will successfully optimize parameters to eliminate errors, demonstrating its capability to handle execution errors as feedback."}, {"hypothesis": "Does the level of abstraction in defining operators affect the optimization performance of Trace?", "method": "Run experiments with varying levels of abstraction in operator definitions and measure the impact on optimization performance.", "expected_outcome": "An optimal level of abstraction will balance complexity and informativeness, leading to better optimization performance."}], "follow_up_work_ideas": ["Explore the use of execution traces in distributed and parallel computing workflows that may involve asynchronous operations.", "Develop specialized propagators within Trace for handling very large graphs, potentially using hierarchical representations.", "Investigate the integration of non-textual feedback (e.g., visual or auditory) into the OPTO framework to handle a wider range of applications.", "Design more efficient token usage strategies for OptoPrime to handle large-scale optimization problems involving extensive computational graphs.", "Study the theoretical properties of OPTO problems, such as optimization landscape and complexity, to identify classes of problems that are efficiently solvable."]}}
{"id": "98316", "url": "https://nips.cc/virtual/2024/poster/98316", "title": "Causal-learn: Causal Discovery in Python", "authors": [], "abstract": "Abstract:Causal discovery aims at revealing causal relations from observational data, which is a fundamental task in science and engineering. We describe causal-learn, an open-source Python library for causal discovery. This library focuses on bringing a comprehensive collection of causal discovery methods to both practitioners and researchers. It provides easy-to-use APIs for non-specialists, modular building blocks for developers, detailed documentation for learners, and comprehensive methods for all. Different from previous packages in R or Java, causal-learn is fully developed in Python, which could be more in tune with the recent preference shift in programming languages within related communities. The library is available at https://github.com/py-why/causal-learn.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/py-why/causal-learn"}, "reproduce_difficulty": {"environment_setup": "Easy (using pip)", "resource_requirements": {"GPUs": "Not required", "memory": "Standard (depends on the dataset)", "API_calls": "None"}, "time_requirements": "1-2 hours", "code_quality": "Well-documented code", "difficulty": 2, "stars": 1287}, "research_ideas": {"problem_statements": ["The paper addresses the need for a comprehensive Python library for causal discovery, as existing tools are primarily based in Java or R, which may not align with the recent trend favoring Python in machine learning communities.", "The challenge of causal discovery from purely observational data without interventions or randomized experiments, which are often impractical due to costs and logistical limitations.", "The difficulty in evaluating causal discovery methods due to the often unknown ground-truth causal relations in real-world data."], "main_takeaways": ["Causal-learn is a fully Python-based library aimed at providing comprehensive causal discovery tools, catering to practitioners, researchers, and developers.", "The library supports various causal discovery methods, including constraint-based, score-based, and functional causal models, and offers tools for causal representation learning.", "Causal-learn facilitates ease of use, modification, and extension through accessible APIs, thorough documentation, and modular design.", "The library is under continuous development to integrate the latest advancements in causal discovery and aims to make causal analysis more accessible to Python users."], "testable_hypotheses": [{"hypothesis": "Does using Python for causal discovery algorithms improve adoption and usability among machine learning practitioners compared to Java or R?", "method": "Conduct a survey or user study comparing the adoption rates and user feedback for causal-learn versus similar tools in Java or R.", "expected_outcome": "Higher adoption and positive usability feedback for causal-learn among Python users."}, {"hypothesis": "Can causal-learn accurately uncover causal relationships in benchmark datasets with known causal structures?", "method": "Apply causal-learn to benchmark datasets with known ground-truth causal relationships and evaluate its performance using precision, recall, and structural Hamming distance.", "expected_outcome": "Causal-learn should demonstrate high precision and recall, accurately uncovering known causal structures."}, {"hypothesis": "Does the modular design of causal-learn facilitate easier development and integration of new causal discovery algorithms?", "method": "Track the number of new algorithms or extensions contributed by the community to the causal-learn repository over a specified period.", "expected_outcome": "An increasing number of contributions and extensions indicating ease of development and integration."}, {"hypothesis": "Will incorporating the latest advancements in causal discovery improve the performance of causal-learn over time?", "method": "Periodically test causal-learn on benchmark datasets as new advancements are integrated and compare results to previous versions.", "expected_outcome": "Improved performance metrics over time, reflecting the integration of recent advancements."}, {"hypothesis": "Does offering utilities for graph operations and evaluation metrics in causal-learn enhance the user experience in building causal analysis pipelines?", "method": "Conduct user feedback sessions focusing on the utility features provided by causal-learn and analyze the ease of pipeline development.", "expected_outcome": "Positive feedback regarding the utility features, highlighting their contribution to ease of pipeline development."}], "follow_up_work_ideas": ["Develop additional modules for causal-learn that focus on domain-specific applications, such as genomics or neuroscience, to further broaden its applicability.", "Explore the integration of causal-learn with other Python-based machine learning and data analysis frameworks to create comprehensive data analysis pipelines.", "Investigate the potential for real-time causal discovery in streaming data environments using causal-learn.", "Conduct studies to compare the effectiveness of causal discovery methods implemented in causal-learn across different data distributions and structures.", "Explore the development of a user-friendly graphical interface or visualization tools to complement causal-learn, enhancing accessibility for non-technical users."]}}
{"id": "95333", "url": "https://nips.cc/virtual/2024/poster/95333", "title": "3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors", "authors": [], "abstract": "Abstract:Novel-view synthesis aims to generate novel views of a scene from multiple inputimages or videos, and recent advancements like 3D Gaussian splatting (3DGS)have achieved notable success in producing photorealistic renderings with efficientpipelines. However, generating high-quality novel views under challenging settings,such as sparse input views, remains difficult due to insufficient information inunder-sampled areas, often resulting in noticeable artifacts. This paper presents3DGS-Enhancer, a novel pipeline for enhancing the representation quality of3DGS representations. We leverage 2D video diffusion priors to address thechallenging 3D view consistency problem, reformulating it as achieving temporalconsistency within a video generation process. 3DGS-Enhancer restores view-consistent latent features of rendered novel views and integrates them with theinput views through a spatial-temporal decoder. The enhanced views are thenused to fine-tune the initial 3DGS model, significantly improving its renderingperformance. Extensive experiments on large-scale datasets of unbounded scenesdemonstrate that 3DGS-Enhancer yields superior reconstruction performance andhigh-fidelity rendering results compared to state-of-the-art methods. The projectwebpage is https://xiliu8006.github.io/3DGS-Enhancer-project.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/xiliu8006/3DGS-Enhancer"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_call": "Yes"}, "time_requirements": "2", "code_quality": "Well-documented code", "difficulty": 3, "stars": 170}, "research_ideas": {"problem_statements": ["Generating high-quality novel views under sparse input settings remains challenging, often resulting in noticeable artifacts due to insufficient information in under-sampled areas.", "Existing enhancement methods for novel view synthesis heavily rely on geometric constraints, which can be sensitive to noise.", "There is a lack of methods that specifically focus on enhancing the rendering quality of 3D Gaussian splatting (3DGS) representations."], "main_takeaways": ["3DGS-Enhancer improves the quality of 3D Gaussian splatting representations by leveraging 2D video diffusion priors to address the 3D view consistency problem.", "The proposed pipeline reformulates the 3D-consistent image restoration task as temporally consistent video generation, thus leveraging video LDMs for high-quality and view-consistent images.", "3DGS-Enhancer significantly enhances rendering performance, achieving superior reconstruction performance on various challenging scenes compared to state-of-the-art methods."], "testable_hypotheses": [{"hypothesis": "Does using 2D video diffusion priors enhance the quality of 3D Gaussian splatting representations?", "method": "Compare the performance of 3DGS-Enhancer using 2D video diffusion priors with a baseline 3DGS model without diffusion priors on a large dataset of unbounded scenes.", "expected_outcome": "The 3DGS-Enhancer using diffusion priors will show improved PSNR, SSIM, and LPIPS scores compared to the baseline."}, {"hypothesis": "Will reformulating 3D consistency as temporal consistency improve novel view synthesis results?", "method": "Implement a controlled experiment where 3D consistency is handled traditionally versus the proposed temporal consistency approach and assess the quality of novel views.", "expected_outcome": "The temporal consistency approach will yield higher fidelity and more consistent rendering results."}, {"hypothesis": "Does confidence-aware 3D Gaussian splatting improve the fine-tuning process of 3DGS models?", "method": "Evaluate the rendering quality of 3DGS models fine-tuned with and without confidence-aware strategies on the DL3DV dataset.", "expected_outcome": "Models fine-tuned with confidence-aware strategies will exhibit fewer artifacts and higher rendering quality."}, {"hypothesis": "Can the proposed video diffusion model effectively interpolate between sparse input views to produce high-quality novel views?", "method": "Test the interpolation capabilities of the video diffusion model on synthetic datasets with varying sparsity levels.", "expected_outcome": "The model will demonstrate effective interpolation with high-quality outputs even in highly sparse scenarios."}, {"hypothesis": "Is the 3DGS-Enhancer framework generalizable to different types of unbounded scenes?", "method": "Test the method on datasets with diverse scene types, such as urban, rural, and natural landscapes.", "expected_outcome": "The framework will show consistent improvement across different scene types, demonstrating generalizability."}], "follow_up_work_ideas": ["Explore integrating confidence maps directly with the video generation model to generate images more aligned with real 3D worlds without post-processing.", "Investigate the potential of leveraging 3DGS's data generation capabilities to create a massive dataset for enhancing video generation models' 3D consistency.", "Develop more advanced fine-tuning strategies that consider dynamic confidence adjustments based on real-time feedback during the training process.", "Extend the 3DGS-Enhancer framework to be applicable for single-view 3D model generation.", "Analyze the impact of different types of diffusion priors on the enhancement process and explore alternative prior models for further improvements."]}}
{"id": "98326", "url": "https://nips.cc/virtual/2024/poster/98326", "title": "TorchOpt: An Efficient Library for Differentiable Optimization", "authors": [], "abstract": "Abstract:Differentiable optimization algorithms often involve expensive computations of various meta-gradients. To address this, we design and implement TorchOpt, a new PyTorch-based differentiable optimization library. TorchOpt provides an expressive and unified programming interface that simplifies the implementation of explicit, implicit, and zero-order gradients. Moreover, TorchOpt has a distributed execution runtime capable of parallelizing diverse operations linked to differentiable optimization tasks across CPU and GPU devices. Experimental results demonstrate that TorchOpt achieves a 5.2\u00d7 training time speedup in a cluster. TorchOpt is open-sourced at https://github.com/metaopt/torchopt and has become a PyTorch Ecosystem project.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/metaopt/torchopt"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "At least 8GB RAM", "additional_requirements": "PyTorch installed"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 570}, "research_ideas": {"problem_statements": ["Differentiable optimization algorithms involve expensive computations of various meta-gradients.", "Existing differentiable optimization libraries lack comprehensive support for explicit, implicit, and zero-order gradients and distributed execution.", "There is a need for a library that supports generic bi-level optimization, distributed execution across CPUs and GPUs, and visualization of gradient flows."], "main_takeaways": ["TorchOpt is a new PyTorch-based differentiable optimization library designed to address the computational inefficiencies of differentiable optimization algorithms.", "TorchOpt provides a versatile array of APIs supporting explicit, implicit, and zero-order differentiation modes.", "It has a high-performance distributed execution runtime capable of parallelizing operations across CPU and GPU devices.", "Experimental results demonstrate a 5.2\u00d7 training time speedup using TorchOpt in a cluster environment."], "testable_hypotheses": [{"hypothesis": "TorchOpt achieves better performance than existing libraries for differentiable optimization in terms of speed and efficiency.", "method": "Compare the training time and computational efficiency of TorchOpt against other differentiable optimization libraries using a standard benchmark.", "expected_outcome": "TorchOpt will show a significant reduction in training time and improved computational efficiency due to its comprehensive differentiation modes and distributed execution runtime."}, {"hypothesis": "The use of implicit gradients in TorchOpt leads to more accurate optimization results compared to explicit gradients for certain tasks.", "method": "Evaluate the performance of algorithms using both implicit and explicit gradients on benchmark tasks where stationary conditions are reached.", "expected_outcome": "Implicit gradient methods will yield more accurate optimization results for tasks with stationary conditions."}, {"hypothesis": "Zero-order differentiation in TorchOpt can effectively solve non-differentiable optimization problems.", "method": "Test zero-order differentiation on non-smooth and non-differentiable functions and compare the results with traditional gradient-based methods.", "expected_outcome": "Zero-order differentiation will successfully optimize non-differentiable functions, outperforming traditional gradient-based methods."}, {"hypothesis": "Distributed execution of differentiable optimization tasks using TorchOpt results in linear speedup with an increasing number of GPUs.", "method": "Conduct experiments with varying numbers of GPUs performing differentiable optimization tasks and measure the speedup ratio.", "expected_outcome": "A near-linear speedup in training time will be observed with an increasing number of GPUs."}, {"hypothesis": "TorchOpt's visualization tools improve the understanding and debugging of gradient flows in complex neural network models.", "method": "Conduct a user study comparing the ease of understanding and debugging gradient flows using TorchOpt's visualization tools versus other available tools.", "expected_outcome": "Users will find TorchOpt's visualization tools to be more intuitive and helpful for debugging gradient flows."}], "follow_up_work_ideas": ["Explore the adaptation of TorchOpt for other machine learning frameworks beyond PyTorch, such as TensorFlow or JAX.", "Investigate the optimization of TorchOpt for real-time applications where rapid computation is critical.", "Develop additional visualization features to further simplify the analysis of complex optimization processes in neural networks.", "Test TorchOpt on a broader range of applications, including reinforcement learning and large-scale neural architecture search.", "Enhance the library to support more complex nested optimization problems, potentially involving multiple levels of differentiation."]}}
{"id": "98318", "url": "https://nips.cc/virtual/2024/poster/98318", "title": "BenchMARL: Benchmarking Multi-Agent Reinforcement Learning", "authors": [], "abstract": "Abstract:The field of Multi-Agent Reinforcement Learning (MARL) is currently facing a reproducibility crisis. While solutions for standardized reporting have been proposed to address the issue, we still lack a benchmarking tool that enables standardization and reproducibility, while leveraging cutting-edge Reinforcement Learning (RL) implementations. In this paper, we introduce BenchMARL, the first MARL training library created to enable standardized benchmarking across different algorithms, models, and environments. BenchMARL uses TorchRL as its backend, granting it high-performance and maintained state-of-the-art implementations while addressing the broad community of MARL PyTorch users. Its design enables systematic configuration and reporting, thus allowing users to create and run complex benchmarks from simple one-line inputs. BenchMARL is open-sourced on GitHub at https://github.com/facebookresearch/BenchMARL", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/facebookresearch/BenchMARL"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires pip installation and possibly additional packages)", "resource_requirements": {"GPUs": "1x NVIDIA GPU (e.g., GTX 1080 or better)", "memory": "At least 8GB RAM recommended"}, "time_requirements": "1-2 hours", "code_quality": "Well-documented code with clear instructions", "difficulty": 3, "stars": 351}, "research_ideas": {"problem_statements": ["The field of Multi-Agent Reinforcement Learning (MARL) is facing a reproducibility crisis due to a lack of standardized tools and reporting practices.", "There is a need for a benchmarking tool that enables standardization and reproducibility while leveraging cutting-edge Reinforcement Learning implementations.", "Existing MARL libraries are fragmented and often lack features necessary for state-of-the-art benchmarking, such as vectorized environments."], "main_takeaways": ["BenchMARL is introduced as the first MARL training library for standardized benchmarking across different algorithms, models, and environments.", "BenchMARL uses TorchRL as its backend, enabling high-performance and state-of-the-art RL implementations.", "The library supports easy integration of new algorithms and environments, enabling fair and systematic comparisons.", "BenchMARL addresses reproducibility through standardized configuration and reporting, integrating with statistical tools for rigorous analysis."], "testable_hypotheses": [{"hypothesis": "Does using BenchMARL improve the reproducibility of MARL experiments compared to traditional methods?", "method": "Conduct a series of MARL experiments using BenchMARL and compare the results with those obtained using traditional methods without standardized configurations.", "expected_outcome": "Experiments conducted using BenchMARL will show higher reproducibility and consistency across different runs and setups."}, {"hypothesis": "Will the use of vectorized environments in BenchMARL lead to improved training efficiency for MARL algorithms?", "method": "Run MARL algorithms on vectorized and non-vectorized environments within BenchMARL and compare the training times and sample efficiency.", "expected_outcome": "Vectorized environments will result in faster training times and improved sample efficiency."}, {"hypothesis": "Can BenchMARL's integration with TorchRL provide performance improvements over other existing MARL libraries?", "method": "Benchmark the same set of MARL tasks using BenchMARL and other MARL libraries, comparing performance metrics such as convergence speed and final reward.", "expected_outcome": "BenchMARL will show better performance metrics due to the high-performance backend of TorchRL."}, {"hypothesis": "Does the use of centralized critics in MARL algorithms lead to better performance on cooperative tasks?", "method": "Compare the performance of MARL algorithms with and without centralized critics on cooperative tasks available in BenchMARL.", "expected_outcome": "Algorithms with centralized critics will outperform those without on cooperative tasks due to better utilization of global state information."}, {"hypothesis": "Will providing public benchmarks and fine-tuned hyperparameters improve the adoption of BenchMARL in the MARL community?", "method": "Track the usage metrics of BenchMARL and community engagement before and after releasing public benchmarks and hyperparameters.", "expected_outcome": "There will be an increase in community engagement and usage of BenchMARL after the release of public benchmarks and hyperparameters."}], "follow_up_work_ideas": ["Extend BenchMARL to include more diverse environments and tasks, covering a wider range of MARL scenarios.", "Investigate the integration of JAX-based MARL libraries with BenchMARL to explore cross-framework benchmarking.", "Explore the development of automated hyperparameter tuning within BenchMARL to further simplify benchmark setup.", "Evaluate the robustness of BenchMARL across different computational setups (e.g., different GPUs, cloud environments).", "Apply BenchMARL to emerging MARL applications such as autonomous driving or robotic swarm control to assess its applicability and scalability."]}}
{"id": "95818", "url": "https://nips.cc/virtual/2024/poster/95818", "title": "Classification Done Right for Vision-Language Pre-Training", "authors": [], "abstract": "Abstract:We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data. Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised classification labels, without the need for additional text filtering or selection. Due to the absence of the text encoding as contrastive target, SuperClass does not require a text encoder and does not need to maintain a large batch size as CLIP does. SuperClass demonstrated superior performance on various downstream tasks, including classic computer vision benchmarks and vision language downstream tasks. We further explored the scaling behavior of SuperClass on model size, training length, or data size, and reported encouraging results and comparisons to CLIP. https://github.com/x-cls/superclass", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/x-cls/superclass"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA V100", "amount": 1}, "memory": "16 GB", "api_calls": "N/A"}, "time_requirements": 4, "code_quality": "well-documented code", "difficulty": 3, "stars": 200}, "research_ideas": {"problem_statements": ["The high computational demand of CLIP limits accessibility for researchers with limited resources.", "Previous classification methods lack scalability and do not demonstrate performance comparable to CLIP.", "The need for a simpler classification approach that eliminates the need for large contrastive batch sizes and text encoders.", "The potential loss of valuable information due to text preprocessing in classification methods."], "main_takeaways": ["SuperClass is a classification method for vision-language pre-training that uses tokenized raw text as labels, eliminating the need for a text encoder.", "SuperClass demonstrated superior performance on various downstream tasks compared to contrastive methods like CLIP.", "The method effectively scales with model size, training length, and data size, showing competitive or superior scaling behavior.", "SuperClass maintains all information from the original text as supervision signals, avoiding the need for manual text preprocessing."], "testable_hypotheses": [{"hypothesis": "SuperClass achieves competitive performance without requiring a text encoder.", "method": "Compare the performance of SuperClass and CLIP on standard datasets without using a text encoder for SuperClass.", "expected_outcome": "SuperClass will perform comparably to CLIP on both image classification and vision-language tasks."}, {"hypothesis": "Using raw text tokens as labels preserves more useful information for model training than preprocessed text.", "method": "Train models using SuperClass with raw text tokens and with preprocessed text on the same datasets, and compare their performance.", "expected_outcome": "SuperClass with raw text tokens will perform better due to the preservation of more informational content."}, {"hypothesis": "SuperClass scales better with dataset size compared to contrastive methods.", "method": "Evaluate the performance of SuperClass and CLIP with increasing amounts of training data.", "expected_outcome": "SuperClass will show improved performance metrics with larger datasets, demonstrating better scalability."}, {"hypothesis": "Inverse Document Frequency (IDF) improves the performance of SuperClass by weighting information content.", "method": "Train SuperClass models with and without IDF weighting and compare their performance on downstream tasks.", "expected_outcome": "Models using IDF weighting will outperform those without it, due to better handling of information significance."}, {"hypothesis": "SuperClass trained models show better integration capabilities with large language models compared to CLIP.", "method": "Combine SuperClass and CLIP pretrained models with a large language model like GPT-2, and evaluate on vision-language tasks.", "expected_outcome": "SuperClass models will demonstrate superior performance in tasks requiring vision-language integration."}], "follow_up_work_ideas": ["Investigate methods to incorporate word order and object relationships into SuperClass to improve its understanding capabilities.", "Explore the use of SuperClass in other domains beyond vision-language tasks, such as audio-visual or text-only tasks.", "Develop techniques to enhance the interpretability of SuperClass predictions to better understand the model's decision-making process.", "Apply SuperClass to real-time applications to evaluate its efficiency and performance in dynamic environments.", "Study the impact of different tokenizer types and vocabularies on SuperClass performance in diverse languages and dialects."]}}
{"id": "95974", "url": "https://nips.cc/virtual/2024/poster/95974", "title": "Reasoning Multi-Agent Behavioral Topology for Interactive Autonomous Driving", "authors": [], "abstract": "Abstract:Autonomous driving system aims for safe and social-consistent driving through the behavioral integration among interactive agents. However, challenges remain due to multi-agent scene uncertainty and heterogeneous interaction. Current dense and sparse behavioral representations struggle with inefficiency and inconsistency in multi-agent modeling, leading to instability of collective behavioral patterns when integrating prediction and planning (IPP). To address this, we initiate a topological formation that serves as a compliant behavioral foreground to guide downstream trajectory generations. Specifically, we introduce Behavioral Topology (BeTop), a pivotal topological formulation that explicitly represents the consensual behavioral pattern among multi-agent future. BeTop is derived from braid theory to distill compliant interactive topology from multi-agent future trajectories. A synergistic learning framework (BeTopNet) supervised by BeTop facilitates the consistency of behavior prediction and planning within the predicted topology priors. Through imitative contingency learning, BeTop also effectively manages behavioral uncertainty for prediction and planning. Extensive verification on large-scale real-world datasets, including nuPlan and WOMD, demonstrates that BeTop achieves state-of-the-art performance in both prediction and planning tasks. Further validations on the proposed interactive scenario benchmark showcase planning compliance in interactive cases. Code and model is available at https://github.com/OpenDriveLab/BeTop.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/OpenDriveLab/BeTop"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 NVIDIA GPU recommended", "memory": "8 GB RAM minimum", "API_calls": "N/A"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 2, "stars": 107}, "research_ideas": {"problem_statements": ["Current dense and sparse behavioral representations struggle with inefficiency and inconsistency in multi-agent modeling, leading to instability of collective behavioral patterns when integrating prediction and planning (IPP).", "Challenges remain due to multi-agent scene uncertainty and heterogeneous interaction in autonomous driving systems.", "Substantial challenges arise in real-world cases due to scene uncertainty and volatile interactive patterns for multi-agent future behaviors."], "main_takeaways": ["BeTop introduces a topological formation that serves as a compliant behavioral foreground to guide downstream trajectory generations.", "BeTop is derived from braid theory to distill compliant interactive topology from multi-agent future trajectories.", "BeTopNet, a synergistic learning framework supervised by BeTop, facilitates the consistency of behavior prediction and planning within the predicted topology priors.", "BeTop effectively manages behavioral uncertainty for prediction and planning through imitative contingency learning.", "BeTop achieves state-of-the-art performance in both prediction and planning tasks on large-scale real-world datasets."], "testable_hypotheses": [{"hypothesis": "Does using BeTop improve prediction accuracy compared to existing dense and sparse representations?", "method": "Compare the prediction accuracy of BeTop against traditional dense and sparse representations on a large-scale dataset.", "expected_outcome": "BeTop will outperform traditional dense and sparse representations in prediction accuracy."}, {"hypothesis": "Can BeTopNet reduce the computational complexity compared to other multi-agent modeling methods?", "method": "Evaluate and compare the computational complexity of BeTopNet with other multi-agent modeling methods on similar tasks.", "expected_outcome": "BeTopNet will have reduced computational complexity due to its topological approach."}, {"hypothesis": "Will BeTopNet improve planning compliance in interactive driving scenarios?", "method": "Test BeTopNet on interactive driving scenarios and measure planning compliance compared to other models.", "expected_outcome": "BeTopNet will show improved planning compliance in interactive driving scenarios."}, {"hypothesis": "Does the integration of BeTop with a contingency planning paradigm enhance safety in autonomous driving?", "method": "Implement a contingency planning paradigm with BeTop and compare safety metrics against a baseline without BeTop.", "expected_outcome": "The integration with BeTop will enhance safety metrics in autonomous driving."}, {"hypothesis": "Can BeTop manage behavioral uncertainty more effectively than existing methods?", "method": "Measure the ability of BeTop to manage behavioral uncertainty compared to existing methods using standard uncertainty metrics.", "expected_outcome": "BeTop will manage behavioral uncertainty more effectively than existing methods."}], "follow_up_work_ideas": ["Develop a recursive version of BeTop for multi-step, multi-agent reasoning and coordination.", "Explore the connectivity of BeTop upon perceptions as tracking for the end-to-end paradigm.", "Extend BeTop to reason behaviors under 3D scenarios for multiple autonomous agents.", "Investigate the potential of BeTop in enhancing naturalistic and efficient decision-making for multiple autonomous agents.", "Examine the effects of using BeTop in different autonomous driving environments and conditions to generalize its applicability."]}}
{"id": "97514", "url": "https://nips.cc/virtual/2024/poster/97514", "title": "HEST-1k: A Dataset For Spatial Transcriptomics and Histology Image Analysis", "authors": [], "abstract": "Abstract:Spatial transcriptomics enables interrogating the molecular composition of tissue with ever-increasing resolution and sensitivity. However, costs, rapidly evolving technology, and lack of standards have constrained computational methods in ST to narrow tasks and small cohorts. In addition, the underlying tissue morphology, as reflected by H&E-stained whole slide images (WSIs), encodes rich information often overlooked in ST studies. Here, we introduce HEST-1k, a collection of 1,229 spatial transcriptomic profiles, each linked to a WSI and extensive metadata. HEST-1k was assembled from 153 public and internal cohorts encompassing 26 organs, two species (Homo Sapiens and Mus Musculus), and 367 cancer samples from 25 cancer types. HEST-1k processing enabled the identification of 2.1 million expression-morphology pairs and over 76 million nuclei. To support its development, we additionally introduce the HEST-Library, a Python package designed to perform a range of actions with HEST samples. We test HEST-1k and Library on three use cases: (1) benchmarking foundation models for pathology (HEST-Benchmark), (2) biomarker exploration, and (3) multimodal representation learning. HEST-1k, HEST-Library, and HEST-Benchmark can be freely accessed at https://github.com/mahmoodlab/hest.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/mahmoodlab/hest"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "2GB", "additional": "Requires additional libraries for WSI manipulation and GPU acceleration."}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 236}, "research_ideas": {"problem_statements": ["The high costs and rapidly evolving technology in spatial transcriptomics (ST) have constrained computational methods to narrow tasks and small cohorts.", "There is a lack of standardized resources and unified formats for handling ST data, limiting the development of deep learning models on a large scale.", "The rich information encoded in tissue morphology, as visualized in H&E-stained whole slide images, is often overlooked in ST studies.", "There is a need for new, diverse, and challenging benchmarks for foundation models in pathology beyond diagnostic tasks."], "main_takeaways": ["HEST-1k is introduced as a comprehensive dataset comprising 1,229 spatial transcriptomic profiles paired with whole-slide images and extensive metadata, covering 26 organs, two species, and 25 cancer types.", "HEST-1k processing enabled the identification of 2.1 million expression\u2013morphology pairs and over 76 million nuclei.", "HEST-Library is a Python package designed to facilitate interaction with HEST samples, supporting tasks such as data assembly, visualization, and running benchmarks.", "The dataset is utilized for three use cases: benchmarking foundation models for pathology, biomarker exploration, and multimodal representation learning.", "HEST-Benchmark is a set of nine tasks designed to evaluate the performance of foundation models in predicting gene expression from histology images."], "testable_hypotheses": [{"hypothesis": "Does the use of HEST-1k improve the performance of foundation models in predicting gene expression from histology compared to existing datasets?", "method": "Benchmark multiple foundation models on HEST-Benchmark tasks and compare their performance with results from existing datasets.", "expected_outcome": "HEST-1k, due to its diversity and comprehensiveness, will lead to improved model performance in predicting gene expression."}, {"hypothesis": "Does multimodal alignment using HEST-1k enhance the expressivity of patch embeddings for specific cancer types?", "method": "Fine-tune a pretrained model using multimodal contrastive alignment on HEST-1k and evaluate on an independent cancer cohort for molecular subtyping.", "expected_outcome": "Fine-tuning with multimodal alignment will yield better tissue-specific patch encoders, enhancing performance on molecular subtyping tasks."}, {"hypothesis": "Can nuclear morphological features predict specific gene expressions in cancer samples?", "method": "Correlate nuclear morphological features from CellViT segmentation with gene expression levels in breast cancer samples using Pearson correlation.", "expected_outcome": "Certain nuclear morphological features, such as area and shape, will show moderate to strong correlation with specific gene expressions."}, {"hypothesis": "Does increasing the size of training data improve the performance of foundation models in the HEST-Benchmark tasks?", "method": "Analyze the relationship between the number of training patches used for pretraining each model and their performance on HEST-Benchmark tasks.", "expected_outcome": "Performance will correlate positively with the amount of training data, although gains may diminish logarithmically."}, {"hypothesis": "Are there specific cancer types for which morphology does not reflect gene expression changes as accurately?", "method": "Evaluate the performance of foundation models across different cancer types in HEST-Benchmark and analyze the variability in Pearson correlation coefficients.", "expected_outcome": "Certain cancer types, like rectum cancer (READ), may show lower performance, suggesting morphology may not always reflect gene expression accurately."}], "follow_up_work_ideas": ["Expand HEST-1k with new datasets as they become available to increase its diversity and applicability.", "Explore the development of novel multimodal learning techniques that can leverage both histology and transcriptomics data more effectively.", "Investigate the potential of using HEST-1k for training models for new predictive tasks, such as drug response prediction in cancer therapy.", "Develop standardized protocols for batch effect mitigation in spatial transcriptomics data to enhance model robustness.", "Apply the methodologies and findings from HEST-1k to other domains, such as neurological diseases, to explore tissue-specific biomarker discovery."]}}
{"id": "97649", "url": "https://nips.cc/virtual/2024/poster/97649", "title": "JaxMARL: Multi-Agent RL Environments and Algorithms in JAX", "authors": [], "abstract": "Abstract:Benchmarks are crucial in the development of machine learning algorithms, significantly influencing reinforcement learning (RL) research through the available environments. Traditionally, RL environments run on the CPU, which limits their scalability with the computational resources typically available in academia. However, recent advancements in JAX have enabled the wider use of hardware acceleration, enabling massively parallel RL training pipelines and environments. While this has been successfully applied to single-agent RL, it has not yet been widely adopted for multi-agent scenarios. In this paper, we present JaxMARL, the first open-source, easy-to-use code base that combines GPU-enabled efficiency with support for a large number of commonly used MARL environments and popular baseline algorithms. Our experiments show that, in terms of wall clock time, our JAX-based training pipeline is up to 12,500 times faster than existing approaches. This enables efficient and thorough evaluations, potentially alleviating the evaluation crisis in the field. We also introduce and benchmark SMAX, a vectorised, simplified version of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. The code is available at https://github.com/flairox/jaxmarl.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/flairox/jaxmarl"}, "reproduce_difficulty": {"environment_setup": "Moderate (pip for installation, Docker available)", "resource_requirements": {"GPUs": "1 NVIDIA GPU (recommended)", "memory": "8GB RAM minimum", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "Well-documented", "difficulty": 3, "stars": 520}, "research_ideas": {"problem_statements": ["The paper addresses the need for scalable and efficient multi-agent reinforcement learning (MARL) environments and algorithms that leverage hardware acceleration.", "There is a lack of standardization in evaluation protocols for MARL, leading to biased comparisons and unclear progress markers.", "Traditional CPU-based RL environments limit scalability and runtime efficiency, especially for multi-agent scenarios.", "Existing MARL environments like SMAC rely on the StarCraft II engine, which is slow and memory-intensive."], "main_takeaways": ["JaxMARL provides a JAX-based library that combines GPU-enabled efficiency with a wide range of MARL environments and algorithms.", "JAX-based training pipelines significantly reduce wall clock time, enabling faster and more thorough evaluations.", "The library introduces SMAX, a JAX-based reimplementation of the StarCraft Multi-Agent Challenge that does not require the StarCraft II engine, allowing for greater flexibility and speed.", "JaxMARL facilitates rapid evaluations across diverse environments, addressing the evaluation crisis in MARL by allowing for more comprehensive testing."], "testable_hypotheses": [{"hypothesis": "JAX-based environments provide significant speedup over CPU-based implementations.", "method": "Benchmark the runtime of JAX-based environments against traditional CPU-based environments using random actions.", "expected_outcome": "JAX-based environments will achieve several orders of magnitude speedup over CPU-based counterparts."}, {"hypothesis": "Using SMAX instead of SMAC will decrease training time without sacrificing performance.", "method": "Compare the training time and performance (win rate) of MARL algorithms on SMAX versus SMAC.", "expected_outcome": "SMAX will significantly reduce training time while maintaining comparable performance to SMAC."}, {"hypothesis": "Vectorized training runs on JAX will allow for substantial parallelization gains in MARL experiments.", "method": "Conduct experiments with varying numbers of parallel training runs using JAX, measuring speed and performance.", "expected_outcome": "JAX will enable thousands of parallel training runs with substantial speedup compared to sequential runs."}, {"hypothesis": "PPO-based methods will outperform Q-Learning based methods in terms of both speed and performance in JAX-based MARL environments.", "method": "Evaluate and compare the performance and training speed of PPO and Q-Learning algorithms across multiple environments.", "expected_outcome": "PPO will show better performance and faster training times than Q-Learning methods."}, {"hypothesis": "JAX-based MARL environments will facilitate improved evaluation protocols by allowing testing across a broader set of environments.", "method": "Analyze the diversity and number of environments used in published MARL studies before and after the introduction of JaxMARL.", "expected_outcome": "Studies using JaxMARL will feature evaluations across a more diverse set of environments."}], "follow_up_work_ideas": ["Develop novel MARL environments that push the boundaries of current capabilities and provide new challenges for the community.", "Explore the impact of hardware-accelerated training on the robustness and generalization of MARL algorithms.", "Investigate the use of JAX for optimizing other computationally intensive components of MARL, such as policy evaluation or exploration strategies.", "Apply JaxMARL to new domains beyond traditional benchmarks, such as finance or autonomous vehicles, to evaluate its applicability and effectiveness.", "Build upon JaxMARL by integrating more complex agent behaviors, such as hierarchical or meta-learning strategies, to explore their performance in accelerated environments."]}}
{"id": "97713", "url": "https://nips.cc/virtual/2024/poster/97713", "title": "WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks", "authors": [], "abstract": "Abstract:The ability of large language models (LLMs) to mimic human-like intelligence has led to a surge in LLM-based autonomous agents. Though recent LLMs seem capable of planning and reasoning given user instructions, their effectiveness in applying these capabilities for autonomous task solving remains underexplored. This is especially true in enterprise settings, where automated agents hold the promise of a high impact. To fill this gap, we propose WorkArena++, a novel benchmark consisting of 682 tasks corresponding to realistic workflows routinely performed by knowledge workers. WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents. Our empirical studies across state-of-the-art LLMs and vision-language models (VLMs), as well as human workers, reveal several challenges for such models to serve as useful assistants in the workplace. In addition to the benchmark, we provide a mechanism to effortlessly generate thousands of ground-truth observation/action traces, which can be used for fine-tuning existing models. Overall, we expect this work to serve as a useful resource to help the community progress towards capable autonomous agents. The benchmark can be found at https://github.com/ServiceNow/WorkArena.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/ServiceNow/WorkArena"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "1", "memory": "8GB", "CPU": "4 cores", "API_calls": "Yes"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 164}, "research_ideas": {"problem_statements": ["How effective are large language models (LLMs) in planning and reasoning to solve autonomous tasks in enterprise settings?", "Can LLM-based autonomous agents be as effective as human workers in performing knowledge work tasks?", "What are the current challenges faced by LLMs and VLMs in workplace automation tasks?", "How can benchmarks like WorkArena++ help in furthering the development of capable autonomous agents?"], "main_takeaways": ["WorkArena++ is a new benchmark designed to evaluate the capabilities of web agents in solving complex knowledge work tasks.", "The benchmark consists of 682 tasks that test planning, problem-solving, logical reasoning, retrieval, and contextual understanding.", "Current state-of-the-art LLMs and VLMs struggle significantly with the tasks in WorkArena++, while human workers can perform them with high success.", "The benchmark provides a mechanism for generating fine-tuning data, facilitating the improvement of model capabilities.", "WorkArena++ highlights the gap between human and AI performance in enterprise task automation and serves as a tool for the community to address these challenges."], "testable_hypotheses": [{"hypothesis": "Does providing LLMs with multimodal inputs (e.g., screenshots) improve their performance on complex tasks?", "method": "Evaluate LLMs like GPT-4o with and without vision inputs on WorkArena++ tasks.", "expected_outcome": "Models with multimodal inputs should perform better, especially on tasks involving visual information retrieval."}, {"hypothesis": "Will increasing the context length available to LLMs improve their performance on tasks requiring memorization?", "method": "Test LLMs with different context lengths on tasks specifically requiring sophisticated memorization.", "expected_outcome": "A longer context length should improve performance on memory-intensive tasks."}, {"hypothesis": "Can fine-tuning LLMs on WorkArena++ traces improve their ability to solve benchmark tasks?", "method": "Fine-tune models on observation-action traces generated from WorkArena++ and evaluate their performance.", "expected_outcome": "Fine-tuned models should show improved task completion rates compared to zero-shot models."}, {"hypothesis": "Does the complexity of task instructions (explicit vs. implicit) affect the performance of LLMs differently?", "method": "Compare LLM performance on L2 (explicit instructions) vs. L3 (implicit instructions) tasks.", "expected_outcome": "LLMs should perform better on L2 tasks due to more explicit guidance."}, {"hypothesis": "Are there unexplored configurations or hyperparameters that could significantly improve LLM performance on enterprise tasks?", "method": "Conduct a hyperparameter optimization study using WorkArena++ as the evaluation benchmark.", "expected_outcome": "Optimized hyperparameters should yield better performance, particularly in decision-making and reasoning tasks."}], "follow_up_work_ideas": ["Develop new task sets focusing on safety and cybersecurity to ensure agent robustness in enterprise settings.", "Explore expanding WorkArena++ to include tasks that involve interaction with multiple software environments.", "Investigate the potential of collaborative tasks where agents must work with humans or other agents to complete workflows.", "Create more complex workflows that require longer-term planning and reasoning for task completion.", "Assess the impact of different training data sizes and sources on the performance of LLMs in complex task automation."]}}
{"id": "93219", "url": "https://nips.cc/virtual/2024/poster/93219", "title": "HAWK: Learning to Understand Open-World Video Anomalies", "authors": [], "abstract": "Abstract:Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs. However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction. Additionally, the prevalent data scarcity in existing datasets restricts their applicability in open-world scenarios.In this paper, we introduce HAWK, a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely. Recognizing the difference in motion information between abnormal and normal videos, HAWK explicitly integrates motion modality to enhance anomaly identification. To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality. Moreover, to improve the interpretation of motion-to-language, we establish a clear supervisory relationship between motion and its linguistic representation. Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users' open-world questions. The final results demonstrate that HAWK achieves SOTA performance, surpassing existing baselines in both video description generation and question-answering. Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/jqtangust/hawk"}, "reproduce_difficulty": {"environment_setup": "Moderate difficulty, requires conda for environment setup and installation of dependencies from an environment.yml file.", "resource_requirements": {"GPUs": "4 x RTX A6000 48G", "memory": "Not specified, but requires high-performance GPUs for training.", "API_calls": "Not specified"}, "time_requirements": "Approximately 5-10 hours for training based on the dataset size and model complexity.", "code_quality": "Well-documented code with clear instructions.", "difficulty": 3, "stars": 177}, "research_ideas": {"problem_statements": ["Current Video Anomaly Detection (VAD) systems are limited by their superficial semantic understanding of scenes and minimal user interaction.", "Prevalent data scarcity in existing datasets restricts the applicability of VAD systems in open-world scenarios.", "There is a lack of robust training data for video anomaly explanation, which constrains practical applicability.", "Existing methods focus more on acquiring long-range context information rather than anomaly-related features."], "main_takeaways": ["HAWK is a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely.", "The framework integrates motion modality to enhance anomaly identification and constructs an auxiliary consistency loss to guide motion attention.", "Over 8,000 anomaly videos have been annotated with language descriptions, and 8,000 question-answering pairs have been created for training.", "HAWK achieves state-of-the-art (SOTA) performance, surpassing existing baselines in both video description generation and question-answering tasks."], "testable_hypotheses": [{"hypothesis": "Integrating motion modality will improve the accuracy of anomaly detection.", "method": "Compare anomaly detection accuracy of HAWK with and without motion modality integration.", "expected_outcome": "HAWK with motion modality integration will show higher accuracy."}, {"hypothesis": "The use of auxiliary consistency loss will improve the focus on motion-related features.", "method": "Evaluate the model's attention maps with and without auxiliary consistency loss.", "expected_outcome": "The model with auxiliary consistency loss will show increased focus on motion-related features."}, {"hypothesis": "The annotated dataset will improve the model's ability to generate accurate video descriptions.", "method": "Train the model with and without the annotated dataset and compare description accuracy.", "expected_outcome": "Using the annotated dataset will result in more accurate video descriptions."}, {"hypothesis": "HAWK will outperform existing models in open-world video anomaly understanding tasks.", "method": "Benchmark HAWK against existing models using open-world video anomaly datasets.", "expected_outcome": "HAWK will demonstrate superior performance in open-world scenarios."}, {"hypothesis": "Fine-tuning on specific video anomaly datasets will improve HAWK's generalization to open-world scenarios.", "method": "Compare HAWK's performance on open-world scenarios with and without fine-tuning on specific datasets.", "expected_outcome": "Fine-tuning will enhance the model's generalization capabilities."}], "follow_up_work_ideas": ["Explore the integration of more diverse anomaly datasets to further improve model generalization in open-world scenarios.", "Investigate the use of additional modalities, such as audio, to enhance anomaly detection capabilities.", "Develop methods to reduce model hallucination and improve the accuracy of semantic interpretation.", "Explore the potential of using HAWK in real-time video anomaly detection and its efficiency in streaming data.", "Investigate the impact of using large-scale pre-trained models on anomaly detection accuracy and efficiency."]}}
{"id": "94065", "url": "https://nips.cc/virtual/2024/poster/94065", "title": "NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface Reconstruction", "authors": [], "abstract": "Abstract:Signed Distance Function (SDF)-based volume rendering has demonstrated significant capabilities in surface reconstruction. Although promising, SDF-based methods often fail to capture detailed geometric structures, resulting in visible defects. By comparing SDF-based volume rendering to density-based volume rendering, we identify two main factors within the SDF-based approach that degrade surface quality: SDF-to-density representation and geometric regularization. These factors introduce challenges that hinder the optimization of the SDF field. To address these issues, we introduce NeuRodin, a novel two-stage neural surface reconstruction framework that not only achieves high-fidelity surface reconstruction but also retains the flexible optimization characteristics of density-based methods.   NeuRodin incorporates innovative strategies that facilitate transformation of arbitrary topologies and reduce artifacts associated with density bias.  Extensive evaluations on the Tanks and Temples and ScanNet++ datasets demonstrate the superiority of NeuRodin, showing strong reconstruction capabilities for both indoor and outdoor environments using solely posed RGB captures. Project website:https://open3dvlab.github.io/NeuRodin/", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/open3dvlab/NeuRodin"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires conda and specific CUDA version)", "resource_requirements": {"GPUs": {"type": "NVIDIA GPU", "amount": "1 or more"}, "memory": "At least 8GB", "additional": "Requires specific datasets from Tanks and Temples and ScanNet++"}, "time_requirements": "2-3 hours", "code_quality": "Well-documented code with clear instructions", "difficulty": 3, "stars": 117}, "research_ideas": {"problem_statements": ["SDF-based volume rendering methods often fail to capture detailed geometric structures, resulting in visible defects.", "The conversion from SDF to density in volume rendering frameworks introduces representation challenges and biases.", "Geometric regularization constraints in SDF-based methods limit topological changes and hinder model convergence.", "Existing benchmarks and methods do not adequately evaluate or optimize surface reconstruction in both indoor and outdoor environments using only RGB captures."], "main_takeaways": ["NeuRodin is a novel two-stage framework that enhances high-fidelity neural surface reconstruction using posed RGB captures.", "The framework addresses the limitations of SDF-based methods by refining SDF-to-density conversion and introducing a novel loss function for better alignment.", "NeuRodin employs a two-stage optimization process to minimize over-regularization and achieve smooth surfaces.", "Experimental results demonstrate NeuRodin's superior performance over state-of-the-art methods on Tanks and Temples and ScanNet++ datasets."], "testable_hypotheses": [{"hypothesis": "Does transitioning from a global to a local adaptive SDF-to-density conversion improve surface reconstruction accuracy?", "method": "Conduct comparative experiments using both global and local adaptive conversion methods on the same dataset.", "expected_outcome": "Local adaptive conversion will yield better reconstruction accuracy, capturing more intricate geometric details."}, {"hypothesis": "Can explicit bias correction in SDF-based rendering enhance the alignment of geometric representations?", "method": "Implement explicit bias correction in the SDF rendering pipeline and assess the alignment accuracy of geometric representations.", "expected_outcome": "Explicit bias correction will lead to improved alignment and reduced geometric inaccuracies."}, {"hypothesis": "Does a two-stage optimization process reduce the impact of over-regularization in surface reconstruction?", "method": "Apply a two-stage optimization method to a dataset and compare results with single-stage optimization methods.", "expected_outcome": "The two-stage process will demonstrate reduced over-regularization and more accurate surface representation."}, {"hypothesis": "Will NeuRodin outperform Neuralangelo in reconstructing fine-grained details with fewer parameters?", "method": "Benchmark both NeuRodin and Neuralangelo on a dataset with complex topologies and compare the reconstruction quality and parameter efficiency.", "expected_outcome": "NeuRodin will outperform Neuralangelo in both detail reconstruction and parameter efficiency."}, {"hypothesis": "Are stochastic-step numerical gradient estimations effective in maintaining natural zero level sets?", "method": "Evaluate the impact of stochastic-step numerical gradient estimation by comparing reconstruction results with analytical gradients.", "expected_outcome": "Stochastic-step estimations will maintain more natural zero level sets compared to analytical gradients."}], "follow_up_work_ideas": ["Explore additional adaptive density conversion techniques to further enhance surface detail capture.", "Investigate the application of NeuRodin for reconstructing dynamic scenes with moving objects.", "Develop new benchmarks that incorporate more diverse and challenging environments to test reconstruction methods.", "Examine the potential of integrating NeuRodin with real-time applications, such as augmented reality or live 3D modeling.", "Assess the robustness of NeuRodin under different lighting conditions and image quality variations."]}}
{"id": "94388", "url": "https://nips.cc/virtual/2024/poster/94388", "title": "Return of Unconditional Generation: A Self-supervised Representation Generation Method", "authors": [], "abstract": "Abstract:Unconditional generation -- the problem of modeling data distribution without relying on human-annotated labels -- is a long-standing and fundamental challenge in generative models, creating a potential of learning from large-scale unlabeled data. In the literature, the generation quality of an unconditional method has been much worse than that of its conditional counterpart. This gap can be attributed to the lack of semantic information provided by labels. In this work, we show that one can close this gap by generating semantic representations in the representation space produced by a self-supervised encoder. These representations can be used to condition the image generator. This framework, called Representation-Conditioned Generation (RCG), provides an effective solution to the unconditional generation problem without using labels. Through comprehensive experiments, we observe that RCG significantly improves unconditional generation quality: e.g., it achieves a new state-of-the-art FID of 2.15 on ImageNet 256x256, largely reducing the previous best of 5.91 by a relative 64%. Our unconditional results are situated in the same tier as the leading class-conditional ones. We hope these encouraging observations will attract the community's attention to the fundamental problem of unconditional generation. Code is available athttps://github.com/LTH14/rcg.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/LTH14/rcg", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/LTH14/rcg"}, "reproduce_difficulty": {"environment_setup": "conda", "resource_requirements": {"GPUs": {"type": "NVIDIA V100", "amount": 4}, "memory": "not specified", "API_calls": "not specified"}, "time_requirements": 200, "code_quality": "well-documented code", "difficulty": 3, "stars": 905}, "research_ideas": {"problem_statements": ["Unconditional generation quality is significantly worse than conditional generation due to the lack of semantic information provided by labels.", "There is a need to utilize large-scale unlabeled data for learning complex distributions without relying on human annotations.", "The current gap between unconditional and conditional generation limits the potential of generative models to learn from unannotated data."], "main_takeaways": ["The Representation-Conditioned Generation (RCG) framework improves unconditional generation by conditioning the image generator on semantic representations from a self-supervised encoder.", "RCG achieves a new state-of-the-art FID of 2.15 on ImageNet 256\u00d7256, significantly reducing the previous best.", "RCG's performance in unconditional generation is comparable to leading class-conditional methods, effectively bridging the performance gap.", "RCG decomposes the complex task of image generation into simpler tasks: generating a representation and then generating an image conditioned on that representation."], "testable_hypotheses": [{"hypothesis": "RCG framework improves the FID score for different types of image generators.", "method": "Evaluate RCG using various image generators like ADM, LDM, and MAGE and compare FID scores with and without RCG.", "expected_outcome": "RCG will significantly reduce the FID score for all tested image generators."}, {"hypothesis": "Self-supervised representations can provide semantic information comparable to class labels for image generation.", "method": "Compare the image quality generated using self-supervised representations against those using class labels as conditions.", "expected_outcome": "Image quality with self-supervised representations will be comparable to or better than those with class labels."}, {"hypothesis": "RCG's performance gains are consistent across different datasets.", "method": "Evaluate RCG on datasets like CIFAR-10 and iNaturalist and compare the results with other unconditional generation methods.", "expected_outcome": "RCG will consistently outperform other methods across different datasets."}, {"hypothesis": "Increasing the dimensionality of representations used in RCG improves generation quality.", "method": "Experiment with different output dimensions of the pre-trained encoder's projection head and evaluate the FID score.", "expected_outcome": "There will be an optimal dimension that provides the best balance between information richness and computational feasibility."}, {"hypothesis": "Guidance in RCG can further improve generation quality.", "method": "Implement classifier-free guidance in the RCG framework and compare FID and IS scores with the unguided version.", "expected_outcome": "Guided RCG will have better FID and IS scores compared to the unguided version."}], "follow_up_work_ideas": ["Explore the use of RCG in generating other media types such as audio or video.", "Investigate the impact of different self-supervised learning methods on RCG performance.", "Optimize the representation generator for faster training and inference times.", "Study the effect of using different architectures for the self-supervised encoder in RCG.", "Develop methods to dynamically adjust the guidance scale during generation for improved image quality."]}}
{"id": "96264", "url": "https://nips.cc/virtual/2024/poster/96264", "title": "Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models", "authors": [], "abstract": "Abstract:We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs). Specifically, we propose meta-buffer to store a series of informative high-level thoughts, namely thought-template, distilled from the problem-solving processes across various tasks. Then for each problem, we retrieve a relevant thought-template and adaptively instantiate it with specific reasoning structures to conduct efficient reasoning. To guarantee the scalability and stability, we further propose buffer-manager to dynamically update the meta-buffer, thus enhancing the capacity of meta-buffer as more tasks are solved. We conduct extensive experiments on 10 challenging reasoning-intensive tasks, and achieve significant performance improvements over previous SOTA methods: 11\\% on Game of 24, 20\\% on Geometric Shapes and 51\\% on Checkmate-in-One. Further analysis demonstrate the superior generalization ability and model robustness of our BoT, while requiring only 12\\% of the cost of multi-query prompting methods (e.g., tree/graph of thoughts) on average. Code is available at: https://github.com/YangLing0818/buffer-of-thought-llm", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/YangLing0818/buffer-of-thought-llm"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_call": true}, "time_requirements": 1, "code_quality": "well-documented code", "difficulty": 3, "stars": 608}, "research_ideas": {"problem_statements": ["How can we enhance the accuracy, efficiency, and robustness of large language models (LLMs) in reasoning tasks?", "What are the limitations of existing single-query and multi-query reasoning methods for LLMs?", "Can a thought-augmented reasoning framework improve reasoning tasks and reduce computational costs?"], "main_takeaways": ["The Buffer of Thoughts (BoT) framework introduces a meta-buffer to store high-level thought templates for reasoning tasks, enhancing LLM performance.", "BoT achieves significant performance improvements over state-of-the-art methods on various reasoning-intensive tasks, with substantial accuracy gains.", "The method demonstrates superior generalization ability and model robustness while reducing the cost of multi-query prompting methods."], "testable_hypotheses": [{"hypothesis": "Does the meta-buffer enhance LLM reasoning accuracy compared to standard methods?", "method": "Conduct experiments comparing BoT with and without the meta-buffer across various reasoning tasks, measuring accuracy.", "expected_outcome": "The meta-buffer will significantly enhance reasoning accuracy in LLMs."}, {"hypothesis": "Will dynamically updating the meta-buffer improve LLM scalability and stability?", "method": "Evaluate the performance of BoT with dynamic vs. static meta-buffer updates over multiple rounds of reasoning tasks.", "expected_outcome": "Dynamic updates will lead to better scalability and stability, improving task performance over time."}, {"hypothesis": "Can BoT achieve comparable reasoning efficiency to single-query methods while maintaining accuracy?", "method": "Measure and compare the reasoning time and accuracy of BoT against single-query methods like CoT on selected tasks.", "expected_outcome": "BoT will achieve similar reasoning efficiency to single-query methods, with higher accuracy."}, {"hypothesis": "Does the use of thought-templates improve the robustness of LLMs across different tasks?", "method": "Test the success rate of BoT across a diverse set of tasks and compare it to baselines without thought-templates.", "expected_outcome": "BoT will show higher robustness and success rates due to the use of thought-templates."}, {"hypothesis": "Will BoT enable smaller LLMs to perform comparably to larger models in reasoning tasks?", "method": "Compare the performance of BoT-equipped smaller models like Llama3-8B with larger models like Llama3-70B on reasoning tasks.", "expected_outcome": "BoT will allow smaller models to achieve performance levels similar to or better than larger models."}], "follow_up_work_ideas": ["Explore integrating external resources with BoT to build open-domain systems similar to agent models.", "Optimize the distillation of thought-templates to enhance their quality for more complex tasks.", "Incorporate BoT into LLM training to elicit more fine-grained and accurate reasoning processes.", "Apply BoT to new domains, such as multimodal reasoning or real-time decision-making tasks.", "Investigate the potential for BoT to improve LLM performance on tasks requiring long-term reasoning and planning."]}}
{"id": "96897", "url": "https://nips.cc/virtual/2024/poster/96897", "title": "BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models", "authors": [], "abstract": "Abstract:This work presents BAdam, an optimization method that leverages the block coordinate descent (BCD) framework with Adam's update rule. BAdam offers a memory efficient approach to the full parameter finetuning of large language models. We conduct   a theoretical convergence analysis for BAdam in the deterministic case. Experimentally, we apply BAdam to finetune the Llama 3-8B and Llama 3-70B models using a single RTX3090-24GB GPU and 4 A100-80GB GPUs, respectively. The results confirm BAdam's efficiency in terms of memory usage, running time, and optimization capability. Furthermore, the downstream performance evaluation based on MT-bench and math benchmarks shows that BAdam outperforms existing memory efficient baselines such as LoRA. It also demonstrates that BAdam can achieve comparable or even superior performance compared to Adam. Finally, the ablation study using SGD's update rule illustrates the suitability of BCD for finetuning LLMs. Our code can be easily integrated into any PyTorch-based codebase and is available at https://github.com/Ledzy/BAdam.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Ledzy/BAdam"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "RTX3090", "amount": 1}, "memory": "23.5 GB for Llama 3-8B, 21.8 GB for Llama 2-7B"}, "time_requirements": "3", "code_quality": "well-documented code", "difficulty": 3, "stars": 244}, "research_ideas": {"problem_statements": ["Large language models require significant GPU memory for full parameter finetuning, which poses challenges in resource-limited settings.", "Existing parameter efficient finetuning methods may limit the downstream performance by restricting updates to a lower-dimensional subspace.", "There is a need for a memory-efficient full parameter optimization method that does not impose low-rank constraints on parameter updates."], "main_takeaways": ["BAdam is a memory-efficient full parameter optimization method leveraging block coordinate descent with Adam's update rule.", "BAdam reduces memory usage by partitioning model parameters into blocks and updating them sequentially, requiring memory comparable to existing parameter-efficient methods.", "BAdam outperforms existing memory-efficient baselines like LoRA in terms of memory usage, running time, and downstream performance.", "The convergence of BAdam is theoretically analyzed, showing it can achieve a convergent scheme.", "BAdam shows comparable or superior performance to Adam, especially in instruction-following tasks."], "testable_hypotheses": [{"hypothesis": "Does BAdam offer better memory efficiency compared to Adam for finetuning large language models?", "method": "Measure and compare the GPU memory usage of BAdam and Adam when finetuning models of varying sizes.", "expected_outcome": "BAdam will show reduced memory usage compared to Adam while maintaining comparable performance."}, {"hypothesis": "Will BAdam achieve comparable downstream performance to Adam on instruction-following tasks?", "method": "Evaluate the performance of models finetuned with BAdam and Adam on instruction-following benchmarks such as MT-bench.", "expected_outcome": "BAdam will achieve comparable or better performance than Adam on instruction-following tasks."}, {"hypothesis": "Does BAdam maintain optimization capability with lower memory consumption compared to LoRA?", "method": "Conduct experiments finetuning large models using BAdam and LoRA, comparing both memory usage and downstream performance.", "expected_outcome": "BAdam will outperform LoRA in terms of downstream performance while using comparable memory."}, {"hypothesis": "Is the block coordinate descent framework suitable for finetuning large language models?", "method": "Compare the performance of BAdam using block coordinate descent with full parameter updates using Adam across various tasks.", "expected_outcome": "The BCD framework in BAdam will maintain or improve performance compared to full parameter updates."}, {"hypothesis": "Will different block partition strategies affect the performance of BAdam?", "method": "Experiment with different block partition strategies (e.g., random, ascending, descending) and measure their impact on model performance.", "expected_outcome": "Different partition strategies will not significantly affect BAdam's performance."}], "follow_up_work_ideas": ["Explore the application of BAdam for preference optimization tasks to further demonstrate its capabilities.", "Investigate the performance of BAdam in the context of continue pretraining (CPT) for large language models.", "Extend the convergence analysis of BAdam to the stochastic gradient scenario.", "Test BAdam's performance and memory efficiency in different domains beyond language models, such as vision models.", "Explore the integration of additional optimization techniques with BAdam's block coordinate descent framework to enhance performance."]}}
{"id": "94480", "url": "https://nips.cc/virtual/2024/poster/94480", "title": "InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory", "authors": [], "abstract": "Abstract:Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents). However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues. Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities. In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning. To this end, we introduce a training-free memory-based method, InfLLM. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences with a limited context window and well capture long-distance dependencies. Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences. Even when the sequence length is scaled to 1,024K, InfLLM still effectively captures long-distance dependencies. Our code can be found at https://github.com/thunlp/InfLLM.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/thunlp/InfLLM"}, "reproduce_difficulty": {"environment_setup": "Moderate difficulty, requires Python packages and configuration files.", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "Minimum 16 GB GPU memory", "other": "Network access for downloading datasets"}, "time_requirements": "Approximately 2 hours for setup and initial tests.", "code_quality": "Well-documented, provides clear instructions and examples.", "difficulty": 3, "stars": 335}, "research_ideas": {"problem_statements": ["Existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to out-of-domain and distraction issues.", "Continual pre-training on longer sequences introduces expensive computational overhead and uncontrollable change in model capabilities.", "How can we improve the length generalizability of LLMs without further training, making them applicable to long sequences directly?"], "main_takeaways": ["InfLLM is a training-free memory-based method that allows LLMs to process extremely long sequences by storing distant contexts in memory units.", "InfLLM uses a sliding window attention mechanism with an efficient context memory for attention computation, capturing long-distance dependencies.", "InfLLM achieves comparable performance to baselines that continually train LLMs on long sequences, with less computational cost.", "The block-level context memory mechanism of InfLLM enables effective and efficient memory lookup by selecting semantically significant tokens as unit representation."], "testable_hypotheses": [{"hypothesis": "InfLLM will outperform LLMs with sliding window attention alone on long sequence tasks.", "method": "Conduct experiments comparing InfLLM with LLMs using only sliding window attention on tasks requiring long sequence processing.", "expected_outcome": "InfLLM will show significant performance improvement due to its context memory mechanism."}, {"hypothesis": "Increasing the number of representative tokens in InfLLM will improve its performance.", "method": "Experiment with varying numbers of representative tokens in the context memory and evaluate performance on long sequence benchmarks.", "expected_outcome": "Performance will improve with more representative tokens up to a point, after which it might plateau or decrease."}, {"hypothesis": "InfLLM can generalize to sequence lengths far beyond the training lengths of base LLMs without additional training.", "method": "Test InfLLM on sequences with lengths significantly longer than the base model's training data and measure performance.", "expected_outcome": "InfLLM will maintain or improve performance on these longer sequences."}, {"hypothesis": "The block-level context memory in InfLLM will reduce computational costs compared to token-level memory units.", "method": "Compare the computational costs between InfLLM with block-level memory units and a version with token-level units.", "expected_outcome": "InfLLM with block-level memory will show reduced computational costs while maintaining performance."}, {"hypothesis": "InfLLM's offloading mechanism will effectively manage GPU memory usage for extremely long sequences.", "method": "Evaluate memory usage and processing efficiency of InfLLM with and without the offloading mechanism on very long sequences.", "expected_outcome": "InfLLM with the offloading mechanism will show lower GPU memory usage and comparable or better computational efficiency."}], "follow_up_work_ideas": ["Explore training an additional encoder to generate more expressive unit representations for the context memory.", "Investigate integrating InfLLM with key-value cache compression methods to further reduce computational and memory costs.", "Apply InfLLM's approach to domains beyond language models, such as image or video processing, to handle long sequences or streams of data.", "Develop methods to dynamically segment context for memory units to optimize retrieval and representation.", "Explore combining InfLLM with fine-tuning on task-specific long sequence data to enhance performance further without significant computation overhead."]}}
{"id": "93638", "url": "https://nips.cc/virtual/2024/poster/93638", "title": "Self-playing Adversarial Language Game Enhances LLM Reasoning", "authors": [], "abstract": "Abstract:We explore the potential of self-play training for large language models (LLMs) in a two-player adversarial language game called Adversarial Taboo. In this game, an attacker and a defender communicate around a target word only visible to the attacker. The attacker aims to induce the defender to speak the target word unconsciously, while the defender tries to infer the target word from the attacker's utterances. To win the game, both players must have sufficient knowledge about the target word and high-level reasoning ability to infer and express in this information-reserved conversation. Hence, we are curious about whether LLMs' reasoning ability can be further enhanced by Self-Playing this Adversarial language Game (SPAG). With this goal, we select several open-source LLMs and let each act as the attacker and play with a copy of itself as the defender on an extensive range of target words. Through reinforcement learning on the game outcomes, we observe that the LLMs' performances uniformly improve on a broad range of reasoning benchmarks. Furthermore, iteratively adopting this self-play process can continuously promote LLMs' reasoning abilities. The code is available at https://github.com/Linear95/SPAG.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Linear95/SPAG"}, "reproduce_difficulty": {"environment_setup": "Medium (Requires pip for installation and specific hardware setup)", "resource_requirements": {"GPUs": {"type": "A100", "amount": 32}, "memory": "40G per GPU", "CUDA_version": "11.0"}, "time_requirements": 24, "code_quality": "Well-documented code", "difficulty": 3, "stars": 120}, "research_ideas": {"problem_statements": ["Can the reasoning ability of large language models (LLMs) be enhanced through self-play in an adversarial language game?", "How can LLMs improve their reasoning abilities without relying on extensive high-quality human-annotated data?", "Is reinforcement learning through adversarial language games an effective method for improving LLMs' reasoning capacities?"], "main_takeaways": ["The self-play training method in the Adversarial Taboo game enhances the reasoning capabilities of LLMs.", "Iterative self-play can continuously improve LLMs' reasoning abilities.", "The proposed SPAG (Self-Play of Adversarial Game) framework shows significant performance improvement across various reasoning benchmarks.", "The adversarial language game setup provides a generalizable and efficient method for reasoning enhancement without human supervision."], "testable_hypotheses": [{"hypothesis": "Does self-play in the Adversarial Taboo game improve the reasoning ability of LLMs more than non-adversarial games?", "method": "Compare the performance of LLMs trained through self-play in adversarial and non-adversarial games on reasoning benchmarks.", "expected_outcome": "LLMs trained with adversarial games will outperform those trained with non-adversarial games."}, {"hypothesis": "Will increasing the number of self-play epochs result in continuous improvement in LLM reasoning abilities?", "method": "Conduct multiple epochs of self-play and evaluate LLM performance on reasoning benchmarks after each epoch.", "expected_outcome": "LLM performance will improve with each additional epoch of self-play."}, {"hypothesis": "Can imitation learning of GPT-4 behaviors in the Adversarial Taboo game enhance the reasoning capacity of smaller LLMs?", "method": "Perform imitation learning using GPT-4 game episodes and evaluate the reasoning capabilities of smaller LLMs.", "expected_outcome": "Smaller LLMs will show improved reasoning performance after imitation learning."}, {"hypothesis": "Does the use of a large target vocabulary in the Adversarial Taboo game contribute to better generalization of reasoning abilities in LLMs?", "method": "Use varying sizes of target vocabularies during self-play training and evaluate the generalization of reasoning abilities.", "expected_outcome": "Larger vocabulary sizes will result in better generalization of reasoning abilities."}, {"hypothesis": "Will the SPAG-trained LLMs perform better in practical applications compared to those trained with traditional supervised fine-tuning?", "method": "Compare LLMs trained via SPAG with those trained through traditional supervised fine-tuning on real-world reasoning tasks.", "expected_outcome": "SPAG-trained LLMs will outperform those trained with traditional supervised fine-tuning."}], "follow_up_work_ideas": ["Explore the use of other adversarial language games to enhance different capabilities of LLMs, such as creativity or empathy.", "Investigate the application of the SPAG framework to larger LLMs and evaluate the scalability of the approach.", "Develop more complex adversarial games that require multi-agent collaboration and competition to further boost reasoning skills.", "Explore the combination of SPAG with other self-improvement techniques, such as model distillation or meta-learning, to enhance LLM capabilities.", "Conduct studies on the safety and ethical implications of adversarial self-play training, ensuring that LLMs do not develop harmful behaviors."]}}
{"id": "94328", "url": "https://nips.cc/virtual/2024/poster/94328", "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs", "authors": [], "abstract": "Abstract:We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism, and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4 bits, without any channels identified for retention in higher precision. Our 4-bit quantized LLAMA2-70B model has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the zero-shot performance. We also show that QuaRot can provide lossless 6 and 8 bit LLAMA-2 models without any calibration data using round-to-nearest quantization. Code is available at github.com/spcl/QuaRot.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/spcl/QuaRot"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": {"type": "NVIDIA A100", "amount": 1}, "memory": "16 GB", "API_calls": "None"}, "time_requirements": 2, "code_quality": "well-documented", "difficulty": 2, "stars": 351}, "research_ideas": {"problem_statements": ["The paper addresses the computational, memory, and energy requirements of using large language models (LLMs) for inference, particularly during the prefill phase.", "Quantizing activations is challenging due to the presence of outlier elements, which have significantly larger values compared to other elements.", "Current quantization methods require keeping certain outlier features in higher precision, which complicates the quantization process.", "The paper aims to enable end-to-end 4-bit quantization for LLMs, including all weights, activations, and KV caches, without calibration data."], "main_takeaways": ["QuaRot introduces a quantization scheme based on rotations that can quantize LLMs end-to-end in 4 bits by removing outliers from the hidden state.", "The method applies randomized Hadamard transformations to the weight matrices, activations, and KV cache to eliminate outlier features without changing the model output.", "QuaRot achieves significant memory savings and speedups during the inference of LLMs, preserving up to 99% of zero-shot performance.", "The approach allows for lossless 6 and 8-bit quantization of LLM models using simple round-to-nearest quantization."], "testable_hypotheses": [{"hypothesis": "Randomized Hadamard transformations can eliminate outlier features in LLMs without changing model outputs.", "method": "Apply randomized Hadamard transformations to different LLM architectures and measure the presence of outlier features in activations.", "expected_outcome": "Outlier features are eliminated, making activations easier to quantize."}, {"hypothesis": "QuaRot can achieve 4-bit quantization of LLMs with minimal accuracy loss compared to FP16 models.", "method": "Quantize LLMs using QuaRot and measure the accuracy loss on standard benchmarks.", "expected_outcome": "QuaRot will retain 99% of the accuracy of zero-shot tasks with minimal accuracy loss."}, {"hypothesis": "QuaRot provides significant speedup and memory savings during LLM inference.", "method": "Benchmark the inference speed and memory usage of LLMs quantized using QuaRot compared to FP16 models.", "expected_outcome": "QuaRot will achieve up to 3.33x prefill speedups and 3.89x memory savings."}, {"hypothesis": "Group-wise quantization can further improve the accuracy of 4-bit quantized LLMs.", "method": "Apply group-wise quantization to LLMs quantized with QuaRot and assess the accuracy on standard benchmarks.", "expected_outcome": "Group-wise quantization will result in improved accuracy compared to non-group-wise methods."}, {"hypothesis": "Using Hadamard transformations in QuaRot is more effective than using random orthogonal matrices.", "method": "Compare the performance of QuaRot using Hadamard transformations versus random orthogonal matrices.", "expected_outcome": "Hadamard transformations will result in better preservation of model accuracy."}], "follow_up_work_ideas": ["Investigate the applicability of QuaRot to other neural network architectures beyond LLMs, such as convolutional neural networks.", "Explore the use of QuaRot in combination with other model compression techniques, like pruning or knowledge distillation.", "Develop optimized hardware implementations that leverage the 4-bit quantization enabled by QuaRot.", "Examine the impact of QuaRot on other tasks beyond zero-shot evaluation, such as fine-tuning or transfer learning scenarios.", "Investigate the potential for further reducing bit-widths below 4 bits by integrating QuaRot with advanced quantization techniques."]}}
{"id": "93408", "url": "https://nips.cc/virtual/2024/poster/93408", "title": "ADOPT: Modified Adam Can Converge with Any $\\beta_2$ with the Optimal Rate", "authors": [], "abstract": "Abstract:Adam is one of the most popular optimization algorithms in deep learning. However, it is known that Adam does not converge in theory unless choosing a hyperparameter, i.e., $\\beta_2$, in a problem-dependent manner. There have been many attempts to fix the non-convergence (e.g., AMSGrad), but they require an impractical assumption that the gradient noise is uniformly bounded. In this paper, we propose a new adaptive gradient method named ADOPT, which achieves the optimal convergence rate of $\\mathcal{O} ( 1 / \\sqrt{T} )$ with any choice of $\\beta_2$ without depending on the bounded noise assumption. ADOPT addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate and changing the order of the momentum update and the normalization by the second moment estimate. We also conduct intensive numerical experiments, and verify that our ADOPT achieves superior results compared to Adam and its variants across a wide range of tasks, including image classification, generative modeling, natural language processing, and deep reinforcement learning. The implementation is available at https://github.com/iShohei220/adopt.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/iShohei220/adopt"}, "reproduce_difficulty": {"environment_setup": "low", "resource_requirements": {"GPUs": "1 x GPU (NVIDIA recommended)", "memory": "8 GB RAM minimum", "API_call": "None specified"}, "time_requirements": "1-2 hours for initial training and experimentation", "code_quality": "well-documented code", "difficulty": 2, "stars": 417}, "research_ideas": {"problem_statements": ["Adam optimization algorithm does not converge in theory unless choosing the hyperparameter \u03b22 in a problem-dependent manner.", "Existing solutions like AMSGrad require an impractical assumption that the gradient noise is uniformly bounded.", "There is a need for adaptive gradient methods with problem-independent convergence guarantees."], "main_takeaways": ["ADOPT, a new adaptive gradient method, achieves the optimal convergence rate of O(1/\u221aT) with any choice of \u03b22 without the bounded noise assumption.", "ADOPT modifies Adam by removing the current gradient from the second moment estimate and changing the order of the momentum update and normalization.", "ADOPT outperforms existing adaptive gradient methods across a wide range of tasks, including image classification, generative modeling, NLP, and deep reinforcement learning."], "testable_hypotheses": [{"hypothesis": "Does ADOPT achieve superior convergence rates compared to Adam and AMSGrad across different tasks?", "method": "Conduct experiments on a variety of tasks such as image classification, NLP, and reinforcement learning, comparing the convergence rates of ADOPT, Adam, and AMSGrad.", "expected_outcome": "ADOPT will achieve superior convergence rates across all tasks."}, {"hypothesis": "Will ADOPT perform better in scenarios with large gradient noise compared to Adam and AMSGrad?", "method": "Introduce controlled gradient noise into optimization tasks and measure the performance of ADOPT, Adam, and AMSGrad.", "expected_outcome": "ADOPT will outperform Adam and AMSGrad in high-noise scenarios."}, {"hypothesis": "Is ADOPT's performance robust to different choices of \u03b22?", "method": "Test ADOPT with varying \u03b22 values across different tasks and measure performance consistency.", "expected_outcome": "ADOPT will maintain consistent performance regardless of \u03b22 choice."}, {"hypothesis": "Does reordering the momentum update and normalization improve convergence in ADOPT?", "method": "Create a variant of ADOPT that follows the original Adam ordering and compare convergence with standard ADOPT.", "expected_outcome": "The reordered ADOPT should show improved convergence."}, {"hypothesis": "Does ADOPT eliminate the need for bounded noise assumptions in convergence guarantees?", "method": "Theoretically and empirically analyze the convergence of ADOPT without relying on bounded noise assumptions.", "expected_outcome": "ADOPT should demonstrate convergence without these assumptions."}], "follow_up_work_ideas": ["Investigate the potential of applying ADOPT to large-scale foundation models to improve their training stability.", "Explore the relaxation of the assumption that the second moment of stochastic gradient is uniformly bounded.", "Analyze the impact of ADOPT on model generalization and robustness across different architectures and datasets.", "Study the theoretical foundations of ADOPT to further optimize its hyperparameters for various machine learning tasks.", "Apply ADOPT to emerging domains such as quantum machine learning to evaluate its adaptability and effectiveness."]}}
{"id": "95262", "url": "https://nips.cc/virtual/2024/poster/95262", "title": "MoE Jetpack: From Dense Checkpoints to Adaptive Mixture of Experts for Vision Tasks", "authors": [], "abstract": "Abstract:The sparsely activated mixture of experts (MoE) model presents an effective alternative to densely activated (dense) models, combining improved accuracy with computational efficiency. However, training MoE models from scratch requires extensive data and computational resources, a challenge that limits their widespread adoption. To address this, we introduce MoE Jetpack, a framework designed to fine-tune the abundant and easily accessible dense checkpoints into MoE models. MoE Jetpack incorporates two key techniques: (1)checkpoint recycling, which initializes MoE models with dense checkpoints to accelerate convergence and enhance accuracy, minimizing the need for extensive pre-training; (2) thehyperspherical adaptive MoE (SpheroMoE) layer, which optimizes the MoE architecture to enhance fine-tuning performance and efficiency.Experimental results indicate that MoE Jetpack doubles the convergence speed and enhances accuracy by 2.8% on ImageNet-1K. On smaller datasets, it achieves up to 8-fold faster convergence and over 30% accuracy gains, highlighting its efficiency.The code is available at https://github.com/Adlith/MoE-Jetpack.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Adlith/MoE-Jetpack"}, "reproduce_difficulty": {"environment_setup": "Moderate difficulty, requires installation of PyTorch, MMCV, and additional dependencies via pip.", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 4}, "memory": "Not specified, but likely requires significant RAM for training.", "api_calls": "Not specified"}, "time_requirements": 2, "code_quality": "Well-documented code with clear instructions.", "difficulty": 3, "stars": 105}, "research_ideas": {"problem_statements": ["Training MoE models from scratch requires extensive data and computational resources, limiting their widespread adoption.", "Unlike dense models, most MoE models must be trained from scratch with randomly initialized weights, demanding substantial computational power and large datasets.", "Current MoE architectures are not designed to incorporate pre-trained dense checkpoints, often resulting in optimization inefficiencies and over-specialization during fine-tuning."], "main_takeaways": ["MoE Jetpack is a framework designed to fine-tune dense checkpoints into MoE models, improving convergence speed and accuracy.", "The framework introduces checkpoint recycling, which initializes MoE models with dense checkpoints to accelerate convergence and enhance accuracy.", "The hyperspherical adaptive MoE (SpheroMoE) layer optimizes MoE architecture for enhanced fine-tuning performance.", "MoE Jetpack achieves significant improvements in convergence speed and accuracy on various datasets compared to models trained from scratch."], "testable_hypotheses": [{"hypothesis": "Checkpoint recycling improves the convergence speed of MoE models.", "method": "Compare the convergence speed of MoE models initialized with checkpoint recycling against those trained from scratch on the same datasets.", "expected_outcome": "MoE models with checkpoint recycling will show faster convergence."}, {"hypothesis": "The SpheroMoE layer enhances the fine-tuning performance of MoE models.", "method": "Evaluate the performance of MoE models with and without the SpheroMoE layer on the same datasets.", "expected_outcome": "Models with the SpheroMoE layer will show improved accuracy."}, {"hypothesis": "Using dense checkpoints as initialization for MoE models improves their accuracy on downstream tasks.", "method": "Compare the accuracy of MoE models initialized with dense checkpoints versus random initialization on downstream classification tasks.", "expected_outcome": "MoE models initialized with dense checkpoints will have higher accuracy."}, {"hypothesis": "The dual-path structure in SpheroMoE layers optimizes resource allocation and improves model efficiency.", "method": "Test MoE models with and without the dual-path structure for efficiency and accuracy across multiple datasets.", "expected_outcome": "Dual-path structure models will achieve higher efficiency and potentially better accuracy."}, {"hypothesis": "Different strategies for checkpoint recycling result in varying levels of performance improvement in MoE models.", "method": "Implement and compare various checkpoint recycling strategies (random, uniform, importance-based) in MoE models.", "expected_outcome": "Importance-based sampling will yield the best performance improvements."}], "follow_up_work_ideas": ["Investigate the extension of MoE Jetpack to other domains such as natural language processing and reinforcement learning.", "Explore methods to further reduce the reliance on high-quality dense checkpoints, enhancing MoE versatility.", "Develop alternative initialization strategies that combine dense checkpoints with other forms of prior knowledge.", "Study the impact of different expert configurations on model robustness and generalization across various tasks.", "Evaluate the scalability of MoE Jetpack in larger, more complex models and datasets."]}}
{"id": "95559", "url": "https://nips.cc/virtual/2024/poster/95559", "title": "Demystify Mamba in Vision: A Linear Attention Perspective", "authors": [], "abstract": "Abstract:Mamba is an effective state space model with linear computation complexity. It has recently shown impressive efficiency in dealing with high-resolution inputs across various vision tasks. In this paper, we reveal that the powerful Mamba model shares surprising similarities with linear attention Transformer, which typically underperform conventional Transformer in practice. By exploring the similarities and disparities between the effective Mamba and subpar linear attention Transformer, we provide comprehensive analyses to demystify the key factors behind Mamba\u2019s success. Specifically, we reformulate the selective state space model and linear attention within a unified formulation, rephrasing Mamba as a variant of linear attention Transformer with six major distinctions: input gate, forget gate, shortcut, no attention normalization, single-head, and modified block design. For each design, we meticulously analyze its pros and cons, and empirically evaluate its impact on model performance in vision tasks. Interestingly, the results highlight the forget gate and block design as the core contributors to Mamba\u2019s success, while the other four designs are less crucial. Based on these findings, we propose a Mamba-Inspired Linear Attention (MILA) model by incorporating the merits of these two key designs into linear attention. The resulting model outperforms various vision Mamba models in both image classification and high-resolution dense prediction tasks, while enjoying parallelizable computation and fast inference speed. Code is available at https://github.com/LeapLabTHU/MLLA.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/LeapLabTHU/MLLA"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires specific versions of Python packages and PyTorch setup with distributed training)", "resource_requirements": "8 GPUs for distributed training, at least 16 GB of RAM, ImageNet dataset", "time_requirements": "3 hours", "code_quality": "Well-documented code with clear instructions for setup and usage", "difficulty": 3, "stars": 276}, "research_ideas": {"problem_statements": ["What factors contribute to Mamba's success and its significant superiority to linear attention Transformer?", "How can the successful designs of Mamba be integrated into linear attention models to improve their performance?", "What are the key distinctions between Mamba and linear attention Transformers, and which of these distinctions contribute most to Mamba\u2019s effectiveness?"], "main_takeaways": ["Mamba shares surprising similarities with linear attention Transformer but performs significantly better due to six major distinctions.", "Forget gate and block design are identified as the core contributors to Mamba's success.", "A Mamba-Inspired Linear Attention (MILA) model is proposed, which integrates key Mamba designs into linear attention, outperforming vision Mamba models in various tasks.", "MILA maintains parallelizable computation and fast inference speed, offering a practical advantage over Mamba."], "testable_hypotheses": [{"hypothesis": "The forget gate is a major contributor to Mamba's performance advantage over linear attention.", "method": "Implement a version of linear attention with a forget gate and compare performance against a baseline linear attention model.", "expected_outcome": "The model with the forget gate will outperform the baseline linear attention model."}, {"hypothesis": "Replacing the forget gate with suitable positional encoding alternatives can maintain performance while improving computation speed.", "method": "Test different positional encodings (e.g., APE, LePE, CPE, RoPE) as substitutes for the forget gate in MILA models and measure both performance and speed.", "expected_outcome": "Positional encodings will match or exceed the performance of the forget gate and result in faster computation."}, {"hypothesis": "The modified block design in Mamba contributes more to its performance than the other architectural changes.", "method": "Isolate each major design change (input gate, forget gate, shortcut, no normalization) in Mamba and evaluate their individual impact on performance using MILA.", "expected_outcome": "The block design has a more significant impact on performance compared to the other changes."}, {"hypothesis": "The parallelizable computation of MILA leads to faster inference speeds compared to Mamba, especially on non-causal data like images.", "method": "Measure and compare inference times of MILA and Mamba models on image data processing tasks.", "expected_outcome": "MILA will demonstrate faster inference speeds than Mamba."}, {"hypothesis": "Incorporating the block design of Mamba into linear attention models will lead to improved performance across various vision tasks.", "method": "Deploy MILA models with Mamba's block design on diverse vision tasks and compare against standard linear attention models.", "expected_outcome": "MILA models will achieve superior results on vision tasks compared to standard linear attention models."}], "follow_up_work_ideas": ["Explore additional design modifications inspired by Mamba for further enhancement of linear attention models.", "Investigate the application of MILA in other domains such as natural language processing to evaluate its generalization capabilities.", "Develop more efficient methods for implementing the forget gate or its alternatives in vision applications.", "Examine the integration of other efficient attention mechanisms with MILA to enhance its performance.", "Conduct deeper analyses on the scalability of MILA architectures to even larger model sizes and datasets."]}}
{"id": "94391", "url": "https://nips.cc/virtual/2024/poster/94391", "title": "CycleNet: Enhancing Time Series Forecasting through Modeling Periodic Patterns", "authors": [], "abstract": "Abstract:The stable periodic patterns present in time series data serve as the foundation for conducting long-horizon forecasts. In this paper, we pioneer the exploration of explicitly modeling this periodicity to enhance the performance of models in long-term time series forecasting (LTSF) tasks. Specifically, we introduce the Residual Cycle Forecasting (RCF) technique, which utilizes learnable recurrent cycles to model the inherent periodic patterns within sequences, and then performs predictions on the residual components of the modeled cycles. Combining RCF with a Linear layer or a shallow MLP forms the simple yet powerful method proposed in this paper, called CycleNet. CycleNet achieves state-of-the-art prediction accuracy in multiple domains including electricity, weather, and energy, while offering significant efficiency advantages by reducing over 90% of the required parameter quantity. Furthermore, as a novel plug-and-play technique, the RCF can also significantly improve the prediction accuracy of existing models, including PatchTST and iTransformer. The source code is available at: https://github.com/ACAT-SCUT/CycleNet.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/ACAT-SCUT/CycleNet"}, "reproduce_difficulty": {"environment_setup": "2", "resource_requirements": {"GPUs": "1 NVIDIA GPU (e.g., Tesla V100 or equivalent)", "memory": "Minimum of 16 GB RAM"}, "time_requirements": "2", "code_quality": "Well-documented code", "difficulty": 3, "stars": 133}, "research_ideas": {"problem_statements": ["How can explicit modeling of periodic patterns in time series data enhance long-term time series forecasting (LTSF) tasks?", "Can a simple method combining Residual Cycle Forecasting (RCF) with a Linear layer or shallow MLP outperform existing models in terms of prediction accuracy and efficiency?", "What are the benefits of using a plug-and-play technique like RCF in improving the prediction accuracy of existing models?"], "main_takeaways": ["The paper introduces CycleNet, which models periodic patterns explicitly using the Residual Cycle Forecasting (RCF) technique, achieving state-of-the-art prediction accuracy in multiple domains.", "CycleNet, combining RCF with a Linear layer or shallow MLP, significantly reduces the required parameter quantity by over 90% compared to other models.", "RCF, as a plug-and-play technique, can enhance the prediction accuracy of existing models like PatchTST and iTransformer.", "CycleNet achieves consistent top-tier performance across various datasets with strong periodic patterns, but it is slightly less effective in traffic scenarios due to the need for better multivariate relationship modeling."], "testable_hypotheses": [{"hypothesis": "Explicitly modeling periodic patterns improves the accuracy of long-term time series forecasting models.", "method": "Compare the performance of models with and without the RCF technique on various datasets with known periodic patterns.", "expected_outcome": "Models using RCF will show improved accuracy over those that do not model periodic patterns explicitly."}, {"hypothesis": "The RCF technique requires fewer parameters than traditional models while maintaining high prediction accuracy.", "method": "Measure and compare the parameter count and prediction accuracy of CycleNet against other state-of-the-art models.", "expected_outcome": "CycleNet will have significantly fewer parameters yet achieve comparable or superior accuracy."}, {"hypothesis": "RCF can enhance the prediction accuracy of existing models as a plug-and-play technique.", "method": "Integrate RCF into existing models like PatchTST and iTransformer and evaluate the change in prediction accuracy.", "expected_outcome": "Models with RCF will outperform their original versions in terms of prediction accuracy."}, {"hypothesis": "The performance of CycleNet is sensitive to the correct setting of the cycle length parameter W.", "method": "Test CycleNet with various incorrect and correct settings of W on datasets with known cycle lengths.", "expected_outcome": "CycleNet will achieve optimal performance when W matches the actual cycle length of the dataset."}, {"hypothesis": "CycleNet's performance is less effective in datasets with significant spatiotemporal relationships and outliers.", "method": "Evaluate CycleNet on datasets with and without strong spatiotemporal relationships and analyze the impact of outliers.", "expected_outcome": "CycleNet will perform less effectively on datasets with significant spatiotemporal relationships and outliers compared to other models."}], "follow_up_work_ideas": ["Develop methods to enhance the robustness of RCF against datasets with significant outliers.", "Explore techniques to incorporate inter-channel relationship modeling within the RCF framework for improved multivariate forecasting.", "Investigate the application of CycleNet to other domains with periodic data patterns, such as finance or healthcare.", "Study the impact of different types of instance normalization strategies on the performance of CycleNet and other time series models.", "Explore adaptive settings of the cycle length parameter W to handle datasets with varying cycle lengths over time."]}}
{"id": "96893", "url": "https://nips.cc/virtual/2024/poster/96893", "title": "SegVol: Universal and Interactive Volumetric Medical Image Segmentation", "authors": [], "abstract": "Abstract:Precise image segmentation provides clinical study with instructive information. Despite the remarkable progress achieved in medical image segmentation, there is still an absence of a 3D foundation segmentation model that can segment a wide range of anatomical categories with easy user interaction. In this paper, we propose a 3D foundation segmentation model, named SegVol, supporting universal and interactive volumetric medical image segmentation. By scaling up training data to 90K unlabeled Computed Tomography (CT) volumes and 6K labeled CT volumes, this foundation model supports the segmentation of over 200 anatomical categories using semantic and spatial prompts. To facilitate efficient and precise inference on volumetric images, we design a zoom-out-zoom-in mechanism. Extensive experiments on 22 anatomical segmentation tasks verify that SegVol outperforms the competitors in 19 tasks, with improvements up to 37.24\\% compared to the runner-up methods. We demonstrate the effectiveness and importance of specific designs by ablation study. We expect this foundation model can promote the development of volumetric medical image analysis. The model and code are publicly available at https://github.com/BAAI-DCAI/SegVol.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/BAAI-DCAI/SegVol"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "Well-documented", "difficulty": 3, "stars": 295}, "research_ideas": {"problem_statements": ["Despite progress in medical image segmentation, there is a lack of a universal 3D model that can segment multiple anatomical categories with user-friendly interaction.", "Existing volumetric medical segmentation methods struggle with generalizing across different datasets due to varying label spaces.", "Current models struggle to segment complex structures like tumors due to insufficient data and lack of interactive capabilities.", "Traditional models are computationally expensive, often relying on inefficient methods like sliding windows for inference."], "main_takeaways": ["SegVol is a proposed 3D foundation model for universal and interactive medical image segmentation that supports over 200 anatomical categories.", "The model uses a zoom-out-zoom-in mechanism to enhance inference efficiency and precision.", "SegVol outperforms existing interactive segmentation methods in 19 out of 22 tasks, with improvements up to 37.24%.", "The model is trained on a large dataset of 90K unlabeled and 6K labeled CT volumes, using both spatial and semantic prompts.", "SegVol facilitates efficient segmentation with a combination of prompts, improving segmentation accuracy and semantic disambiguation."], "testable_hypotheses": [{"hypothesis": "Does scaling up the training data improve the segmentation performance of SegVol?", "method": "Train SegVol on varying amounts of data and evaluate performance on a fixed test set.", "expected_outcome": "Increased training data will lead to better segmentation performance."}, {"hypothesis": "Does the zoom-out-zoom-in mechanism improve inference efficiency while maintaining precision?", "method": "Compare the inference time and segmentation accuracy of SegVol with and without the zoom-out-zoom-in mechanism.", "expected_outcome": "The zoom-out-zoom-in mechanism will reduce inference time without sacrificing accuracy."}, {"hypothesis": "Can SegVol generalize to segment MRI data without fine-tuning?", "method": "Evaluate SegVol on MRI datasets without additional training.", "expected_outcome": "SegVol will demonstrate reasonable segmentation performance on MRI data due to its foundational training."}, {"hypothesis": "Does combining spatial and semantic prompts lead to better segmentation accuracy than using either alone?", "method": "Conduct segmentation tasks using only spatial prompts, only semantic prompts, and a combination, then compare the results.", "expected_outcome": "The combination of spatial and semantic prompts will result in higher segmentation accuracy."}, {"hypothesis": "Is SegVol more effective at segmenting complex anatomical structures compared to existing methods?", "method": "Compare SegVol with traditional methods on datasets featuring complex structures like tumors.", "expected_outcome": "SegVol will outperform traditional methods in segmenting complex structures."}], "follow_up_work_ideas": ["Explore the application of SegVol to other imaging modalities such as PET or ultrasound.", "Investigate the potential for fine-tuning SegVol on smaller, specific datasets for specialized applications.", "Develop methods to further reduce computational cost during training and inference.", "Expand SegVol's capabilities to include real-time segmentation for surgical assistance.", "Explore integration of additional data such as patient history or genetic information to enhance segmentation accuracy."]}}
{"id": "94155", "url": "https://nips.cc/virtual/2024/poster/94155", "title": "Voxel Mamba: Group-Free State Space Models for Point Cloud based 3D Object Detection", "authors": [], "abstract": "Abstract:Serialization-based methods, which serialize the 3D voxels and group them into multiple sequences before inputting to Transformers, have demonstrated their effectiveness in 3D object detection. However, serializing 3D voxels into 1D sequences will inevitably sacrifice the voxel spatial proximity. Such an issue is hard to be addressed by enlarging the group size with existing serialization-based methods due to the quadratic complexity of Transformers with feature sizes. Inspired by the recent advances of state space models (SSMs), we present a Voxel SSM, termed as Voxel Mamba, which employs a group-free strategy to serialize the whole space of voxels into a single sequence. The linear complexity of SSMs encourages our group-free design, alleviating the loss of spatial proximity of voxels. To further enhance the spatial proximity, we propose a Dual-scale SSM Block to establish a hierarchical structure, enabling a larger receptive field in the 1D serialization curve, as well as more complete local regions in 3D space. Moreover, we implicitly apply window partition under the group-free framework by positional encoding, which further enhances spatial proximity by encoding voxel positional information. Our experiments on Waymo Open Dataset and nuScenes dataset show that Voxel Mamba not only achieves higher accuracy than state-of-the-art methods, but also demonstrates significant advantages in computational efficiency. The source code is available at https://github.com/gwenzhang/Voxel-Mamba.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/gwenzhang/Voxel-Mamba"}, "reproduce_difficulty": {"environment_setup": "Moderate (Requires multi-GPU setup and dependencies from OpenPCDet)", "resource_requirements": {"GPUs": {"type": "NVIDIA A100", "amount": 8}, "memory": "Not specified, but high memory usage expected due to multi-GPU training", "API_calls": "Not specified"}, "time_requirements": "Estimated 2-4 hours for training depending on hardware setup", "code_quality": "Well-documented code", "difficulty": 3, "stars": 110}, "research_ideas": {"problem_statements": ["How can we address the loss of spatial proximity in 3D voxel serialization for point cloud-based 3D object detection?", "Can a group-free design using state space models (SSMs) improve computational efficiency and spatial proximity in voxel-based 3D detection?", "What are the limitations of existing serialization-based methods in maintaining voxel spatial proximity and how can these be mitigated?"], "main_takeaways": ["Voxel Mamba is introduced as a group-free state space model for 3D object detection that serializes the entire space of voxels into a single sequence.", "The group-free design of Voxel Mamba enhances spatial proximity and computational efficiency compared to traditional serialization methods.", "The proposed Dual-scale SSM Block and Implicit Window Partition further enhance spatial proximity and allow larger receptive fields.", "Voxel Mamba achieves superior performance and computational efficiency on the Waymo Open Dataset and nuScenes dataset compared to state-of-the-art methods."], "testable_hypotheses": [{"hypothesis": "Does the group-free design of Voxel Mamba improve computational efficiency compared to group-based methods?", "method": "Conduct time complexity analysis and compare execution time of Voxel Mamba against group-based methods on standard datasets.", "expected_outcome": "Voxel Mamba will demonstrate improved computational efficiency due to its linear complexity."}, {"hypothesis": "Will the Dual-scale SSM Block enhance the spatial proximity and effective receptive field in Voxel Mamba?", "method": "Visualize and compare the effective receptive fields of Voxel Mamba with and without the Dual-scale SSM Block.", "expected_outcome": "The Dual-scale SSM Block will show enhanced spatial proximity and larger effective receptive fields."}, {"hypothesis": "Does Voxel Mamba achieve higher accuracy in 3D object detection compared to existing serialization-based methods?", "method": "Evaluate Voxel Mamba on standard datasets such as Waymo Open and nuScenes and compare accuracy metrics with existing methods.", "expected_outcome": "Voxel Mamba will outperform existing serialization-based methods in terms of detection accuracy."}, {"hypothesis": "Can Implicit Window Partition (IWP) enhance voxel proximity without explicit window partitioning?", "method": "Compare the voxel proximity metrics of Voxel Mamba models with and without IWP on test datasets.", "expected_outcome": "Models with IWP will demonstrate better voxel proximity while maintaining computational efficiency."}, {"hypothesis": "Is the Hilbert curve-based serialization more effective than random or other space-filling curves in preserving voxel proximity?", "method": "Implement and compare different space-filling curves for serialization and evaluate their impact on model performance.", "expected_outcome": "Hilbert curve-based serialization will show improved performance due to better preservation of voxel proximity."}], "follow_up_work_ideas": ["Explore more efficient downsampling and upsampling operations in the Dual-scale SSM Block to further improve model efficiency.", "Investigate alternative space-filling curves or proximity-preserving methods to enhance voxel proximity in serialization further.", "Apply the Voxel Mamba approach to other 3D vision tasks beyond object detection, such as 3D scene reconstruction or segmentation.", "Study the impact of different state space model configurations on the performance and efficiency of Voxel Mamba.", "Explore the integration of Voxel Mamba with other modalities (e.g., RGB images) for multi-modal 3D object detection."]}}
{"id": "97431", "url": "https://nips.cc/virtual/2024/poster/97431", "title": "Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs", "authors": [], "abstract": "Abstract:Although Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks in a zero-shot manner, they are susceptible to jailbreak attacks and can be manipulated to produce harmful outputs. Recently, a growing body of research has categorized jailbreak attacks into token-level and prompt-level attacks.  However, previous work primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs. To address these issues, we introduced JailTrickBench to evaluate the impact of various attack settings on LLM performance and provide a baseline for jailbreak attacks, encouraging the adoption of a standardized evaluation framework. Specifically, we evaluate the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives. We further conduct seven representative jailbreak attacks on six defense methods across two widely used datasets, encompassing approximately 354 experiments with about 55,000 GPU hours on A800-80G. Our experimental results highlight the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is available at https://github.com/usail-hkust/JailTrickBench.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/usail-hkust/JailTrickBench"}, "reproduce_difficulty": {"environment_setup": "Requires installation via pip; no specific mention of conda or docker.", "resource_requirements": "Requires access to multiple GPUs (50 A800 GPUs recommended); approximately 55,000 GPU hours for experiments.", "time_requirements": "Estimated time for running main commands is significant, likely several hours to days depending on the setup.", "code_quality": "Well-documented code with detailed instructions for setup and running experiments.", "difficulty": 4, "stars": 122}, "research_ideas": {"problem_statements": ["Large Language Models (LLMs) are susceptible to jailbreak attacks that can manipulate them to produce harmful outputs.", "Previous research lacks exploration of defense-enhanced LLMs and diverse key factors affecting jailbreak attacks.", "There is a need for a standardized evaluation framework to benchmark jailbreak attacks on LLMs."], "main_takeaways": ["Model robustness against jailbreak attacks does not strictly correlate with its size.", "Fine-tuning can significantly affect the original LLM\u2019s safety alignment.", "Safe system prompts can significantly enhance LLM robustness.", "Improper attack settings can degrade attack performance significantly.", "Standardized benchmarking is needed to evaluate jailbreak attacks on defense-enhanced LLMs."], "testable_hypotheses": [{"hypothesis": "Does the size of an LLM correlate with its robustness against jailbreak attacks?", "method": "Conduct experiments comparing the attack success rate on various LLM sizes using identical jailbreak attack methods.", "expected_outcome": "Model robustness does not strictly correlate with its size; both smaller and larger models show varying degrees of robustness."}, {"hypothesis": "Does fine-tuning LLMs with adversarial examples reduce their safety alignment?", "method": "Evaluate the attack success rate on LLMs before and after fine-tuning with adversarial examples.", "expected_outcome": "Fine-tuning reduces the original LLM's safety alignment, increasing its vulnerability to jailbreak attacks."}, {"hypothesis": "Can the inclusion of safety system prompts improve LLM robustness against jailbreak attacks?", "method": "Compare attack success rates on LLMs with and without safety system prompts under identical attack conditions.", "expected_outcome": "Safety system prompts significantly enhance LLM robustness against jailbreak attacks."}, {"hypothesis": "Does the type of template used in LLMs affect their vulnerability to jailbreak attacks?", "method": "Test different LLM templates (e.g., default vs. zero-shot) and measure the attack success rate under identical conditions.", "expected_outcome": "The choice of template significantly impacts the model\u2019s vulnerability to jailbreak attacks."}, {"hypothesis": "Do longer adversarial suffixes increase the likelihood of generating jailbroken responses?", "method": "Vary the length of adversarial suffixes in token-level jailbreak attacks and measure the attack success rate.", "expected_outcome": "Longer adversarial suffixes increase the likelihood of generating jailbroken responses up to a certain point."}], "follow_up_work_ideas": ["Investigate cost-effective, innovative jailbreak attack methods that do not require access to closed-source LLMs.", "Explore the impact of combining defense methods on the utility and performance of LLMs.", "Develop advanced defense strategies that minimally impact the performance of LLMs while enhancing security.", "Expand benchmarking to include more diverse datasets and target models to improve the understanding of LLM vulnerabilities."]}}
{"id": "97605", "url": "https://nips.cc/virtual/2024/poster/97605", "title": "WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark", "authors": [], "abstract": "Abstract:Underwater Object Tracking (UOT) is essential for identifying and tracking submerged objects in underwater videos, but existing datasets are limited in scale, diversity of target categories and scenarios covered, impeding the development of advanced tracking algorithms. To bridge this gap, we take the first step and introduce WebUOT-1M, \\ie, the largest public UOT benchmark to date, sourced from complex and realistic underwater environments. It comprises 1.1 million frames across 1,500 video clips filtered from 408 target categories, largely surpassing previous UOT datasets, \\eg, UVOT400. Through meticulous manual annotation and verification, we provide high-quality bounding boxes for underwater targets. Additionally, WebUOT-1M includes language prompts for video sequences, expanding its application areas, \\eg, underwater vision-language tracking. Given that most existing trackers are designed for open-air conditions and perform poorly in underwater environments due to domain gaps, we propose a novel framework that uses omni-knowledge distillation to train a student Transformer model effectively. To the best of our knowledge, this framework is the first to effectively transfer open-air domain knowledge to the UOT model through knowledge distillation, as demonstrated by results on both existing UOT datasets and the newly proposed WebUOT-1M. We have thoroughly tested WebUOT-1M with 30 deep trackers, showcasing its potential as a benchmark for future UOT research. The complete dataset, along with codes and tracking results, are publicly accessible at \\href{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}{\\color{magenta}{here}}.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/983632847/Awesome-Multimodal-Object-Tracking"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires installation of dependencies, likely with pip/conda)", "resource_requirements": {"GPUs": "1 NVIDIA GPU with at least 8GB memory", "memory": "16GB RAM", "API_call": "Possible for dataset downloads"}, "time_requirements": "3 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 160}, "research_ideas": {"problem_statements": ["Existing underwater object tracking (UOT) datasets are limited in scale and diversity, impeding the development of advanced tracking algorithms.", "Most existing trackers are designed for open-air conditions and perform poorly in underwater environments due to domain gaps.", "There is a lack of million-scale benchmark datasets for underwater object tracking, which hinders comprehensive evaluation and development of UOT algorithms."], "main_takeaways": ["WebUOT-1M is introduced as the largest public UOT benchmark to date, containing 1.1 million frames across 1,500 video clips with 408 target categories.", "The dataset includes language prompts for video sequences, enabling applications in underwater vision-language tracking.", "A novel omni-knowledge distillation framework is proposed to effectively transfer open-air domain knowledge to UOT models, using a student Transformer model.", "Evaluation with 30 deep trackers demonstrates WebUOT-1M's potential as a benchmark for future UOT research."], "testable_hypotheses": [{"hypothesis": "Does integrating language prompts with visual data improve tracking performance in underwater environments?", "method": "Conduct experiments comparing tracking performance with and without language prompts using the WebUOT-1M dataset.", "expected_outcome": "Integrating language prompts will improve tracking performance by providing additional context to the model."}, {"hypothesis": "Will the omni-knowledge distillation approach outperform traditional training methods for UOT?", "method": "Compare the performance of models trained with omni-knowledge distillation against those trained with standard methods on WebUOT-1M.", "expected_outcome": "Omni-knowledge distillation will lead to better performance due to effective knowledge transfer from open-air to underwater scenarios."}, {"hypothesis": "Can the proposed OKTrack outperform existing UOT-specific methods on WebUOT-1M?", "method": "Evaluate OKTrack against existing UOT-specific methods using standard tracking metrics on WebUOT-1M.", "expected_outcome": "OKTrack will outperform existing methods due to its use of omni-knowledge distillation and Transformer architecture."}, {"hypothesis": "Do Transformer-based trackers have a significant advantage over CNN-based trackers in underwater scenarios?", "method": "Conduct a comparative analysis of Transformer-based and CNN-based trackers on WebUOT-1M.", "expected_outcome": "Transformer-based trackers will show superior performance due to their ability to model long-range dependencies."}, {"hypothesis": "Will increasing the number of Transformer layers in the student model improve tracking accuracy?", "method": "Experiment with student models having different numbers of Transformer layers to evaluate their impact on tracking accuracy.", "expected_outcome": "More Transformer layers will lead to improved accuracy but with diminishing returns and increased computational cost."}], "follow_up_work_ideas": ["Expand the WebUOT-1M dataset to include additional modalities such as depth and audio for richer multi-modal tracking capabilities.", "Investigate the development of more efficient network architectures for underwater vision-language tracking to reduce computational costs.", "Explore methods to further reduce the domain gap between open-air and underwater environments for improved generalization of tracking models.", "Develop comprehensive evaluation protocols that consider long-term tracking scenarios and low frame rate conditions in underwater environments.", "Study the applicability of the omni-knowledge distillation framework to other challenging tracking domains beyond underwater environments."]}}
{"id": "97791", "url": "https://nips.cc/virtual/2024/poster/97791", "title": "The Multimodal Universe: Enabling Large-Scale Machine Learning with 100 TB of Astronomical Scientific Data", "authors": [], "abstract": "Abstract:We present theMultimodal Universe, a large-scale multimodal dataset of scientific astronomical data, compiled specifically to facilitate machine learning research. Overall, our dataset contains hundreds of millions of astronomical observations, constituting 100TB of multi-channel and hyper-spectral images, spectra, multivariate time series, as well as a wide variety of associated scientific measurements and metadata. In addition, we include a range of benchmark tasks representative of standard practices for machine learning methods in astrophysics. This massive dataset will enable the development of large multi-modal models specifically targeted towards scientific applications. All codes used to compile the dataset, and a description of how to access the data is available at https://github.com/MultimodalUniverse/MultimodalUniverse", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/MultimodalUniverse/MultimodalUniverse"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "2 NVIDIA A100", "memory": "64 GB RAM", "storage": "100 TB disk space"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 379}, "research_ideas": {"problem_statements": ["Despite the data-rich nature of astrophysics, existing datasets are fragmented and not optimized for machine learning applications.", "The lack of large-scale, standardized, and comprehensive scientific datasets spanning multiple modalities is hindering the development of machine learning models for scientific data.", "Current astronomical imaging surveys are not well-suited for ML applications due to non-uniform data storage and a high level of expertise required for data access.", "There is a need for a more streamlined and standardized approach to data access and preparation in astrophysics.", "The lack of associated textual data in scientific datasets limits the development of integrated multimodal models in astronomy."], "main_takeaways": ["The Multimodal Universe dataset integrates over 100TB of astronomical data, including multi-channel and hyperspectral images, spectra, and multivariate time series.", "The dataset is designed to enable the development of large multimodal models specifically for scientific applications.", "The dataset includes benchmark tasks and baseline deep learning models to evaluate new models in astrophysics.", "The project emphasizes extensibility and accessibility, offering data through Hugging Face datasets and providing utilities for cross-matching datasets.", "The Multimodal Universe aims to catalyze innovation in the astrophysics and ML communities by providing a comprehensive and standardized dataset."], "testable_hypotheses": [{"hypothesis": "Does the inclusion of metadata improve the performance of ML models on the Multimodal Universe dataset?", "method": "Train models with and without metadata and compare their performance on benchmark tasks.", "expected_outcome": "Models using metadata will show improved performance due to the additional contextual information."}, {"hypothesis": "Does the use of multimodal data lead to better model performance compared to single-modality data?", "method": "Train models on single-modality and multimodal datasets and compare their performance on benchmark tasks.", "expected_outcome": "Multimodal data will result in better performance due to richer information content."}, {"hypothesis": "Can cross-modality pretraining (e.g., image-spectrum) improve model generalization on unseen tasks?", "method": "Pretrain models using cross-modality tasks and evaluate them on new unseen tasks.", "expected_outcome": "Cross-modality pretraining will improve generalization and performance on unseen tasks."}, {"hypothesis": "Will models pretrained on the Multimodal Universe dataset outperform models trained on smaller, less diverse datasets?", "method": "Pretrain models on the Multimodal Universe dataset and smaller datasets, then evaluate on benchmark tasks.", "expected_outcome": "Models pretrained on the Multimodal Universe will outperform due to the larger and more diverse data."}, {"hypothesis": "Does the use of standardized data formats reduce the barrier to entry for researchers working with astronomical data?", "method": "Survey researchers before and after using the Multimodal Universe dataset to assess perceived ease of use.", "expected_outcome": "Researchers will report reduced barriers to entry and increased ease of use."}], "follow_up_work_ideas": ["Explore the integration of textual data with the existing multimodal dataset to develop text-aware ML models.", "Investigate the use of the dataset for developing models that can handle distribution shifts and uncertainty quantification in astronomical data.", "Develop new methods for more efficient cross-matching of multimodal data to further enhance data utility.", "Apply the Multimodal Universe framework to other domains beyond astronomy to test its versatility and adaptability.", "Extend the dataset to include more fully cross-matched samples as more all-sky surveys become available."]}}
{"id": "97609", "url": "https://nips.cc/virtual/2024/poster/97609", "title": "DrivAerNet++: A Large-Scale Multimodal Car Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks", "authors": [], "abstract": "Abstract:We present DrivAerNet++, the largest and most comprehensive multimodal dataset for aerodynamic car design. DrivAerNet++ comprises 8,000 diverse car designs modeled with high-fidelity computational fluid dynamics (CFD) simulations. The dataset includes diverse car configurations such as fastback, notchback, and estateback, with different underbody and wheel designs to represent both internal combustion engines and electric vehicles. Each entry in the dataset features detailed 3D meshes, parametric models, aerodynamic coefficients, and extensive flow and surface field data, along with segmented parts for car classification and point cloud data. This dataset supports a wide array of machine learning applications including data-driven design optimization, generative modeling, surrogate model training, CFD simulation acceleration, and geometric classification. With more than 39 TB of publicly available engineering data, DrivAerNet++ fills a significant gap in available resources, providing high-quality, diverse data to enhance model training, promote generalization, and accelerate automotive design processes. Along with rigorous dataset validation, we also provide ML benchmarking results on the task of aerodynamic drag prediction, showcasing the breadth of applications supported by our dataset. This dataset is set to significantly impact automotive design and broader engineering disciplines by fostering innovation and improving the fidelity of aerodynamic evaluations. Dataset and code available at: https://github.com/Mohamedelrefaie/DrivAerNet", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Mohamedelrefaie/DrivAerNet"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": "2 NVIDIA GPUs (e.g., RTX 3090 or equivalent)", "memory": "1000 GB", "storage": "39 TB"}, "time_requirements": "3 million CPU-hours", "code_quality": "Well documented", "difficulty": 4, "stars": 274}, "research_ideas": {"problem_statements": ["The paper addresses the lack of publicly available, large-scale, and multimodal car datasets which hinders progress in data-driven aerodynamic design.", "Current datasets often lack critical components like wheels, mirrors, and underbodies, which are essential for accurate aerodynamic assessments.", "There is a need for high-fidelity datasets that include experimental validation to confirm the accuracy and reliability of computational models.", "The challenge of achieving a balance between aesthetic appeal and aerodynamic efficiency in car design, which impacts fuel consumption and vehicle range.", "The paper aims to provide a dataset that enhances model training, promotes generalization, and accelerates automotive design processes."], "main_takeaways": ["DrivAerNet++ is the largest and most comprehensive multimodal dataset for aerodynamic car design, comprising 8,000 diverse car designs modeled with high-fidelity CFD simulations.", "The dataset includes detailed 3D meshes, parametric models, aerodynamic coefficients, flow and surface field data, and segmented parts for classification and point cloud data.", "DrivAerNet++ supports a wide array of machine learning applications, including design optimization, surrogate model training, and geometric classification.", "The dataset is validated with ML benchmarking results on aerodynamic drag prediction, showcasing its applicability in automotive design and broader engineering disciplines.", "DrivAerNet++ fills a significant gap in available resources, providing high-quality, diverse data to enhance model training and promote generalization."], "testable_hypotheses": [{"hypothesis": "Does including detailed underbody and wheel designs improve the predictive accuracy of aerodynamic models?", "method": "Train models on datasets with and without these components and compare the prediction accuracy.", "expected_outcome": "Models trained on datasets with detailed underbody and wheel designs will show improved accuracy."}, {"hypothesis": "Can the DrivAerNet++ dataset be used to train models that outperform existing solutions in aerodynamic drag prediction?", "method": "Benchmark models trained on DrivAerNet++ against those trained on other datasets using standard metrics.", "expected_outcome": "Models trained on DrivAerNet++ are expected to achieve better performance due to the dataset's diversity and fidelity."}, {"hypothesis": "Will increasing the dataset size lead to better generalization and performance of deep learning models in predicting aerodynamic coefficients?", "method": "Train models on subsets of varying sizes from DrivAerNet++ and evaluate their performance on a separate test set.", "expected_outcome": "Larger subsets will lead to better generalization and prediction performance."}, {"hypothesis": "Does the inclusion of experimental validation data improve the reliability of simulations and model predictions?", "method": "Compare simulation results and model predictions with and without using experimental validation data.", "expected_outcome": "Inclusion of experimental validation data will improve the reliability of both simulations and predictions."}, {"hypothesis": "Are there unexplored geometric parameters that could significantly impact the aerodynamic performance within the DrivAerNet++ dataset?", "method": "Conduct sensitivity analysis on the geometric parameters to identify those with significant impact.", "expected_outcome": "Some parameters not previously considered might show a significant impact on aerodynamic performance."}], "follow_up_work_ideas": ["Integrate transient CFD simulations to capture more complex aerodynamic phenomena and improve model robustness.", "Expand the dataset to include 2D image renderings to support multimodal learning approaches.", "Develop advanced surrogate models using Geometry-Informed Neural Operators or Convolutional Occupancy Networks for better predictive performance.", "Explore the application of DrivAerNet++ in other domains, such as aerospace design, to leverage its comprehensive data for broader engineering challenges.", "Investigate the potential of using DrivAerNet++ for accelerating CFD simulations through transfer learning and multi-fidelity modeling."]}}
{"id": "97674", "url": "https://nips.cc/virtual/2024/poster/97674", "title": "Needle In A Multimodal Haystack", "authors": [], "abstract": "Abstract:With the rapid advancement of multimodal large language models (MLLMs), their evaluation has become increasingly comprehensive. However, understanding long multimodal content, as a foundational ability for real-world applications, remains underexplored. In this work, we present Needle In A Multimodal Haystack (MM-NIAH), the first benchmark specifically designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents. Our benchmark includes three types of evaluation tasks: multimodal retrieval, counting, and reasoning. In each task, the model is required to answer the questions according to different key information scattered throughout the given multimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation. We hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs. Code and benchmark are released at https://github.com/OpenGVLab/MM-NIAH.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/OpenGVLab/MM-NIAH"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 8}, "memory": "not specified", "api_calls": "not specified"}, "time_requirements": 5, "code_quality": "well-documented code", "difficulty": 3, "stars": 112}, "research_ideas": {"problem_statements": ["Existing Multimodal Large Language Models (MLLMs) struggle to comprehend long multimodal documents effectively due to limitations in context window size.", "There is a lack of appropriate evaluation benchmarks specifically designed for long-context multimodal understanding in MLLMs.", "Current evaluation benchmarks for MLLMs focus on short contexts or vision-dominant understanding rather than text-dominant multimodal document comprehension."], "main_takeaways": ["MM-NIAH is the first benchmark specifically designed to evaluate the comprehension capability of MLLMs for long multimodal documents.", "Existing MLLMs perform significantly worse on tasks involving image needles compared to text needles.", "Training on image-text interleaved data does not necessarily improve MLLMs' performance on long multimodal document comprehension.", "The RAG method is ineffective for retrieving image needles in MM-NIAH, highlighting the challenges in long multimodal document comprehension."], "testable_hypotheses": [{"hypothesis": "Existing MLLMs have a better comprehension capability for text needles than image needles in long multimodal documents.", "method": "Evaluate MLLMs on MM-NIAH using both text and image needles and compare performance metrics across these modalities.", "expected_outcome": "MLLMs will perform better on text needles compared to image needles."}, {"hypothesis": "RAG enhances text needle retrieval but not image needle retrieval in long multimodal documents.", "method": "Implement RAG on MLLMs and measure retrieval performance for both text and image needles on MM-NIAH.", "expected_outcome": "RAG will significantly improve text needle retrieval but show minimal improvement for image needle retrieval."}, {"hypothesis": "Increasing the context length degrades the performance of MLLMs in multimodal document comprehension tasks.", "method": "Evaluate MLLMs on MM-NIAH with varying context lengths and analyze the performance trend.", "expected_outcome": "Performance of MLLMs will decrease as context length increases."}, {"hypothesis": "MLLMs pre-trained on image-text interleaved data do not exhibit superior performance on long-context multimodal document comprehension compared to those trained on image-text pair data.", "method": "Compare the performance of MLLMs trained on interleaved data against those trained on paired data using MM-NIAH.", "expected_outcome": "No significant performance advantage for MLLMs trained on interleaved data."}, {"hypothesis": "Humans achieve near-perfect performance on MM-NIAH, indicating a gap between human-level comprehension and current MLLMs.", "method": "Conduct a human evaluation using MM-NIAH and compare the results with MLLMs' performance.", "expected_outcome": "Humans will significantly outperform MLLMs, achieving near-perfect scores."}], "follow_up_work_ideas": ["Develop new training techniques or data augmentations specifically designed to improve MLLMs' performance on long-context multimodal document comprehension.", "Investigate the integration of alternative retrieval methods that might offer improved performance for image needle retrieval in MLLMs.", "Explore the use of larger context windows during the training of MLLMs to maintain the underlying LLMs' long-context capabilities.", "Extend MM-NIAH to include more diverse and complex real-world multimodal documents to better challenge and evaluate MLLMs.", "Research into enhancing MLLMs' ability to follow instructions and accurately count or reason over long multimodal documents."]}}
{"id": "97882", "url": "https://nips.cc/virtual/2024/poster/97882", "title": "The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning", "authors": [], "abstract": "Abstract:Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at https://github.com/PolymathicAI/the_well.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/PolymathicAI/the_well"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA CUDA", "amount": 1}, "memory": "16GB", "disk_space": "15TB for full datasets"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 761}, "research_ideas": {"problem_statements": ["The efficacy of new machine learning approaches for surrogate modeling in physics is difficult to evaluate due to limited datasets covering diverse physical behaviors.", "Existing datasets for physics simulations either lack diversity or are limited to small classes of physical behavior, creating a gap for comprehensive evaluation.", "High-quality scientific simulations require specialized software, domain expertise, and significant computational resources, posing challenges for conventional deep learning training."], "main_takeaways": ["The Well provides a large-scale collection of diverse physics simulations across various domains, offering 15TB of data in 16 datasets.", "A unified PyTorch interface is provided to facilitate the training and evaluation of machine learning models using these datasets.", "Preliminary benchmarks using common machine learning models demonstrate the complexity and challenges posed by the datasets in The Well.", "The Well is designed to address the gap between complex real-world physics simulations and the datasets available for machine learning."], "testable_hypotheses": [{"hypothesis": "Does the diversity of datasets in The Well improve the generalization capability of machine learning models?", "method": "Train models using a subset of datasets and test on unseen datasets within The Well to evaluate generalization performance.", "expected_outcome": "Models trained on diverse datasets will show better generalization on unseen datasets compared to those trained on homogeneous datasets."}, {"hypothesis": "Will incorporating boundary conditions as constraints in the models improve prediction accuracy?", "method": "Implement models with explicit boundary condition handling and compare their performance against models without such constraints.", "expected_outcome": "Models with boundary condition constraints will outperform those without in terms of prediction accuracy."}, {"hypothesis": "Can the use of The Well accelerate the development of surrogate models for real-world physics applications?", "method": "Measure the time and computational resources required to develop surrogate models using The Well datasets compared to traditional methods.", "expected_outcome": "The Well will significantly reduce the development time and resource requirements for surrogate models."}, {"hypothesis": "Do models trained on high-resolution datasets from The Well perform better on super-resolution tasks?", "method": "Train models on high-resolution datasets and test their performance on lower-resolution datasets requiring super-resolution.", "expected_outcome": "Models trained on high-resolution datasets will outperform those trained on low-resolution datasets in super-resolution tasks."}, {"hypothesis": "Can The Well datasets be used to effectively benchmark the stability of long-term surrogate model predictions?", "method": "Evaluate model predictions over extended time horizons and assess their stability using The Well datasets.", "expected_outcome": "The Well will provide comprehensive benchmarks for evaluating the stability of long-term predictions in surrogate models."}], "follow_up_work_ideas": ["Explore the use of The Well for developing physics foundation models capable of handling multiple physics domains simultaneously.", "Investigate the potential of transfer learning using models trained on The Well datasets to other physics simulation tasks not included in The Well.", "Develop methods to incorporate physical constraints explicitly in machine learning models using The Well to improve prediction accuracy.", "Extend The Well to include higher resolution and more complex simulations to continue challenging the machine learning community.", "Study the impact of different data preprocessing techniques on model performance using The Well datasets."]}}
{"id": "97594", "url": "https://nips.cc/virtual/2024/poster/97594", "title": "IMDL-BenCo: A Comprehensive Benchmark and Codebase for Image Manipulation Detection & Localization", "authors": [], "abstract": "Abstract:A comprehensive benchmark is yet to be established in the Image Manipulation Detection \\& Localization (IMDL) field. The absence of such a benchmark leads to insufficient and misleading model evaluations, severely undermining the development of this field. However, the scarcity of open-sourced baseline models and inconsistent training and evaluation protocols make conducting rigorous experiments and faithful comparisons among IMDL models challenging. To address these challenges, we introduce IMDL-BenCo, the first comprehensive IMDL benchmark and modular codebase. IMDL-BenCo: i) decomposes the IMDL framework into standardized, reusable components and revises the model construction pipeline, improving coding efficiency and customization flexibility; ii) fully implements or incorporates training code for state-of-the-art models to establish a comprehensive IMDL benchmark; and iii) conducts deep analysis based on the established benchmark and codebase, offering new insights into IMDL model architecture, dataset characteristics, and evaluation standards.Specifically, IMDL-BenCo includes common processing algorithms, 8 state-of-the-art IMDL models (1 of which are reproduced from scratch), 2 sets of standard training and evaluation protocols, 15 GPU-accelerated evaluation metrics, and 3 kinds of robustness evaluation. This benchmark and codebase represent a significant leap forward in calibrating the current progress in the IMDL field and inspiring future breakthroughs.Code is available at: https://github.com/scu-zjz/IMDLBenCo", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/scu-zjz/IMDLBenCo"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 NVIDIA GPU (minimum 4GB VRAM recommended)", "memory": "8GB RAM minimum"}, "time_requirements": "1", "code_quality": "well-documented", "difficulty": 2, "stars": 115}, "research_ideas": {"problem_statements": ["The lack of a comprehensive benchmark in the Image Manipulation Detection & Localization (IMDL) field leads to insufficient and misleading model evaluations.", "There is a scarcity of open-sourced baseline models and inconsistent training and evaluation protocols in IMDL, making it challenging to conduct rigorous experiments and fair comparisons.", "Existing IMDL models suffer from inconsistent training and evaluation protocols, resulting in incompatible and unfair comparisons.", "The high coupling between various loss functions and training architectures makes it extremely difficult to extend different model training frameworks."], "main_takeaways": ["IMDL-BenCo introduces a comprehensive benchmark and modular codebase for the IMDL field, which decomposes the IMDL framework into standardized, reusable components.", "IMDL-BenCo includes a modular codebase with four components: data loader, model zoo, training script, and evaluator, which improves coding efficiency and customization flexibility.", "The benchmark includes 8 state-of-the-art IMDL models, 2 sets of standard training and evaluation protocols, 15 GPU-accelerated evaluation metrics, and 3 kinds of robustness evaluation.", "IMDL-BenCo conducts in-depth analysis based on the established benchmark and codebase, offering new insights into IMDL model architecture, dataset characteristics, and evaluation standards.", "The paper identifies significant inconsistencies in existing IMDL model evaluations and highlights the need for a unified benchmark to ensure fair and consistent evaluation."], "testable_hypotheses": [{"hypothesis": "Does using a unified training protocol improve the fairness of IMDL model evaluations?", "method": "Implement and compare various IMDL models using the unified training protocols provided by IMDL-BenCo.", "expected_outcome": "Unified training protocols should result in more consistent and fair comparisons across different models."}, {"hypothesis": "Can low-level feature extractors improve the performance of IMDL models?", "method": "Evaluate the performance of IMDL models with and without low-level feature extractors.", "expected_outcome": "Appropriate low-level feature extractors may enhance the performance of specific architectures like ResNet."}, {"hypothesis": "Will IMDL-BenCo's modular codebase improve coding efficiency and scalability compared to existing frameworks?", "method": "Measure the time and effort required to implement new IMDL models using IMDL-BenCo versus existing frameworks.", "expected_outcome": "IMDL-BenCo should demonstrate improved coding efficiency and flexibility in model implementation."}, {"hypothesis": "Are current evaluation metrics for IMDL models overestimating model performance?", "method": "Analyze the performance of IMDL models using different evaluation metrics, focusing on the discrepancies between AUC and F1 scores.", "expected_outcome": "AUC scores may overestimate performance compared to F1 scores, which better reflect model localization capabilities."}, {"hypothesis": "Does dataset bias, such as label leakage, significantly impact the evaluation of IMDL models?", "method": "Evaluate model performance on both the original and cleansed versions of datasets like NIST16 to identify performance discrepancies.", "expected_outcome": "Dataset cleansing should result in more accurate assessments of model performance by removing bias."}], "follow_up_work_ideas": ["Explore alternative feature extraction methods that may further enhance IMDL model performance.", "Investigate the impact of different backbone architectures on IMDL model generalization and robustness.", "Develop new evaluation metrics that better capture the nuances of IMDL tasks, considering the limitations of existing metrics.", "Apply the IMDL-BenCo benchmark to other domains in information forensics and security to test its generalizability.", "Conduct user studies to assess the usability and effectiveness of IMDL-BenCo's modular codebase in real-world research settings."]}}
{"id": "97868", "url": "https://nips.cc/virtual/2024/poster/97868", "title": "Vript: A Video Is Worth Thousands of Words", "authors": [], "abstract": "Abstract:Advancements in multimodal learning, particularly in video understanding and generation, require high-quality video-text datasets for improved model performance. Vript addresses this issue with a meticulously annotated corpus of 12K high-resolution videos, offering detailed, dense, and script-like captions for over 420K clips. Each clip has a caption of ~145 words, which is over 10x longer than most video-text datasets. Unlike captions only documenting static content in previous datasets, we enhance video captioning to video scripting by documenting not just the content, but also the camera operations, which include the shot types (medium shot, close-up, etc) and camera movements (panning, tilting, etc). By utilizing the Vript, we explore three training paradigms of aligning more text with the video modality rather than clip-caption pairs. This results in Vriptor, a top-performing video captioning model among open-source models, comparable to GPT-4V in performance. Vriptor is also a powerful model capable of end-to-end generation of dense and detailed captions for long videos. Moreover, we introduce Vript-Hard, a benchmark consisting of three video understanding tasks that are more challenging than existing benchmarks: Vript-HAL is the first benchmark evaluating action and object hallucinations in video LLMs, Vript-RR combines reasoning with retrieval resolving question ambiguity in long-video QAs, and Vript-ERO is a new task to evaluate the temporal understanding of events in long videos rather than actions in short videos in previous works. All code, models, and datasets are available in https://github.com/mutonix/Vript.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/mutonix/Vript"}, "reproduce_difficulty": {"environment_setup": "Easy (using pip and conda)", "resource_requirements": {"GPUs": "1 NVIDIA 1080 or equivalent", "memory": "16 GB RAM", "API calls": "Requires access to OpenAI API for model inference"}, "time_requirements": "2 hours", "code_quality": "Well-documented code with clear instructions", "difficulty": 2, "stars": 136}, "research_ideas": {"problem_statements": ["The need for high-quality video-text datasets for improved multimodal learning, particularly in video understanding and generation.", "Existing video-text datasets are limited by short, coarse-grained descriptions that fail to capture the temporal and detailed content of videos.", "Current benchmarks for video understanding do not adequately evaluate the comprehension of complex video content, including hallucination detection and temporal event understanding."], "main_takeaways": ["Vript is a meticulously annotated corpus of 12K high-resolution videos with detailed, dense captions, enhancing video captioning to video scripting.", "The Vript dataset offers captions that are significantly longer and more detailed than those in existing datasets, documenting not just content but also camera operations.", "Vriptor, a video captioning model trained on the Vript dataset, achieves state-of-the-art performance among open-source models, comparable to GPT-4V.", "Vript-Hard is introduced as a challenging benchmark for video understanding, evaluating action and object hallucinations, reasoning with retrieval, and temporal event understanding."], "testable_hypotheses": [{"hypothesis": "Does the inclusion of camera operation details in captions improve video-language model performance?", "method": "Train video-language models on datasets with and without camera operation details in captions, and compare their performance on video understanding tasks.", "expected_outcome": "Models trained on datasets with camera operation details will perform better on video understanding tasks."}, {"hypothesis": "Will Vriptor outperform other open-source video captioning models in generating detailed captions for long videos?", "method": "Evaluate Vriptor and other models on a set of long video captioning tasks, measuring detail and accuracy of captions.", "expected_outcome": "Vriptor will generate more detailed and accurate captions compared to other models."}, {"hypothesis": "Can voice-over transcription reduce hallucinations in video captions?", "method": "Compare the hallucination rates in captions generated with and without voice-over transcription data.", "expected_outcome": "Inclusion of voice-over transcription will reduce hallucination rates in video captions."}, {"hypothesis": "Does the use of video timestamps improve the temporal understanding of video-Language models?", "method": "Evaluate models trained with and without timestamps on tasks requiring temporal understanding, such as event re-ordering.", "expected_outcome": "Models with timestamp data will show improved performance on temporal understanding tasks."}, {"hypothesis": "Are longer captions more prone to generating hallucinations in video language models?", "method": "Analyze the precision and recall of hallucinations in captions of varying lengths generated by video language models.", "expected_outcome": "Longer captions will have higher hallucination rates due to increased detail."}], "follow_up_work_ideas": ["Investigate the impact of different levels of detail in captions on various video understanding tasks.", "Explore the integration of additional modalities, such as audio cues, to further enhance model understanding and caption accuracy.", "Develop new benchmarks focusing on the interaction between multiple characters or objects within videos.", "Apply the Vript dataset and methodologies to other domains, such as instructional videos or documentaries, to evaluate broader applicability.", "Enhance the Vript dataset by incorporating user-generated content from diverse cultural contexts to study the model's adaptability to different styles and narratives."]}}
{"id": "97746", "url": "https://nips.cc/virtual/2024/poster/97746", "title": "Benchmarking LLMs via Uncertainty Quantification", "authors": [], "abstract": "Abstract:The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves nine LLMs (LLM series) spanning five representative natural language processing tasks. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. These results underscore the significance of incorporating uncertainty in the evaluation of LLMs. Our implementation is available at https://github.com/smartyfh/LLM-Uncertainty-Bench.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/smartyfh/LLM-Uncertainty-Bench"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "NVIDIA GPUs recommended", "amount": "1 or more", "memory": "16 GB or more recommended"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 209}, "research_ideas": {"problem_statements": ["The current evaluation platforms for Large Language Models (LLMs) neglect the aspect of uncertainty quantification, which is crucial for a comprehensive assessment.", "Existing leaderboards focus only on accuracy metrics, ignoring the varying levels of uncertainty that different LLMs may exhibit.", "There is a need to develop a systematic and statistically rigorous method to quantify uncertainty in LLMs."], "main_takeaways": ["LLMs with higher accuracy may not necessarily show higher certainty.", "Larger-scale LLMs tend to display greater uncertainty than their smaller counterparts.", "Instruction-finetuning LLMs generally increases their uncertainty.", "Conformal prediction is proposed as a practical and principled method for assessing the uncertainty of LLMs."], "testable_hypotheses": [{"hypothesis": "Does increasing the model size of LLMs lead to higher uncertainty?", "method": "Evaluate LLMs of varying sizes using the set size (SS) metric from conformal prediction.", "expected_outcome": "Larger LLMs will show higher uncertainty compared to smaller ones."}, {"hypothesis": "Does instruction-finetuning increase the uncertainty of LLMs?", "method": "Compare the uncertainty levels (using SS metric) of models before and after instruction-finetuning.", "expected_outcome": "Instruction-finetuned models will display higher uncertainty."}, {"hypothesis": "Can conformal prediction provide a more accurate uncertainty quantification than entropy-based methods?", "method": "Compare the uncertainty metrics (SS) derived from conformal prediction and entropy on the same set of LLMs.", "expected_outcome": "Conformal prediction will yield more reliable uncertainty metrics."}, {"hypothesis": "Does the inclusion of a calibration set size significantly affect the uncertainty quantification?", "method": "Vary the size of the calibration set and evaluate changes in the coverage rate and SS.", "expected_outcome": "Changes in calibration set size will not significantly affect coverage rates."}, {"hypothesis": "Are the uncertainty assessments consistent across different NLP tasks?", "method": "Evaluate the same model across different tasks (QA, RC, CI, etc.) using SS metric.", "expected_outcome": "Uncertainty levels will vary across different tasks, highlighting task-specific uncertainty."}], "follow_up_work_ideas": ["Explore the use of conformal prediction for uncertainty quantification in generative tasks of LLMs.", "Investigate the impact of different conformal score functions on the uncertainty quantification of LLMs.", "Develop methods to incorporate uncertainty quantification into the training process of LLMs to improve robustness.", "Apply the proposed benchmarking with uncertainty quantification to multi-modal models that handle non-language data.", "Investigate the relationship between uncertainty quantification and the interpretability of LLMs."]}}
{"id": "97425", "url": "https://nips.cc/virtual/2024/poster/97425", "title": "XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX", "authors": [], "abstract": "Abstract:Inspired by the diversity and depth of XLand and the simplicity and minimalism of MiniGrid, we present XLand-MiniGrid, a suite of tools and grid-world environments for meta-reinforcement learning research. Written in JAX, XLand-MiniGrid is designed to be highly scalable and can potentially run on GPU or TPU accelerators, democratizing large-scale experimentation with limited resources. Along with the environments, XLand-MiniGrid provides pre-sampled benchmarks with millions of unique tasks of varying difficulty and easy-to-use baselines that allow users to quickly start training adaptive agents. In addition, we have conducted a preliminary analysis of scaling and generalization, showing that our baselines are capable of reaching millions of steps per second during training and validating that the proposed benchmarks are challenging. XLand-MiniGrid is open-source and available at \\url{https://github.com/corl-team/xland-minigrid}.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/corl-team/xland-minigrid"}, "reproduce_difficulty": {"environment_setup": "moderate", "resource_requirements": {"GPUs": "1 (NVIDIA recommended for JAX)", "memory": "8GB or more recommended", "API_call": "HuggingFace for benchmark datasets"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 265}, "research_ideas": {"problem_statements": ["Reinforcement learning (RL) is known to be extremely sample inefficient and prone to overfitting, sometimes failing to generalize to even subtle variations in environmental dynamics or goals.", "Meta-RL methods require thousands of different tasks for generalization, resulting in significantly increased pre-training requirements.", "The XLand environment is not publicly available, hindering the use of complex environments for adaptive agent research.", "There is a need for scalable meta-RL environments that do not compromise on task complexity while being accessible to researchers with limited resources."], "main_takeaways": ["XLand-MiniGrid is a scalable suite of tools and grid-world environments for meta-reinforcement learning research, written in JAX, allowing it to run efficiently on GPU or TPU accelerators.", "The library provides pre-sampled benchmarks with millions of unique tasks and easy-to-use baselines for training adaptive agents.", "XLand-MiniGrid introduces a system of extensible rules and goals to generate diverse task distributions and supports goal-oriented grid world environments.", "The proposed environments are capable of reaching millions of steps per second, making large-scale experimentation more accessible.", "The benchmarks provided are challenging and highlight significant room for improvement in terms of generalization and performance of current RL baselines."], "testable_hypotheses": [{"hypothesis": "Does the use of JAX in XLand-MiniGrid improve simulation throughput compared to other environments?", "method": "Benchmark simulation throughput of XLand-MiniGrid against other popular RL environments implemented in different frameworks.", "expected_outcome": "XLand-MiniGrid will achieve higher simulation throughput due to JAX's ability to utilize GPUs and TPUs efficiently."}, {"hypothesis": "Can the RL2 agent trained with XLand-MiniGrid generalize better to new tasks compared to traditional RL agents?", "method": "Train both RL2 and traditional RL agents on XLand-MiniGrid benchmarks and compare their performance on unseen tasks.", "expected_outcome": "RL2 agents will show better generalization to new tasks due to their meta-learning capabilities."}, {"hypothesis": "Does increasing the number of parallel environments improve the training throughput of RL2 agents?", "method": "Conduct experiments varying the number of parallel environments and measure the training throughput for RL2 agents.", "expected_outcome": "Training throughput will increase with more parallel environments until saturation is reached."}, {"hypothesis": "Will providing agents with rule and goal information improve their task-solving efficiency in XLand-MiniGrid?", "method": "Train agents with and without access to rule and goal information and compare their task-solving efficiency.", "expected_outcome": "Agents with access to rule and goal information will solve tasks more efficiently."}, {"hypothesis": "Does the introduction of distractor rules affect the learning performance of RL agents in XLand-MiniGrid?", "method": "Train RL agents on tasks with and without distractor rules and compare their learning performance.", "expected_outcome": "Distractor rules will decrease learning performance as they introduce complexity and potential dead ends."}], "follow_up_work_ideas": ["Explore multi-agent simulations and procedural generation of complex worlds in XLand-MiniGrid to increase task diversity.", "Investigate the impact of different rule and goal combinations on the generalization capabilities of RL agents.", "Develop more sophisticated baselines by incorporating state-of-the-art neural architectures such as Transformers to improve training throughput and efficiency.", "Extend XLand-MiniGrid to support language-based rules and goals for research in language-conditioned meta-RL.", "Conduct large-scale experiments to study the effect of different hyperparameter settings on the scalability and performance of RL2 agents in XLand-MiniGrid."]}}
{"id": "97445", "url": "https://nips.cc/virtual/2024/poster/97445", "title": "ProG: A Graph Prompt Learning Benchmark", "authors": [], "abstract": "Abstract:Artificial general intelligence on graphs has shown significant advancements across various applications, yet the traditional `Pre-train \\& Fine-tune' paradigm faces inefficiencies and negative transfer issues, particularly in complex and few-shot settings. Graph prompt learning emerges as a promising alternative, leveraging lightweight prompts to manipulate data and fill the task gap by reformulating downstream tasks to the pretext. However, several critical challenges still remain: how to unify diverse graph prompt models, how to evaluate the quality of graph prompts, and to improve their usability for practical comparisons and selection. In response to these challenges, we introduce the first comprehensive benchmark for graph prompt learning. Our benchmark integratesSIXpre-training methods andFIVEstate-of-the-art graph prompt techniques, evaluated acrossFIFTEENdiverse datasets to assess performance, flexibility, and efficiency. We also present 'ProG', an easy-to-use open-source library that streamlines the execution of various graph prompt models, facilitating objective evaluations. Additionally, we propose a unified framework that categorizes existing graph prompt methods into two main approaches: prompts as graphs and prompts as tokens. This framework enhances the applicability and comparison of graph prompt techniques. The code is available at: https://github.com/sheldonresearch/ProG.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/sheldonresearch/ProG"}, "reproduce_difficulty": {"environment_setup": "Moderate difficulty, requires conda for environment setup and installation of multiple dependencies.", "resource_requirements": {"GPUs": {"type": "CUDA-enabled GPU", "amount": "1"}, "memory": "At least 8GB RAM recommended", "others": "Python 3.9 or higher"}, "time_requirements": "Approximately 5 hours for initial setup and running experiments.", "code_quality": "Well-documented code with clear instructions.", "difficulty": 3, "stars": 529}, "research_ideas": {"problem_statements": ["How can we unify diverse graph prompt models given their varying methodologies?", "How do we evaluate the quality of graph prompts in terms of efficiency, power, and flexibility?", "How can we make graph prompt approaches more user-friendly for practical comparison and selection?"], "main_takeaways": ["The paper introduces the first comprehensive benchmark for graph prompt learning, integrating six pre-training methods and five state-of-the-art graph prompting methods across fifteen datasets.", "Graph prompt methods generally outperform traditional pre-training and fine-tuning methods, particularly in few-shot learning settings, demonstrating superior positive transfer and flexibility.", "The proposed benchmark and the ProG library facilitate objective evaluation and practical application of graph prompt methods, providing a unified framework for existing graph prompting approaches.", "Graph prompt methods significantly reduce negative transfer issues associated with traditional pre-trained models."], "testable_hypotheses": [{"hypothesis": "Graph prompt methods will outperform traditional pre-training & fine-tuning methods in few-shot settings.", "method": "Conduct experiments comparing graph prompt methods to pre-training & fine-tuning methods on few-shot learning tasks across various datasets.", "expected_outcome": "Graph prompt methods will show superior performance in terms of accuracy and reduced negative transfer."}, {"hypothesis": "The flexibility of graph prompts can be quantified by measuring the error bound between manipulated and original graphs.", "method": "Compute the error bound using Equation 1 to assess flexibility across different graph prompt methods and datasets.", "expected_outcome": "Graph prompt methods will achieve low error bounds, indicating high flexibility."}, {"hypothesis": "Graph-level pre-training methods are more effective for graph-level tasks, while node-level pre-training is more suitable for node-level tasks.", "method": "Evaluate the performance of graph prompt methods using different pre-training levels on corresponding tasks.", "expected_outcome": "Graph-level tasks will benefit more from graph-level pre-training, and node-level tasks will benefit more from node-level pre-training."}, {"hypothesis": "Graph prompt methods can mitigate negative transfer more effectively than fine-tuning traditional models.", "method": "Compare the rate of negative transfer occurrences between graph prompt methods and traditional fine-tuning methods across datasets.", "expected_outcome": "Graph prompt methods will show a lower rate of negative transfer."}, {"hypothesis": "Graph prompt methods will exhibit varying adaptability across different domains and dataset sizes.", "method": "Assess graph prompt performance on datasets of varying domains and sizes, including homophilic, heterophilic, and large-scale datasets.", "expected_outcome": "Graph prompt methods will perform consistently across different domains, with potential challenges on large-scale datasets."}], "follow_up_work_ideas": ["Explore the application of graph prompt learning to other graph-based tasks beyond classification, such as link prediction or clustering.", "Investigate the impact of different prompt structures and insertion patterns on the performance of graph prompt methods.", "Develop strategies to enhance the scalability of graph prompt methods for large-scale datasets.", "Examine the robustness of graph prompt methods against adversarial attacks or noisy data.", "Expand the ProG library to include automated prompt selection and optimization features."]}}
{"id": "97533", "url": "https://nips.cc/virtual/2024/poster/97533", "title": "shapiq: Shapley Interactions for Machine Learning", "authors": [], "abstract": "Abstract:Originally rooted in game theory, the Shapley Value (SV) has recently become an important tool in machine learning research. Perhaps most notably, it is used for feature attribution and data valuation in explainable artificial intelligence. Shapley Interactions (SIs) naturally extend the SV and address its limitations by assigning joint contributions to groups of entities, which enhance understanding of black box machine learning models. Due to the exponential complexity of computing SVs and SIs, various methods have been proposed that exploit structural assumptions or yield probabilistic estimates given limited resources. In this work, we introduce shapiq, an open-source Python package that unifies state-of-the-art algorithms to efficiently compute SVs and any-order SIs in an application-agnostic framework. Moreover, it includes a benchmarking suite containing 11 machine learning applications of SIs with pre-computed games and ground-truth values to systematically assess computational performance across domains. For practitioners, shapiq is able to explain and visualize any-order feature interactions in predictions of models, including vision transformers, language models, as well as XGBoost and LightGBM with TreeSHAP-IQ. With shapiq, we extend shap beyond feature attributions and consolidate the application of SVs and SIs in machine learning that facilitates future research. The source code and documentation are available at https://github.com/mmschlk/shapiq.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/mmschlk/shapiq"}, "reproduce_difficulty": {"environment_setup": "Easy, can be installed using pip.", "resource_requirements": "Standard CPU is sufficient, no specific GPU required.", "time_requirements": "Approximately 1 hour to run the main example.", "code_quality": "Well-documented code.", "difficulty": 2, "stars": 408}, "research_ideas": {"problem_statements": ["The paper addresses the computational complexity of calculating Shapley Values (SVs) and Shapley Interactions (SIs), which are important for explainable AI but traditionally require exponential resources.", "There is a need for a unified, application-agnostic tool that can compute SVs and any-order SIs efficiently and be used across various machine learning domains.", "Existing Python packages for SVs and SIs are limited in scope and do not adequately support higher-order interactions or provide comprehensive benchmarking capabilities."], "main_takeaways": ["The shapiq Python package unifies state-of-the-art algorithms for efficiently computing Shapley Values and any-order Shapley Interactions.", "shapiq provides a benchmarking suite across 11 machine learning applications to systematically assess computational performance.", "The package includes algorithms that can explain and visualize any-order feature interactions for various models, such as vision transformers and language models.", "shapiq extends the application of Shapley Values beyond feature attributions by providing tools for generalized cooperative game analysis in machine learning."], "testable_hypotheses": [{"hypothesis": "Does using TreeSHAP-IQ for tree models significantly reduce the time complexity compared to traditional methods?", "method": "Benchmark TreeSHAP-IQ against traditional Shapley Value computation methods on tree models of varying sizes.", "expected_outcome": "TreeSHAP-IQ will demonstrate reduced time complexity and computational resources compared to traditional methods."}, {"hypothesis": "Will KernelSHAP-IQ provide more accurate approximations of SIs compared to standard KernelSHAP in a limited budget setting?", "method": "Compare the approximation accuracy of KernelSHAP-IQ and KernelSHAP using pre-computed benchmark games with a fixed budget.", "expected_outcome": "KernelSHAP-IQ is expected to provide more accurate approximations due to its optimized algorithm for higher-order interactions."}, {"hypothesis": "Can the inclusion of higher-order interactions (3-order and above) in explanations improve the interpretability of complex model predictions?", "method": "Conduct user studies comparing interpretability of model predictions with and without higher-order interactions using shapiq visualizations.", "expected_outcome": "Inclusion of higher-order interactions will enhance interpretability, especially in complex models with intricate feature dependencies."}, {"hypothesis": "Does the use of conditional feature imputation improve the accuracy of Shapley Interaction calculations in datasets with dependent features?", "method": "Evaluate the accuracy of SIs calculated using conditional vs. marginal imputation techniques on datasets with known feature dependencies.", "expected_outcome": "Conditional imputation will yield more accurate Shapley Interaction values in datasets with dependent features."}, {"hypothesis": "Will shapiq's benchmarking suite reveal significant differences in computational performance across different machine learning domains?", "method": "Analyze performance metrics from the benchmarking suite across 11 different machine learning applications provided by shapiq.", "expected_outcome": "Significant differences in computational performance are expected, highlighting domain-specific challenges and optimizations."}], "follow_up_work_ideas": ["Explore the implementation of TreeSHAP-IQ in C++ to enhance efficiency and integration into production systems.", "Investigate the development of new visualization techniques for higher-order feature interactions to improve human interpretability.", "Conduct research on the impact of different Shapley Interaction indices on model interpretability and decision-making in real-world applications.", "Expand the shapiq benchmarking suite to include more datasets and models, especially in emerging fields like reinforcement learning and unsupervised learning.", "Develop user-friendly interfaces and tutorials to broaden the accessibility of shapiq for non-expert users in explainable AI."]}}
{"id": "97454", "url": "https://nips.cc/virtual/2024/poster/97454", "title": "CALE: Continuous Arcade Learning Environment", "authors": [], "abstract": "Abstract:We introduce the Continuous Arcade Learning Environment (CALE), an extension of the well-known Arcade Learning Environment (ALE) [Bellemare et al., 2013]. The CALE uses the same underlying emulator of the Atari 2600 gaming system (Stella), but adds support for continuous actions. This enables the benchmarking and evaluation of continuous-control agents (such as PPO [Schulman et al., 2017] and SAC [Haarnoja et al., 2018]) and value-based agents (such as DQN [Mnih et al., 2015] and Rainbow [Hessel et al., 2018]) on the same environment suite. We provide a series of open questions and research directions that CALE enables, as well as initial baseline results using Soft Actor-Critic. CALE is available as part of the ALE athttps://github.com/Farama-Foundation/Arcade-Learning-Environment.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Farama-Foundation/Arcade-Learning-Environment"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "Optional, depends on the complexity of the agents being developed", "memory": "Not specified, but typical requirements for running AI agents in Python", "API_calls": "None specified"}, "time_requirements": "1", "code_quality": "well-documented code", "difficulty": 2, "stars": 2237}, "research_ideas": {"problem_statements": ["The need for a benchmark that supports both discrete and continuous action spaces for evaluating reinforcement learning agents.", "The lack of a unified platform to evaluate the performance of both discrete and continuous control agents on the same set of tasks.", "The challenges in representation learning, exploration, transfer, and offline RL in the context of continuous action spaces."], "main_takeaways": ["CALE extends the Arcade Learning Environment (ALE) to support continuous actions, enabling the evaluation of both discrete and continuous control agents.", "The introduction of a continuous action space allows for a more human-like interaction with the Atari 2600 console.", "Initial baseline results using Soft Actor-Critic highlight the need for further research in developing general agents capable of handling diverse domains.", "The paper identifies key challenges in representation learning, exploration, transfer, and offline RL that CALE enables researchers to explore."], "testable_hypotheses": [{"hypothesis": "Continuous control agents will outperform discrete control agents in CALE when the threshold for joystick sensitivity is optimally set.", "method": "Conduct experiments comparing the performance of continuous and discrete agents on CALE with varying joystick sensitivity thresholds.", "expected_outcome": "Continuous agents will show improved performance with optimal threshold settings compared to discrete agents."}, {"hypothesis": "The choice of network architecture significantly affects the performance of continuous control agents in CALE.", "method": "Evaluate continuous control agents with different network architectures (e.g., convolutional vs. MLP) on CALE and compare their performance.", "expected_outcome": "Certain network architectures will provide better performance due to their ability to represent continuous action spaces effectively."}, {"hypothesis": "Continuous control agents using entropy maximization for exploration will outperform those using epsilon-greedy exploration in CALE.", "method": "Implement both exploration strategies in continuous control agents and compare their performance on CALE.", "expected_outcome": "Entropy maximization will lead to better exploration and performance compared to epsilon-greedy strategies."}, {"hypothesis": "Adjusting the initialization of action outputs in continuous control agents will lead to improved performance in CALE.", "method": "Experiment with different initializations of action outputs for continuous control agents and measure their impact on performance in CALE.", "expected_outcome": "Optimized initialization settings will enhance the performance of continuous control agents."}, {"hypothesis": "Continuous control agents will have different performance profiles across games with varying reward sparsity compared to discrete control agents.", "method": "Analyze the performance of continuous and discrete agents on CALE games with different levels of reward sparsity.", "expected_outcome": "Continuous agents may perform better in environments with denser rewards due to their ability to handle continuous action spaces."}], "follow_up_work_ideas": ["Explore alternative action parameterizations and inductive biases for continuous control agents in CALE.", "Investigate the impact of different exploration strategies on the performance of continuous control agents in non-robotics tasks.", "Develop new representation learning techniques that are tailored for continuous action spaces in CALE.", "Study the applicability of offline RL techniques in continuous action environments like CALE.", "Expand CALE to include support for different types of controllers, such as paddles, to provide a broader range of continuous actions."]}}
{"id": "97581", "url": "https://nips.cc/virtual/2024/poster/97581", "title": "UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling", "authors": [], "abstract": "Abstract:Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. Yet, with an ever-growing number of benchmarks,researchers are tasked with the heavy burden of implementing each protocol, bearing a non-trivial computational cost, and making sense of how all these benchmarks translate into meaningful axes of progress.To facilitate a systematic evaluation of VLM progress, we introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a range of carefully categorized vision-centric capabilities from object recognition to spatial awareness, counting, and much more. We showcase the utility of UniBench for measuring progress by evaluating nearly 60 publicly available vision-language models, trained on scales of up to 12.8B samples. We find that while scaling training data or model size can boost many vision-language model capabilities, scaling offers little benefit for reasoning or relations.  Surprisingly, we also discover today's best VLMs struggle on simple digit recognition and counting tasks, e.g. MNIST, which much simpler networks can solve. Where scale falls short, we find that more precise interventions, such as data quality or tailored-learning objectives offer more promise. For practitioners, we also offer guidance on selecting a suitable VLM for a given application. Finally, we release an easy-to-run UniBench code-base with the full set of 50+ benchmarks and comparisons across 59 models as well as a distilled, representative set of benchmarks that runs in 5 minutes on a single GPU. UniBench with model evaluations on all benchmarks are provided as a toolbox at: https://github.com/facebookresearch/unibench", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/facebookresearch/unibench"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires conda for environment setup and specific package installations)", "resource_requirements": "1 GPU (NVIDIA recommended), 8GB RAM minimum", "time_requirements": "1-2 hours (depends on dataset size and model complexity)", "code_quality": "Well-documented code with clear instructions and examples", "difficulty": 3, "stars": 192}, "research_ideas": {"problem_statements": ["The paper addresses the fragmented evaluation landscape of vision-language models (VLMs) due to the proliferation of benchmarks.", "There is a need for a unified evaluation framework to systematically assess VLMs across diverse capabilities such as reasoning, spatial understanding, and robustness.", "Current VLMs struggle with basic tasks like digit recognition and counting, which simpler models can solve, highlighting blind spots in their capabilities.", "Scaling model size and training data has limited impact on VLMs' reasoning and relational understanding capabilities."], "main_takeaways": ["UniBench is introduced as a unified framework to evaluate VLMs across 50+ benchmarks, allowing for comprehensive assessments of model strengths and weaknesses.", "Scaling data or model size improves many VLM capabilities but offers little benefit for reasoning and relational tasks.", "VLMs currently perform poorly on simple tasks such as MNIST, which demonstrates a need for better training data quality and tailored learning objectives.", "Data quality and specific learning objectives are more effective than scale alone for improving reasoning and relational understanding in VLMs."], "testable_hypotheses": [{"hypothesis": "Does increasing the quality of training data improve VLM performance on reasoning tasks more than increasing the quantity?", "method": "Train VLMs on datasets filtered for quality versus datasets increased in size without quality filtering and compare performance on reasoning benchmarks.", "expected_outcome": "Models trained on higher quality data will perform better on reasoning tasks."}, {"hypothesis": "Can tailored learning objectives significantly improve VLM performance on relational benchmarks?", "method": "Implement tailored learning objectives focused on relational tasks and evaluate on benchmarks like Visual Genome.", "expected_outcome": "Models with tailored learning objectives will outperform baseline models on relational benchmarks."}, {"hypothesis": "Will smaller, specialized models outperform larger VLMs on simple tasks like MNIST?", "method": "Compare performance of small specialized neural networks against large VLMs on MNIST.", "expected_outcome": "Smaller models will have higher accuracy on MNIST than large VLMs."}, {"hypothesis": "Does the inclusion of diverse geographic data in training sets improve VLMs\u2019 performance on geographic diversity benchmarks?", "method": "Train VLMs with and without diverse geographic data and evaluate on benchmarks like Dollar Street.", "expected_outcome": "Models trained with diverse geographic data will show better performance on geographic diversity benchmarks."}, {"hypothesis": "Can prompt engineering improve VLM performance on digit recognition tasks?", "method": "Test different prompt engineering strategies on VLMs for tasks like MNIST and evaluate performance.", "expected_outcome": "Specific prompts will lead to improved performance on digit recognition tasks compared to generic prompts."}], "follow_up_work_ideas": ["Investigate the impact of integrating advanced data filtering techniques on the performance of VLMs in complex reasoning tasks.", "Explore the development of new learning objectives specifically tailored for improving VLMs\u2019 relational understanding.", "Adapt the UniBench framework to evaluate VLMs in novel domains such as augmented reality or virtual environments.", "Investigate the potential of hybrid models combining VLMs with task-specific smaller models to improve performance on simple tasks.", "Expand the UniBench framework to include additional benchmarks focused on real-world applications such as autonomous driving and medical diagnostics."]}}
{"id": "97769", "url": "https://nips.cc/virtual/2024/poster/97769", "title": "MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens", "authors": [], "abstract": "Abstract:Multimodal interleaved datasets featuring free-form interleaved sequences of images and text are crucial for training frontier large multimodal models (LMMs). Despite the rapid progression of open-source LMMs, there remains a pronounced scarcity of large-scale, open-source multimodal interleaved datasets.In response, we introduce MINT-1T, the most extensive and diverse open-source Multimodal INTerleaved dataset to date. MINT-1T comprises of one trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. As scaling multimodal interleaved datasets requires substantial engineering effort, sharing the data curation process and releasing the dataset greatly benefits the community. Our experiments show that LMMs trained on MINT-1T rival the performance of models trained on the previous leading dataset, OBELICS. We release our data at https://github.com/mlfoundations/MINT-1T.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/mlfoundations/MINT-1T"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": "1-2 NVIDIA A100 or equivalent", "memory": "32 GB RAM", "storage": "10 TB disk space for dataset"}, "time_requirements": "8 hours", "code_quality": "Well-documented", "difficulty": 3, "stars": 799}, "research_ideas": {"problem_statements": ["Despite the rapid progression of open-source multimodal models, there remains a scarcity of large-scale, open-source multimodal interleaved datasets.", "Existing open-source multimodal interleaved datasets are smaller and less diverse compared to their text-only counterparts, limiting the development of robust open-source multimodal models.", "There is an increasing gap in the multimodal training data between closed- and open-source models, which hampers the development of open-source models.", "Handling larger document sizes and preserving the original ordering of images and text in multimodal interleaved datasets presents a significant engineering challenge."], "main_takeaways": ["MINT-1T is the most extensive and diverse open-source multimodal interleaved dataset to date, comprising one trillion text tokens and 3.4 billion images.", "MINT-1T includes previously untapped data sources such as PDFs and ArXiv papers, providing more diverse data than prior datasets.", "Models trained on MINT-1T potentially surpass the performance of models trained on the previous leading dataset, OBELICS, offering a tenfold increase in scale.", "MINT-1T's data curation process and release address the scarcity of large-scale, open-source multimodal interleaved datasets, benefiting the research community."], "testable_hypotheses": [{"hypothesis": "LMMs trained on MINT-1T will outperform those trained on OBELICS in visual question answering tasks.", "method": "Train models using MINT-1T and OBELICS datasets and evaluate them on VQA benchmarks.", "expected_outcome": "Models trained on MINT-1T will achieve higher performance metrics on VQA tasks."}, {"hypothesis": "Including diverse data sources like PDFs and ArXiv in MINT-1T will improve model performance on science and technology domains.", "method": "Evaluate models trained on MINT-1T on domain-specific tasks and compare against models trained on datasets without PDF and ArXiv data.", "expected_outcome": "MINT-1T trained models will show higher accuracy in science and technology domains."}, {"hypothesis": "The use of MINT-1T's diverse dataset will result in better generalization across various multimodal reasoning benchmarks compared to HTML-only datasets.", "method": "Evaluate models trained on MINT-1T and HTML-only datasets on benchmarks like MMMU and Mantis-Eval.", "expected_outcome": "MINT-1T trained models will outperform HTML-only dataset models in generalization tasks."}, {"hypothesis": "Increasing the number of images per document in training data will enhance multi-image reasoning abilities of LMMs.", "method": "Train models with varying proportions of multi-image documents and evaluate on multi-image reasoning tasks.", "expected_outcome": "Models trained with more multi-image documents will perform better on multi-image reasoning tasks."}, {"hypothesis": "Using a combination of HTML, PDF, and ArXiv sources in MINT-1T will lead to a more balanced domain representation in trained models.", "method": "Analyze domain coverage of models trained on MINT-1T versus those trained on datasets lacking PDF and ArXiv sources.", "expected_outcome": "MINT-1T models will show more balanced domain coverage across evaluated tasks."}], "follow_up_work_ideas": ["Explore methods to further enhance the safety and quality of multimodal datasets by improving filtering processes for harmful content.", "Investigate the impact of using additional diverse data sources beyond PDFs and ArXiv to further increase dataset diversity.", "Develop efficient algorithms for determining reading order in complex PDF layouts to improve data parsing accuracy.", "Expand MINT-1T by incorporating additional years of data or new types of documents to assess long-term trends in model performance.", "Apply MINT-1T based models to new domains or real-world applications to test model robustness and adaptability."]}}
{"id": "97532", "url": "https://nips.cc/virtual/2024/poster/97532", "title": "TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series", "authors": [], "abstract": "Abstract:Time series data are essential in a wide range of machine learning (ML) applications. However, temporal data are often scarce or highly sensitive, limiting data sharing and the use of data-intensive ML methods. A possible solution to this problem is the generation of synthetic datasets that resemble real data. In this work, we introduce Time Series Generative Modeling (TSGM), an open-source framework for the generative modeling and evaluation of synthetic time series datasets. TSGM includes a broad repertoire of machine learning methods: generative models, probabilistic, simulation-based approaches, and augmentation techniques. The framework enables users to evaluate the quality of the produced data from different angles: similarity, downstream effectiveness, predictive consistency, diversity, fairness, and privacy. TSGM is extensible and user-friendly, which allows researchers to rapidly implement their own methods and compare them in a shareable environment. The framework has been tested on open datasets and in production and proved to be beneficial in both cases. https://github.com/AlexanderVNikitin/tsgm", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/AlexanderVNikitin/tsgm"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires both pip and conda for some dependencies)", "resource_requirements": {"GPUs": "1 x NVIDIA Tesla V100 or similar", "memory": "At least 16 GB RAM recommended", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "Well-documented code with clear installation and usage instructions", "difficulty": 3, "stars": 152}, "research_ideas": {"problem_statements": ["Time series data are often scarce or sensitive, limiting data sharing and the use of data-intensive ML methods.", "There is a need for synthetic time series data generators to extend available datasets with synthetic data.", "A lack of unified metrics and frameworks hinders the progress and applicability of synthetic time series generation methods to real-world problems."], "main_takeaways": ["TSGM is an open-source framework for generating and evaluating synthetic time series datasets.", "TSGM includes a variety of machine learning methods and metrics to assess the quality of synthetic datasets.", "The framework supports both simulation-based and data-driven approaches, providing a unified interface for time series generation.", "TSGM has been tested on open datasets and in production, proving beneficial in both cases."], "testable_hypotheses": [{"hypothesis": "Does the inclusion of synthetic data in training datasets improve model performance for time series prediction?", "method": "Train models with and without synthetic data using TSGM and compare their predictive performance.", "expected_outcome": "Including synthetic data is expected to improve model performance, particularly for small datasets."}, {"hypothesis": "Can TSGM-generated synthetic data maintain privacy while providing useful insights?", "method": "Evaluate the precision of membership inference attacks on synthetic data generated by TSGM.", "expected_outcome": "Synthetic data should resist membership inference attacks, indicating maintained privacy."}, {"hypothesis": "Will TSGM's data-driven methods outperform simulation-based methods in generating realistic time series data?", "method": "Compare the quality of data generated through data-driven and simulation-based methods using TSGM's metrics.", "expected_outcome": "Data-driven methods are expected to generate more realistic synthetic time series data."}, {"hypothesis": "Is there a significant difference in performance when using conditional versus non-conditional synthetic data generation?", "method": "Use TSGM to generate synthetic datasets with and without conditionings and compare their downstream task performances.", "expected_outcome": "Conditional synthetic data generation is expected to yield better performance in tasks requiring context awareness."}, {"hypothesis": "Does the use of augmentation techniques available in TSGM improve the quality of synthetic time series data?", "method": "Apply various augmentation techniques provided by TSGM to synthetic data and assess using TSGM's quality metrics.", "expected_outcome": "Augmentation is expected to improve the diversity and downstream effectiveness of the generated data."}], "follow_up_work_ideas": ["Explore the application of TSGM to generate synthetic time series data in new domains, such as finance or healthcare.", "Investigate the impact of different hyperparameter tuning strategies on the quality of synthetic data generated by TSGM.", "Develop new evaluation metrics tailored to specific use cases of synthetic time series data, such as anomaly detection.", "Extend TSGM to support irregularly sampled time series data and test its performance on such datasets.", "Implement and test robust watermarking algorithms to ensure the authenticity of synthetic data generated by TSGM."]}}
{"id": "97440", "url": "https://nips.cc/virtual/2024/poster/97440", "title": "Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification", "authors": [], "abstract": "Abstract:Graph Transformers (GTs) have recently emerged as popular alternatives to traditional message-passing Graph Neural Networks (GNNs), due to their theoretically superior expressiveness and impressive performance reported on standard node classification benchmarks, often significantly outperforming GNNs. In this paper, we conduct a thorough empirical analysis to reevaluate the performance of three classic GNN models (GCN, GAT, and GraphSAGE) against GTs. Our findings suggest that the previously reported superiority of GTs may have been overstated due to suboptimal hyperparameter configurations in GNNs. Remarkably, with slight hyperparameter tuning, these classic GNN models achieve state-of-the-art performance, matching or even exceeding that of recent GTs across 17 out of the 18 diverse datasets examined. Additionally, we conduct detailed ablation studies to investigate the influence of various GNN configurations\u2014such as normalization, dropout, residual connections, and network depth\u2014on node classification performance. Our study aims to promote a higher standard of empirical rigor in the field of graph machine learning, encouraging more accurate comparisons and evaluations of model capabilities. Our implementation is available at https://github.com/LUOyk1999/tunedGNN.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/LUOyk1999/tunedGNN"}, "reproduce_difficulty": {"environment_setup": "conda", "resource_requirements": {"GPUs": "None specified", "memory": "None specified", "API_calls": "None specified"}, "time_requirements": 1, "code_quality": "well-documented code", "difficulty": 2, "stars": 120}, "research_ideas": {"problem_statements": ["The paper seeks to reevaluate the performance of classic GNN models against Graph Transformers, suggesting that previous claims of GT superiority may be overstated due to suboptimal hyperparameter configurations in GNNs.", "The study aims to understand the influence of various GNN configurations such as normalization, dropout, residual connections, and network depth on node classification performance.", "The paper investigates whether the potential of message-passing GNNs for node classification has been underestimated."], "main_takeaways": ["Classic GNNs, with proper hyperparameter tuning, can achieve performance comparable to or better than state-of-the-art Graph Transformers across a wide range of datasets.", "Ablation studies reveal that normalization is crucial for large-scale graphs, dropout consistently improves performance, residual connections enhance performance on heterophilous graphs, and deeper networks are beneficial in such cases.", "The study challenges the perceived superiority of Graph Transformers over GNNs in node classification tasks."], "testable_hypotheses": [{"hypothesis": "Does normalization significantly improve the performance of GNNs on large-scale graphs?", "method": "Test GNN models with and without normalization layers on large-scale datasets and compare accuracy.", "expected_outcome": "Normalization will lead to a noticeable improvement in performance on large-scale graphs."}, {"hypothesis": "Does dropout consistently benefit GNN performance across different graph types?", "method": "Conduct experiments with dropout enabled and disabled across various datasets and compare results.", "expected_outcome": "Dropout is expected to consistently improve GNN performance across datasets."}, {"hypothesis": "Will classic GNNs outperform Graph Transformers on heterophilous graphs?", "method": "Run node classification tasks on heterophilous datasets using both GNNs and Graph Transformers, and evaluate accuracy.", "expected_outcome": "Classic GNNs will outperform Graph Transformers on heterophilous graphs."}, {"hypothesis": "Do residual connections significantly enhance GNN performance on heterophilous graphs?", "method": "Test GNN models with and without residual connections on heterophilous datasets and compare accuracy.", "expected_outcome": "Residual connections will lead to significant performance enhancements on heterophilous graphs."}, {"hypothesis": "Are deeper GNN architectures more beneficial for heterophilous graphs than homophilous ones?", "method": "Compare performance of shallow versus deeper GNN architectures on both graph types and analyze results.", "expected_outcome": "Deeper architectures will show more significant improvements on heterophilous graphs."}], "follow_up_work_ideas": ["Investigate the performance of classic GNNs on other graph machine learning tasks such as graph classification and link prediction.", "Explore the impact of different graph sampling techniques on the performance of GNNs and GTs.", "Study the applicability of the proposed hyperparameter configurations to other GNN variants and newer models.", "Examine the potential of combining GNNs and GTs in a hybrid model to leverage the strengths of both architectures.", "Investigate the role of different graph feature engineering techniques in enhancing GNN performance."]}}
{"id": "97779", "url": "https://nips.cc/virtual/2024/poster/97779", "title": "NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking", "authors": [], "abstract": "Abstract:Benchmarking vision-based driving policies is challenging. On one hand, open-loop evaluation with real data is easy, but these results do not reflect closed-loop performance. On the other, closed-loop evaluation is possible in simulation, but is hard to scale due to its significant computational demands. Further, the simulators available today exhibit a large domain gap to real data. This has resulted in an inability to draw clear conclusions from the rapidly growing body of research on end-to-end autonomous driving. In this paper, we present NAVSIM, a middle ground between these evaluation paradigms, where we use large datasets in combination with a non-reactive simulator to enable large-scale real-world benchmarking. Specifically, we gather simulation-based metrics, such as progress and time to collision, by unrolling bird's eye view abstractions of the test scenes for a short simulation horizon. Our simulation is non-reactive, i.e., the evaluated policy and environment do not influence each other. As we demonstrate empirically, this decoupling allows open-loop metric computation while being better aligned with closed-loop evaluations than traditional displacement errors. NAVSIM enabled a new competition held at CVPR 2024, where 143 teams submitted 463 entries, resulting in several new insights. On a large set of challenging scenarios, we observe that simple methods with moderate compute requirements such as TransFuser can match recent large-scale end-to-end driving architectures such as UniAD. Our modular framework can potentially be extended with new datasets, data curation strategies, and metrics, and will be continually maintained to host future challenges. Our code is available at https://github.com/autonomousvision/navsim.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/autonomousvision/navsim"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "8GB RAM", "other": "Python 3.x, various Python packages"}, "time_requirements": "2", "code_quality": "Well-documented", "difficulty": 3, "stars": 382}, "research_ideas": {"problem_statements": ["Benchmarking vision-based driving policies is challenging due to the difficulty of scaling closed-loop evaluations and the domain gap of available simulators.", "Existing benchmarks often misrepresent the accuracy of driving policies due to inadequate evaluation metrics.", "There is a need for more principled benchmarks to accurately evaluate and compare autonomous vehicle algorithms.", "Traditional evaluation metrics like average displacement error do not effectively capture the multi-modal nature of driving tasks."], "main_takeaways": ["NA VSIM offers a middle ground between open-loop and closed-loop evaluation by using a non-reactive simulation approach.", "The proposed framework allows the evaluation of driving policies with large-scale real-world datasets without the computationally expensive closed-loop simulations.", "NA VSIM provides a standardized evaluation setup that addresses inconsistencies in existing benchmarks.", "Empirical results show that simpler models like TransFuser can perform comparably to complex architectures like UniAD in challenging scenarios."], "testable_hypotheses": [{"hypothesis": "Does using a non-reactive simulation framework provide better alignment with closed-loop evaluation metrics compared to traditional open-loop metrics?", "method": "Compare the correlation between non-reactive simulation metrics and closed-loop metrics across various driving policies.", "expected_outcome": "Non-reactive simulation metrics will show better alignment with closed-loop evaluation metrics than traditional open-loop metrics."}, {"hypothesis": "Will extending the field of view (FOV) of camera inputs improve the performance of driving policies in NA VSIM?", "method": "Conduct experiments by varying the FOV in camera inputs and measure the performance using PDMS.", "expected_outcome": "Increasing the FOV will improve performance metrics like PDMS due to better environment perception."}, {"hypothesis": "Does incorporating LiDAR data significantly improve the performance of driving models in NA VSIM?", "method": "Evaluate the performance of driving models with and without LiDAR data and compare the results.", "expected_outcome": "Incorporating LiDAR data will improve PDMS scores compared to models using only camera data."}, {"hypothesis": "Will the use of auxiliary tasks in training improve the performance of sensor-based driving policies?", "method": "Compare the performance of models trained with and without auxiliary tasks such as BEV semantic segmentation or 3D detection.", "expected_outcome": "Models trained with auxiliary tasks will perform better due to richer feature learning."}, {"hypothesis": "Can simpler driving models achieve performance comparable to complex models on NA VSIM scenarios?", "method": "Benchmark simpler models like TransFuser against complex models like UniAD in terms of PDMS and subscore metrics.", "expected_outcome": "Simpler models can achieve similar performance to complex models under certain conditions."}], "follow_up_work_ideas": ["Explore the integration of reactive simulation to further close the gap between simulated and real-world driving evaluations.", "Investigate the impact of different data curation strategies on the robustness and generalization of driving policies.", "Extend NA VSIM to support additional datasets with diverse driving scenarios to enhance benchmark comprehensiveness.", "Develop new metrics that incorporate more traffic rules and efficiency considerations, such as fuel consumption or transit time.", "Assess the effectiveness of multi-objective evaluation approaches in capturing the trade-offs between different driving objectives."]}}
{"id": "93911", "url": "https://nips.cc/virtual/2024/poster/93911", "title": "Large Language Models Play StarCraft II:Benchmarks and A Chain of Summarization Approach", "authors": [], "abstract": "Abstract:With the continued advancement of Large Language Models (LLMs) Agents in reasoning, planning, and decision-making, benchmarks have become crucial in evaluating these skills. However, there is a notable gap in benchmarks for real-time strategic decision-making. StarCraft II (SC2), with its complex and dynamic nature, serves as an ideal setting for such evaluations. To this end, we have developed TextStarCraft II, a specialized environment for assessing LLMs in real-time strategic scenarios within SC2. Addressing the limitations of traditional Chain of Thought (CoT) methods, we introduce the Chain of Summarization (CoS) method, enhancing LLMs' capabilities in rapid and effective decision-making. Our key experiments included:1. LLM Evaluation: Tested 10 LLMs in TextStarCraft II, most of them defeating LV5 build-in AI, showcasing effective strategy skills.2. Commercial Model Knowledge: Evaluated four commercial  models on SC2 knowledge; GPT-4 ranked highest by Grandmaster-level experts.3. Human-AI Matches: Experimental results showed that fine-tuned LLMs performed on par with Gold-level players in real-time matches, demonstrating comparable strategic abilities.All code and data from thisstudy have been made pulicly available at https://github.com/histmeisah/Large-Language-Models-play-StarCraftII", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/histmeisah/Large-Language-Models-play-StarCraftII"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA V100", "amount": 1}, "CPUs": 1, "memory": "8GB", "other": "OpenAI API key required"}, "time_requirements": 7, "code_quality": "Well-documented code", "difficulty": 3, "stars": 249}, "research_ideas": {"problem_statements": ["There is a notable gap in benchmarks for real-time strategic decision-making using LLMs, particularly in dynamic gaming environments like StarCraft II.", "Existing SC2 environments lack language support for evaluating LLMs' capabilities in strategic decision-making and long-term planning.", "Traditional Chain of Thought (CoT) methods have limitations in rapid and effective decision-making in real-time strategy games."], "main_takeaways": ["TextStarCraft II was developed to evaluate LLMs in real-time strategic scenarios within SC2, using a text-based format converted from complex gameplay dynamics.", "The Chain of Summarization (CoS) method was introduced to enhance LLMs' abilities in rapid decision-making by incorporating single-frame and multi-frame summarization modules.", "LLMs demonstrated effective strategy skills in TextStarCraft II, with most models defeating a level 5 built-in AI, and performed on par with Gold-level human players in real-time matches.", "TextStarCraft II provides an open-source environment for further community development and interaction to foster LLM research in gaming contexts."], "testable_hypotheses": [{"hypothesis": "Does the CoS method improve LLM performance compared to traditional CoT methods in real-time strategy games?", "method": "Compare the win rates and decision-making speed of LLMs using CoS versus CoT in TextStarCraft II.", "expected_outcome": "The CoS method will improve LLM performance, showing higher win rates and faster decision-making."}, {"hypothesis": "Will fine-tuning LLMs using high APU performance games improve their strategic decision-making in TextStarCraft II?", "method": "Fine-tune LLMs using datasets of games with varying APU performance and evaluate their win rates against built-in AI.", "expected_outcome": "Fine-tuning on high APU performance games will lead to improved win rates compared to using the full dataset."}, {"hypothesis": "Can LLMs maintain comparable performance to human players of different skill levels in real-time matches?", "method": "Conduct matches between fine-tuned LLMs and human players of varying skill levels, recording win/loss ratios.", "expected_outcome": "LLMs will perform on par with Gold-level players and below, but struggle against higher-ranked players."}, {"hypothesis": "Does the use of complex prompts enhance LLM strategic capabilities in TextStarCraft II?", "method": "Evaluate LLM performance using simple versus complex prompts, analyzing win rates and strategic depth.", "expected_outcome": "Complex prompts will enhance LLM performance, resulting in higher win rates and more sophisticated strategies."}, {"hypothesis": "Is there a correlation between the quality of fine-tuning data and LLM performance in strategic gaming tasks?", "method": "Assess LLM performance using datasets of varying quality, as determined by game outcome and strategic metrics.", "expected_outcome": "Higher quality fine-tuning data, measured by wins and strategic effectiveness, will correlate with better LLM performance."}], "follow_up_work_ideas": ["Explore integration of visual data inputs to enhance LLM performance in strategic decision-making in gaming environments.", "Develop adaptive micro-strategic policies that go beyond rule-based approaches to improve LLMs' real-time decision-making.", "Evaluate the generalizability of CoS and TextStarCraft II to other real-time strategy games and dynamic environments.", "Investigate the impact of various LLM architectures and sizes on performance in strategic decision-making tasks.", "Study the potential for LLMs to collaborate with human players in team-based strategic games, analyzing synergies and communication methods."]}}
{"id": "93709", "url": "https://nips.cc/virtual/2024/poster/93709", "title": "UniTS: A Unified Multi-Task Time Series Model", "authors": [], "abstract": "Abstract:Although pre-trained transformers and reprogrammed text-based LLMs have shown strong performance on time series tasks, the best-performing architectures vary widely across tasks, with most models narrowly focused on specific areas, such as time series forecasting. Unifying predictive and generative time series tasks within a single model remains challenging. We introduce UniTS, a unified multi-task time series model that utilizes task tokenization to integrate predictive and generative tasks into a single framework. UniTS employs a modified transformer block to capture universal time series representations, enabling transferability from a heterogeneous, multi-domain pre-training dataset\u2014characterized by diverse dynamic patterns, sampling rates, and temporal scales\u2014to a wide range of downstream datasets with varied task specifications and data domains. Tested on 38 datasets across human activity sensors, healthcare, engineering, and finance, UniTS achieves superior performance compared to 12 forecasting models, 20 classification models, 18 anomaly detection models, and 16 imputation models, including adapted text-based LLMs. UniTS also demonstrates strong few-shot and prompt capabilities when applied to new domains and tasks. In single-task settings, UniTS outperforms competitive task-specialized time series models. Code and datasets are available at https://github.com/mims-harvard/UniTS.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/mims-harvard/UniTS"}, "reproduce_difficulty": {"environment_setup": "moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_calls": "none"}, "time_requirements": 2, "code_quality": "well-documented", "difficulty": 3, "stars": 507}, "research_ideas": {"problem_statements": ["Unifying predictive and generative time series tasks within a single model remains challenging.", "Existing time series models are often task-specific and lack adaptability to diverse domains and tasks.", "Pre-trained models for time series often require extensive fine-tuning or additional task-specific modules.", "There is a need for a versatile time series model that can handle data from diverse domains and support a wide range of tasks."], "main_takeaways": ["UNITS is a unified multi-task time series model that integrates predictive and generative tasks using task tokenization.", "UNITS employs a modified transformer architecture for capturing universal time series representations.", "The model demonstrates superior performance across 38 datasets from different domains, outperforming specialized models.", "UNITS supports few-shot and prompt-based learning, enabling rapid adaptation to new tasks and domains."], "testable_hypotheses": [{"hypothesis": "UNITS will outperform task-specific models across various time series tasks.", "method": "Conduct experiments comparing UNITS to task-specific models on forecasting, classification, anomaly detection, and imputation tasks.", "expected_outcome": "UNITS will show superior performance in the majority of tasks."}, {"hypothesis": "The use of task tokenization in UNITS improves its adaptability to new tasks compared to models without tokenization.", "method": "Evaluate the performance of UNITS with and without task tokenization on a set of diverse tasks.", "expected_outcome": "UNITS with task tokenization will adapt more effectively to new tasks."}, {"hypothesis": "Multi-task training in UNITS enhances its performance over single-task training.", "method": "Train UNITS in both multi-task and single-task settings and compare their performances.", "expected_outcome": "Multi-task training will result in better overall performance."}, {"hypothesis": "Prompt learning in UNITS allows for efficient adaptation to new datasets with limited data.", "method": "Test UNITS with prompt learning on a few-shot learning setup with new datasets.", "expected_outcome": "UNITS will achieve comparable performance to fully fine-tuned models."}, {"hypothesis": "UNITS's pre-training approach provides robust representations that reduce the need for extensive fine-tuning.", "method": "Compare the performance of UNITS with and without pre-training on various tasks.", "expected_outcome": "Pre-trained UNITS will require less fine-tuning and perform better."}], "follow_up_work_ideas": ["Explore the integration of other backbone architectures, like MLP-based blocks, into the UNITS framework.", "Investigate the performance of UNITS on additional time series datasets, including univariate datasets and more physiologic signals.", "Develop methods to enhance the zero-shot learning capabilities of UNITS for entirely unseen tasks.", "Study the impact of different types of pre-training data on UNITS's performance and adaptability.", "Examine potential improvements in UNITS's computational efficiency and scalability for larger datasets."]}}
{"id": "93507", "url": "https://nips.cc/virtual/2024/poster/93507", "title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning", "authors": [], "abstract": "Abstract:Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA. However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets. This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance. Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA. Building on these insights, we have developed HydraLoRA, a LoRA framework with an asymmetric structure that eliminates the need for domain expertise. Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases. Our anonymous codes are submitted with the paper and will be publicly available. Code is available: https://github.com/Clin0212/HydraLoRA.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Clin0212/HydraLoRA"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": [{"type": "NVIDIA", "amount": 1}], "memory": "16GB", "API_calls": "None"}, "time_requirements": 2, "code_quality": "well-documented", "difficulty": 3, "stars": 167}, "research_ideas": {"problem_statements": ["How can Parameter-Efficient Fine-Tuning (PEFT) techniques be improved to achieve better performance in complex domains compared to full fine-tuning?", "What is the optimal architecture that can deliver superior model performance while still capitalizing on the efficiency benefits of a reduced parameter footprint?", "How can domain or task interference be minimized to improve the effectiveness of PEFT methods?"], "main_takeaways": ["HydraLoRA is an improved LoRA framework with an asymmetric architecture that outperforms existing PEFT methods, including those that require domain knowledge.", "By using multiple smaller LoRA heads instead of a single one for the entire domain, interference among tasks can be minimized, thus improving performance.", "HydraLoRA employs a shared A matrix and multiple B matrices, which allows for better adaptation to distinct intrinsic components of a dataset, enhancing both parameter efficiency and model performance.", "HydraLoRA demonstrates that training multiple B matrices individually captures different intrinsic knowledge, reducing interference and enhancing effectiveness."], "testable_hypotheses": [{"hypothesis": "Deploying multiple smaller LoRA heads for specific tasks will outperform a single LoRA head for the entire domain dataset.", "method": "Conduct experiments comparing the performance of a single LoRA head versus multiple smaller LoRA heads on various datasets.", "expected_outcome": "Multiple smaller LoRA heads will show superior performance due to reduced task interference."}, {"hypothesis": "Sharing the A matrix across multiple tasks while maintaining distinct B matrices will enhance parameter efficiency and performance.", "method": "Implement an asymmetric LoRA structure with shared A matrix and multiple B matrices and compare its performance with symmetric structures.", "expected_outcome": "The asymmetric structure will demonstrate improved parameter efficiency and overall effectiveness."}, {"hypothesis": "The use of a trainable router in HydraLoRA will improve the composition of multiple B matrices, leading to better performance than fixed-weight configurations.", "method": "Compare the performance of HydraLoRA with and without a trainable router across diverse tasks.", "expected_outcome": "The HydraLoRA with a trainable router will outperform the fixed-weight configuration."}, {"hypothesis": "HydraLoRA will consume less energy and have reduced latency compared to traditional LoRA methods during training.", "method": "Measure and compare the energy consumption and latency of HydraLoRA and standard LoRA during fine-tuning on a standard dataset.", "expected_outcome": "HydraLoRA will show reduced energy consumption and latency."}, {"hypothesis": "The number of intrinsic components (clusters) in HydraLoRA will not significantly impact performance within a certain range.", "method": "Conduct sensitivity analysis by varying the number of clusters and evaluating performance on a benchmark dataset.", "expected_outcome": "Performance will remain stable across a reasonable range of cluster numbers."}], "follow_up_work_ideas": ["Extend the HydraLoRA framework to explore its application in other PEFT configurations such as prompt-tuning and adapters.", "Investigate the effectiveness of HydraLoRA during the pre-training phase to potentially enhance the model's initial adaptability.", "Explore the integration of robustness-enhancing measures, such as data sanitization and anomaly detection, to address potential challenges posed by heterogeneous task conditions.", "Apply the HydraLoRA approach to more diverse and challenging domains, such as adversarial environments, to test its robustness and adaptability."]}}
{"id": "96675", "url": "https://nips.cc/virtual/2024/poster/96675", "title": "Long-form factuality in large language models", "authors": [], "abstract": "Abstract:Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model\u2019s long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user\u2019s preferred response length (recall).Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotators\u2014on a set of\u223c16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators.  We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/google-deepmind/long-form-factuality"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_call": "OpenAI or Anthropic"}, "time_requirements": "2", "code_quality": "well-documented code", "difficulty": 3, "stars": 586}, "research_ideas": {"problem_statements": ["Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics.", "There is a need to benchmark a model's long-form factuality in open domains.", "Existing factuality datasets do not cover a broad range of topics or require long-form responses."], "main_takeaways": ["The paper proposes LongFact, a new prompt set for benchmarking long-form factuality in large language models.", "Search-Augmented Factuality Evaluator (SAFE) is introduced as a method for automatic evaluation of long-form factuality, leveraging a language model to fact-check using Google Search.", "SAFE outperforms crowdsourced human annotators, agreeing with them 72% of the time and being more than 20 times cheaper.", "Larger language models generally achieve better long-form factuality."], "testable_hypotheses": [{"hypothesis": "Larger language models achieve better long-form factuality than smaller models.", "method": "Benchmark various language models of different sizes on the LongFact dataset and evaluate using SAFE.", "expected_outcome": "Larger models will have higher F1@K scores."}, {"hypothesis": "SAFE can annotate long-form factuality more reliably than crowdsourced human annotators.", "method": "Compare SAFE's annotations with human annotations on a set of fact-checking tasks, then manually verify a subset of disagreements.", "expected_outcome": "SAFE will be correct more often in cases of disagreement."}, {"hypothesis": "Increasing the number of search queries in SAFE improves the accuracy of annotations.", "method": "Vary the number of search queries SAFE can issue and measure annotation accuracy against a ground truth.", "expected_outcome": "Annotation accuracy will improve up to a certain number of queries."}, {"hypothesis": "Responses with longer lengths have lower factual precision.", "method": "Instruct a language model to generate responses of varying lengths and evaluate the precision of each response.", "expected_outcome": "Longer responses will have lower precision due to increased factual errors."}, {"hypothesis": "RLHF (Reinforcement Learning from Human Feedback) improves long-form factuality in language models.", "method": "Compare models trained with and without RLHF on LongFact using SAFE.", "expected_outcome": "Models trained with RLHF will have higher F1@K scores."}], "follow_up_work_ideas": ["Explore the application of SAFE in other domains such as law and medicine, where factuality is crucial.", "Investigate methods to improve the diversity of search queries generated by SAFE to reduce redundancy.", "Examine the impact of different model pretraining techniques on long-form factuality.", "Develop methods to measure and reduce hallucination in long-form responses."]}}
{"id": "96041", "url": "https://nips.cc/virtual/2024/poster/96041", "title": "MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making", "authors": [], "abstract": "Abstract:Foundation models are becoming valuable tools in medicine. Yet despite their promise, the best way to leverage Large Language Models (LLMs) in complex medical tasks remains an open question. We introduce a novel multi-agent framework, named **M**edical **D**ecision-making **Agents** (**MDAgents**) that helps to address this gap by automatically assigning a collaboration structure to a team of LLMs. The assigned solo or group collaboration structure is tailored to the medical task at hand, a simple emulation inspired by the way real-world medical decision-making processes are adapted to tasks of different complexities. We evaluate our framework and baseline methods using state-of-the-art LLMs across a suite of real-world medical knowledge and clinical diagnosis benchmarks, including a comparison ofLLMs\u2019 medical complexity classification against human physicians. MDAgents achieved the **best performance in seven out of ten** benchmarks on tasks requiring an understanding of medical knowledge and multi-modal reasoning, showing a significant **improvement of up to 4.2\\%** ($p$ < 0.05) compared to previous methods' best performances. Ablation studies reveal that MDAgents effectively determines medical complexity to optimize for efficiency and accuracy across diverse medical tasks. Notably, the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy **improvement of 11.8\\%**. Our code can be found at https://github.com/mitmedialab/MDAgents.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/mitmedialab/MDAgents"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "1 x NVIDIA GPU with at least 8GB memory", "memory": "8GB RAM minimum", "API_call": "Requires OpenAI API key and GenAI API key"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 117}, "research_ideas": {"problem_statements": ["How can Large Language Models (LLMs) be effectively utilized in complex medical decision-making tasks?", "What is the best structure for collaboration among LLMs in medical applications to improve efficiency and accuracy?", "How can the complexity of medical tasks be classified to optimize collaboration structures among AI agents?", "How do LLMs compare with human physicians in terms of medical complexity classification and decision-making performance?"], "main_takeaways": ["MDAgents is an adaptive framework designed to improve medical decision-making by leveraging LLMs in a collaborative structure tailored to the complexity of the task.", "The framework dynamically assigns solo or group collaboration structures based on medical task complexity, inspired by real-world medical practices.", "MDAgents outperformed previous solo and group methods in 7 out of 10 medical benchmarks, demonstrating significant improvements in accuracy and efficiency.", "The combination of moderator review and external medical knowledge integration in group collaboration resulted in an average accuracy improvement of 11.8%."], "testable_hypotheses": [{"hypothesis": "Does dynamically assigning collaboration structures improve the performance of LLMs in medical decision-making compared to static structures?", "method": "Compare the performance of MDAgents with static collaboration structures across various medical benchmarks.", "expected_outcome": "MDAgents will outperform static structures, showing greater adaptability and accuracy in handling medical tasks."}, {"hypothesis": "Does the inclusion of external medical knowledge sources improve the decision-making accuracy of LLMs?", "method": "Implement a retrieval-augmented generation (RAG) approach and compare the accuracy with and without external knowledge sources.", "expected_outcome": "The inclusion of external medical knowledge sources will lead to improved accuracy in decision-making."}, {"hypothesis": "Can LLMs accurately classify the complexity of medical tasks compared to human experts?", "method": "Conduct an annotation study comparing LLMs' complexity classification with that of human physicians across a representative set of medical questions.", "expected_outcome": "LLMs will show moderate alignment with human experts but may reveal areas for improvement in complexity assessment."}, {"hypothesis": "Does increasing the number of collaborating agents in high-complexity tasks improve decision accuracy?", "method": "Vary the number of agents in high-complexity tasks and measure the impact on decision accuracy.", "expected_outcome": "There will be an optimal number of agents beyond which accuracy gains plateau or decrease."}, {"hypothesis": "Can moderator reviews enhance the consensus-building process among LLM agents?", "method": "Compare decision accuracy with and without moderator reviews in collaborative settings.", "expected_outcome": "Moderator reviews will enhance consensus-building, leading to more accurate final decisions."}], "follow_up_work_ideas": ["Explore the integration of medical-focused foundation models like Med-Gemini and Med-PaLM 2 into the MDAgents framework to enhance domain-specific reasoning.", "Develop interactive systems that incorporate patient and caregiver inputs for a more holistic and patient-centered diagnostic approach.", "Investigate the potential for LLMs to self-correct and learn from past diagnostic errors to improve accuracy and reduce hallucinations.", "Study the impact of different collaboration strategies on computational efficiency and scalability in real-world medical applications.", "Examine the ethical implications and safeguards necessary for deploying LLM-assisted medical decision-making systems in clinical settings."]}}
{"id": "95721", "url": "https://nips.cc/virtual/2024/poster/95721", "title": "Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention", "authors": [], "abstract": "Abstract:Conditional diffusion models have shown remarkable success in visual content generation, producing high-quality samples across various domains, largely due to classifier-free guidance (CFG). Recent attempts to extend guidance to unconditional models have relied on heuristic techniques, resulting in suboptimal generation quality and unintended effects. In this work, we propose Smoothed Energy Guidance (SEG), a novel training- and condition-free approach that leverages the energy-based perspective of the self-attention mechanism to enhance image generation. By defining the energy of self-attention, we introduce a method to reduce the curvature of the energy landscape of attention and use the output as the unconditional prediction. Practically, we control the curvature of the energy landscape by adjusting the Gaussian kernel parameter while keeping the guidance scale parameter fixed. Additionally, we present a query blurring method that is equivalent to blurring the entire attention weights without incurring quadratic complexity in the number of tokens. In our experiments, SEG achieves a Pareto improvement in both quality and the reduction of side effects. The code is available at https://github.com/SusungHong/SEG-SDXL.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/SusungHong/SEG-SDXL"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires installing specific versions of torch and diffusers)", "resource_requirements": "1 GPU (CUDA compatible, ideally NVIDIA), 8GB RAM minimum, torch 2.0.1, diffusers 0.27.2", "time_requirements": "1 hour", "code_quality": "Well-documented code", "difficulty": 3, "stars": 117}, "research_ideas": {"problem_statements": ["Conditional diffusion models have shown remarkable success in visual content generation, but unconditional models face challenges in achieving the same quality due to reliance on heuristic techniques.", "Existing unconditional guidance methods such as Self-Attention Guidance (SAG) and Perturbed Attention Guidance (PAG) have unintended effects like smoothed-out details and color shifts.", "The mathematical underpinnings of unconditional guidance approaches are not well elucidated, leading to suboptimal generation quality.", "There is a need for a training- and condition-free approach to enhance image generation in unconditional models."], "main_takeaways": ["Smoothed Energy Guidance (SEG) is a novel approach that leverages the energy-based perspective of the self-attention mechanism to enhance image generation.", "SEG reduces the curvature of the energy landscape of attention by adjusting the Gaussian kernel parameter, improving image quality without inducing side effects.", "SEG achieves a Pareto improvement in both quality and reduction of side effects compared to previous methods like SAG and PAG.", "SEG can be used for both unconditional and conditional image generation, supporting diffusion models in generating high-quality images without training."], "testable_hypotheses": [{"hypothesis": "Does reducing the curvature of the energy landscape in self-attention improve image generation quality?", "method": "Implement SEG and compare image quality metrics (e.g., FID, LPIPS) with and without curvature reduction.", "expected_outcome": "SEG will show improved image quality metrics compared to models without curvature reduction."}, {"hypothesis": "Does SEG reduce unintended effects like color shifts and structural changes in generated images?", "method": "Qualitatively compare images generated with SEG and previous methods (SAG, PAG) to assess side effects.", "expected_outcome": "SEG will produce images with fewer unintended effects compared to SAG and PAG."}, {"hypothesis": "Can SEG achieve high-quality image generation without relying on external conditions?", "method": "Generate images using SEG in unconditional mode and evaluate using standard quality metrics.", "expected_outcome": "SEG will generate high-quality images comparable to conditional diffusion models."}, {"hypothesis": "Does adjusting the Gaussian kernel parameter significantly impact the quality of generated images?", "method": "Experiment with different Gaussian kernel parameters and observe changes in image quality metrics.", "expected_outcome": "Larger Gaussian kernel parameters will lead to improved image quality."}, {"hypothesis": "Is SEG effective in enhancing text-conditional image generation quality?", "method": "Apply SEG to text-conditional diffusion models and compare image quality metrics with baseline methods.", "expected_outcome": "SEG will enhance text-conditional generation quality, achieving better FID and CLIP scores."}], "follow_up_work_ideas": ["Investigate the application of SEG to video and 3D generation tasks to assess its effectiveness in other domains.", "Explore the integration of SEG with other diffusion model architectures to further enhance image generation quality.", "Analyze the impact of different attention mechanisms on SEG's performance to identify potential improvements.", "Develop adaptive techniques for dynamically adjusting Gaussian kernel parameters based on input conditions.", "Examine the potential of SEG in addressing fairness and bias issues in generative models by controlling attention dynamics."]}}
{"id": "96223", "url": "https://nips.cc/virtual/2024/poster/96223", "title": "MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views", "authors": [], "abstract": "Abstract:We introduce MVSplat360, a feed-forward approach for 360\u00b0 novel view synthesis (NVS) of diverse real-world scenes, using only sparse observations. This setting is inherently ill-posed due to minimal overlap among input views and insufficient visual information provided, making it challenging for conventional methods to achieve high-quality results. Our MVSplat360 addresses this by effectively combining geometry-aware 3D reconstruction with temporally consistent video generation. Specifically, it refactors a feed-forward 3D Gaussian Splatting (3DGS) model to render features directly into the latent space of a pre-trained Stable Video Diffusion (SVD) model, where these features then act as pose and visual cues to guide the denoising process and produce photorealistic 3D-consistent views. Our model is end-to-end trainable and supports rendering arbitrary views with as few as 5 sparse input views. To evaluate MVSplat360's performance, we introduce a new benchmark using the challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual quality compared to state-of-the-art methods on wide-sweeping or even 360\u00b0 NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the effectiveness of our model. Readers are highly recommended to view the video results atdonydchen.github.io/mvsplat360.", "pdf_url": "", "supplementary_url": "", "code_url": "https://donydchen.github.io/mvsplat360", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/donydchen/mvsplat360"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 or more GPUs with at least 22G VRAM for evaluation and 80G for training", "memory": "22G or 80G VRAM depending on the task"}, "time_requirements": "2-4 hours for training depending on the dataset size", "code_quality": "well-documented code", "difficulty": 3, "stars": 231}, "research_ideas": {"problem_statements": ["The paper addresses the challenge of 360\u00b0 novel view synthesis (NVS) from extremely sparse views in diverse real-world scenes.", "Existing methods require dense views and per-scene optimization, which is impractical for casual users.", "There is a need to ensemble visible information under minimal overlap and generate missing details in novel views."], "main_takeaways": ["MVSplat360 is a feed-forward approach capable of synthesizing high-quality 360\u00b0 views from as few as 5 sparse input images.", "The method combines geometry-aware 3D reconstruction with a pre-trained Stable Video Diffusion model for temporally consistent video generation.", "MVSplat360 achieves superior visual quality on benchmarks like DL3DV-10K and RealEstate10K compared to existing state-of-the-art methods."], "testable_hypotheses": [{"hypothesis": "Does the use of a pre-trained Stable Video Diffusion model improve the photorealism of synthesized views compared to traditional methods?", "method": "Compare visual quality metrics like PSNR, SSIM, and FID for outputs with and without the diffusion model on benchmark datasets.", "expected_outcome": "The use of the diffusion model will significantly improve photorealism and consistency in synthesized views."}, {"hypothesis": "Can MVSplat360 maintain high-quality synthesis with fewer than 5 input views?", "method": "Evaluate the model's performance on DL3DV-10K with varying numbers of input views (e.g., 3, 4, 5) and compare the results.", "expected_outcome": "Performance will degrade with fewer views, but MVSplat360 will still outperform traditional methods with 3 or more views."}, {"hypothesis": "Does the Gaussian feature rendering improve the integration of geometric and visual cues for the diffusion model?", "method": "Conduct ablation studies by disabling Gaussian feature rendering and comparing visual quality metrics.", "expected_outcome": "Gaussian feature rendering significantly enhances the model's ability to condition the diffusion process, resulting in better visual quality."}, {"hypothesis": "Is the multi-frame appearance refinement module essential for achieving temporal consistency in synthesized views?", "method": "Compare temporal consistency and visual quality of MVSplat360 with a version lacking the multi-frame refinement module.", "expected_outcome": "The multi-frame refinement module will be crucial for maintaining temporal consistency and high visual quality."}, {"hypothesis": "Can the integration of cross-view attention and local group cost volume construction improve model robustness in wide displacement settings?", "method": "Evaluate the model's performance on scenes with varying camera displacements and compare with a version without these integrations.", "expected_outcome": "Cross-view attention and local group cost volume construction will enhance robustness and visual quality in wide displacement scenarios."}], "follow_up_work_ideas": ["Explore the application of MVSplat360 in other domains like medical imaging or robotics where sparse data acquisition is common.", "Investigate ways to reduce inference time for the diffusion process to make the approach more practical for real-time applications.", "Enhance the model's ability to handle dynamic scenes with moving objects by incorporating motion estimation techniques.", "Explore the integration of additional sensory data, such as depth information, to further improve synthesis quality and accuracy.", "Develop methods to reduce color oversaturation and hallucinations in the diffusion model's outputs to improve realism."]}}
{"id": "96085", "url": "https://nips.cc/virtual/2024/poster/96085", "title": "Are Language Models Actually Useful for Time Series Forecasting?", "authors": [], "abstract": "Abstract:Large language models (LLMs) are being applied to time series forecasting. But are language models actually useful for time series? In a series of ablation studies on three recent and popular LLM-based time series forecasting methods, we find that removing the LLM component or replacing it with a basic attention layer does not degrade forecasting performance---in most cases, the results even improve! We also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings. Additionally, we explore time series encoders and find that patching and attention structures perform similarly to LLM-based forecasters. All resources needed to reproduce our work are available: https://github.com/BennyTMT/LLMsForTimeSeries.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/BennyTMT/LLMsForTimeSeries"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires setup of different repositories and scripts for environment)", "resource_requirements": {"GPUs": "N/A", "memory": "N/A", "API_call": "N/A"}, "time_requirements": "2", "code_quality": "Well-documented code", "difficulty": 3, "stars": 102}, "research_ideas": {"problem_statements": ["Are large language models (LLMs) actually useful for time series forecasting?", "Do LLM-based time series forecasters perform worse or the same as simpler models?", "Do pretrained LLMs improve time series forecasting performance?", "Can LLMs effectively model sequential dependencies in time series?"], "main_takeaways": ["Removing the LLM component or replacing it with a basic attention layer does not degrade forecasting performance; in most cases, it improves.", "Pretrained LLMs do not outperform models trained from scratch in time series forecasting.", "LLMs do not assist in few-shot settings for time series forecasting.", "A simple linear model with patching and attention can achieve similar performance to LLMs in time series forecasting.", "LLM-based methods significantly increase computational costs without improving performance."], "testable_hypotheses": [{"hypothesis": "Does removing the LLM component improve time series forecasting performance?", "method": "Conduct ablation studies on popular LLM-based time series forecasting methods by removing the LLM component and comparing performance.", "expected_outcome": "Removing the LLM component will not degrade and may improve forecasting performance."}, {"hypothesis": "Does replacing LLMs with basic attention layers maintain forecasting accuracy?", "method": "Replace the LLM component in forecasting models with a simple attention layer and compare the results.", "expected_outcome": "Replacing LLMs with attention layers will maintain or improve forecasting accuracy."}, {"hypothesis": "Do pretrained LLMs enhance forecasting performance over models trained from scratch?", "method": "Compare forecasting performance of models using pretrained LLMs versus models trained from scratch.", "expected_outcome": "Pretrained LLMs will not outperform models trained from scratch."}, {"hypothesis": "Do LLMs help in few-shot learning scenarios for time series forecasting?", "method": "Evaluate forecasting performance using only 10% of the data to test few-shot capabilities of LLM-based models.", "expected_outcome": "LLMs will not provide a significant advantage in few-shot learning scenarios."}, {"hypothesis": "Can a simple model with patching and attention achieve performance comparable to LLMs?", "method": "Develop a simple model using patching and attention as an encoder and compare its performance to LLM-based models.", "expected_outcome": "The simple model will achieve performance similar to or better than LLM-based models."}], "follow_up_work_ideas": ["Explore the effectiveness of LLMs in other time series tasks such as classification and anomaly detection.", "Investigate the potential of LLMs in multimodal applications combining time series with text data.", "Apply the proposed simple model with patching and attention to other domains and datasets to test its generalizability.", "Develop computationally efficient models for time series forecasting that leverage minimal components of LLMs.", "Explore the potential of combining LLMs with other neural network architectures to harness their strengths in time series forecasting."]}}
{"id": "96771", "url": "https://nips.cc/virtual/2024/poster/96771", "title": "X-Ray: A Sequential 3D Representation For Generation", "authors": [], "abstract": "Abstract:We introduce X-Ray, a novel 3D sequential representation inspired by the penetrability of x-ray scans. X-Ray transforms a 3D object into a series of surface frames at different layers, making it suitable for generating 3D models from images. Our method utilizes ray casting from the camera center to capture geometric and textured details, including depth, normal, and color, across all intersected surfaces. This process efficiently condenses the whole 3D object into a multi-frame video format, motivating the utilize of a network architecture similar to those in video diffusion models. This design ensures an efficient 3D representation by focusing solely on surface information. Also, we propose a two-stage pipeline to generate 3D objects from X-Ray Diffusion Model and Upsampler. We demonstrate the practicality and adaptability of our X-Ray representation by synthesizing the complete visible and hidden surfaces of a 3D object from a single input image. Experimental results reveal the state-of-the-art superiority of our representation in enhancing the accuracy of 3D generation, paving the way for new 3D representation research and practical applications. Our project page is in \\url{https://tau-yihouxiang.github.io/projects/X-Ray/X-Ray.html}.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/tau-yihouxiang/X-Ray"}, "reproduce_difficulty": {"environment_setup": "Medium (requires conda and pip for installation)", "resource_requirements": {"GPUs": {"type": "NVIDIA GPU", "amount": 1}, "memory": "At least 8 GB RAM", "API_call": "Hugging Face API for dataset access"}, "time_requirements": 4, "code_quality": "Well-documented code with clear instructions", "difficulty": 3, "stars": 108}, "research_ideas": {"problem_statements": ["Existing rendering-based 3D generative methods cannot completely generate objects that include both visible and hidden surfaces, which results in incomplete or unrealistic reconstructions.", "Traditional 3D representations like meshes, point clouds, and voxels face challenges in satisfying the requirements of generality, accuracy, and efficiency for 3D synthesis.", "The need for a 3D representation model that can facilitate the generation of 3D objects from single images by accurately predicting characteristics that are not immediately visible."], "main_takeaways": ["The paper introduces X-Ray, a novel 3D sequential representation inspired by x-ray imaging, capable of capturing both visible and hidden surfaces of 3D objects.", "X-Ray efficiently condenses a 3D object into a multi-frame video format using ray casting, which is compatible with video diffusion models for generating high-quality 3D objects.", "X-Ray outperforms existing rendering-based methods by providing more complete and realistic reconstructions of 3D objects from single images.", "The method sets a new benchmark for image-to-3D modeling, achieving state-of-the-art performance in terms of 3D generation quality."], "testable_hypotheses": [{"hypothesis": "Does the X-Ray representation provide more complete reconstructions than traditional rendering-based methods?", "method": "Compare the completeness of 3D object reconstructions using X-Ray against traditional methods on a standardized dataset.", "expected_outcome": "X-Ray will yield more complete reconstructions due to its ability to capture hidden surfaces."}, {"hypothesis": "Can the X-Ray representation improve the quality of 3D object generation in terms of accuracy and efficiency?", "method": "Evaluate the accuracy and efficiency of 3D object generation using X-Ray and compare it with traditional representations on benchmark datasets.", "expected_outcome": "X-Ray will show improved accuracy and efficiency over traditional methods."}, {"hypothesis": "Will increasing the number of layers in the X-Ray representation further reduce intrinsic errors in encoding-decoding processes?", "method": "Conduct experiments with varying numbers of layers in the X-Ray representation and measure the intrinsic error using Chamfer Distance.", "expected_outcome": "Increasing the number of layers will reduce intrinsic errors until a point of convergence."}, {"hypothesis": "Does the inclusion of the 'Hit' attribute in the X-Ray representation contribute to more accurate 3D generation?", "method": "Conduct an ablation study to compare 3D generation accuracy with and without the 'Hit' attribute in the X-Ray representation.", "expected_outcome": "The inclusion of the 'Hit' attribute will improve the accuracy of 3D generation."}, {"hypothesis": "Can the X-Ray representation be effectively used for generating 3D objects from text prompts via intermediary image generation?", "method": "Use diffusion models to generate images from text prompts, segment objects, and then use X-Ray to generate 3D objects, evaluating the results.", "expected_outcome": "X-Ray will effectively generate accurate 3D objects from text prompts via intermediary image generation."}], "follow_up_work_ideas": ["Explore advanced network architectures, such as Large Language Models, to better handle the complexities of X-Ray data, including layer sparsity and sequential format.", "Investigate the application of X-Ray representation in other fields such as medical imaging or virtual reality to broaden its utility.", "Develop more efficient generative models that can handle more complex objects with a large number of layers in the X-Ray representation.", "Refine the X-Ray representation to improve the smoothness and continuity of generated meshes.", "Apply the X-Ray representation to other 3D generative tasks, such as multi-view generation or unsupervised learning for 3D modeling."]}}
{"id": "95654", "url": "https://nips.cc/virtual/2024/poster/95654", "title": "PointMamba: A Simple State Space Model for Point Cloud Analysis", "authors": [], "abstract": "Abstract:Transformers have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability. However, the attention mechanism has quadratic complexity, making the design of a linear complexity method with global modeling appealing. In this paper, we propose PointMamba, transferring the success of Mamba, a recent representative state space model (SSM), from NLP to point cloud analysis tasks. Unlike traditional Transformers, PointMamba employs a linear complexity algorithm, presenting global modeling capacity while significantly reducing computational costs. Specifically, our method leverages space-filling curves for effective point tokenization and adopts an extremely simple, non-hierarchical Mamba encoder as the backbone. Comprehensive evaluations demonstrate that PointMamba achieves superior performance across multiple datasets while significantly reducing GPU memory usage and FLOPs. This work underscores the potential of SSMs in 3D vision-related tasks and presents a simple yet effective Mamba-based baseline for future research. The code is available at https://github.com/LMD0311/PointMamba.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/LMD0311/PointMamba"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires specific datasets and environment configuration, potentially using pip or conda)", "resource_requirements": {"GPUs": [{"type": "NVIDIA", "amount": 1}], "memory": "8GB or more recommended"}, "time_requirements": 2, "code_quality": "Well-documented", "difficulty": 3, "stars": 394}, "research_ideas": {"problem_statements": ["How can we design a method for point cloud analysis with linear complexity while retaining the benefits of global modeling?", "Can the State Space Models (SSMs), specifically Mamba, be a viable alternative to Transformers for point cloud analysis tasks?", "What are the limitations of traditional Mamba models when applied directly to point cloud data?"], "main_takeaways": ["PointMamba is a simple state space model that achieves linear complexity for point cloud analysis tasks by utilizing space-filling curves for point tokenization.", "PointMamba significantly reduces GPU memory usage and FLOPs compared to Transformer-based methods while maintaining or improving performance.", "The method leverages a plain, non-hierarchical Mamba encoder, emphasizing structural simplicity and efficiency.", "PointMamba's performance on various datasets demonstrates the potential of SSMs in 3D vision-related tasks."], "testable_hypotheses": [{"hypothesis": "PointMamba will outperform existing Transformer-based methods in point cloud analysis tasks in terms of computational efficiency.", "method": "Benchmark PointMamba against Transformer-based methods using metrics such as inference speed, GPU memory usage, and FLOPs across multiple datasets.", "expected_outcome": "PointMamba will demonstrate lower GPU memory usage and FLOPs, with faster inference speed than Transformer-based methods."}, {"hypothesis": "The use of space-filling curves in PointMamba improves the preservation of spatial locality in point cloud data.", "method": "Compare the performance of PointMamba with and without space-filling curve tokenization on tasks like object classification and segmentation.", "expected_outcome": "PointMamba with space-filling curves will show improved performance over the version without them."}, {"hypothesis": "PointMamba's linear complexity provides scalability advantages for large-scale point cloud datasets.", "method": "Evaluate PointMamba's performance and resource usage on increasingly large point cloud datasets.", "expected_outcome": "PointMamba will maintain efficient resource usage and performance even as dataset size increases."}, {"hypothesis": "Different space-filling curves will affect the performance of PointMamba differently.", "method": "Experiment with different types of space-filling curves (e.g., Hilbert, Z-order) and measure their impact on PointMamba's accuracy and efficiency.", "expected_outcome": "Certain space-filling curves, like Hilbert, will result in better performance due to superior locality-preserving properties."}, {"hypothesis": "The order indicator in PointMamba is critical for maintaining the distinct spatial characteristics of point tokens.", "method": "Test PointMamba with and without the order indicator and compare the results on various datasets.", "expected_outcome": "PointMamba with the order indicator will perform better as it helps maintain spatial integrity."}], "follow_up_work_ideas": ["Explore the application of PointMamba to other 3D vision tasks such as object detection or scene reconstruction.", "Investigate the integration of multi-modal data (e.g., combining point clouds with 2D images) within the PointMamba framework.", "Develop a unified Mamba-based model for various 3D vision tasks to evaluate its generalization across different domains.", "Examine the impact of different pre-training strategies on PointMamba's performance.", "Further optimize the Mamba architecture to enhance its performance and efficiency for real-time applications."]}}
{"id": "94905", "url": "https://nips.cc/virtual/2024/poster/94905", "title": "Autoregressive Image Generation without Vector Quantization", "authors": [], "abstract": "Abstract:Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available athttps://github.com/LTH14/mar.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/LTH14/mar", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/LTH14/mar"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": "8-32 H100 GPUs", "memory": "Varies based on model size, up to 943M params", "api_calls": "None specified"}, "time_requirements": "1-2 days", "code_quality": "Well-documented code", "difficulty": 3, "stars": 1309}, "research_ideas": {"problem_statements": ["Is it necessary for autoregressive models to be coupled with vector-quantized representations?", "Can autoregressive models be effectively used in continuous-valued domains without vector quantization?", "How can we model per-token probability distributions in continuous-valued spaces for autoregressive image generation?"], "main_takeaways": ["The paper proposes using a diffusion process to model per-token probability distributions in continuous-valued spaces, eliminating the need for vector-quantized tokenizers.", "Diffusion Loss is introduced as an alternative to categorical cross-entropy loss, suitable for continuous-valued autoregressive models.", "By removing vector quantization, the proposed method achieves strong image generation results while benefiting from the speed of sequence modeling.", "The method demonstrates flexibility by working with various types of tokenizers and supports autoregressive and masked autoregressive models.", "The proposed method achieves competitive results on ImageNet benchmarks, showing potential for scaling and application to other domains."], "testable_hypotheses": [{"hypothesis": "Does using Diffusion Loss improve image generation quality compared to cross-entropy loss in autoregressive models?", "method": "Compare the performance (e.g., FID score) of autoregressive models using Diffusion Loss and cross-entropy loss on a standard dataset like ImageNet.", "expected_outcome": "Diffusion Loss will show improved FID scores over cross-entropy loss."}, {"hypothesis": "Can the proposed method achieve faster image generation than vector-quantized autoregressive models?", "method": "Measure the time taken for image generation using the proposed continuous-valued method and compare it with a vector-quantized approach.", "expected_outcome": "The continuous-valued method will be faster due to the elimination of vector quantization overhead."}, {"hypothesis": "Does the temperature parameter in Diffusion Loss significantly influence the diversity and quality of generated images?", "method": "Experiment with different temperature settings in the diffusion sampler and evaluate the impact on image diversity and fidelity.", "expected_outcome": "Temperature adjustments will significantly affect the diversity and quality of the generated images."}, {"hypothesis": "Can the proposed method be effectively applied to other continuous-valued domains beyond image generation?", "method": "Apply the method to a different domain, such as audio or video generation, and evaluate its performance.", "expected_outcome": "The method will successfully generalize to other continuous-valued domains, maintaining or improving performance metrics."}, {"hypothesis": "Does the use of bidirectional attention in masked autoregressive models improve image generation quality compared to causal attention?", "method": "Compare the performance of masked autoregressive models using bidirectional and causal attention on a dataset like ImageNet.", "expected_outcome": "Bidirectional attention will enhance image generation quality due to better token communication."}], "follow_up_work_ideas": ["Investigate the application of Diffusion Loss in other continuous-valued domains such as audio or video generation.", "Explore ways to further optimize the diffusion process for even faster image generation without compromising quality.", "Develop advanced tokenizers that are specifically tailored for continuous-valued autoregressive models to further improve generation quality.", "Examine the robustness and scalability of the proposed method on larger and more diverse datasets.", "Investigate the integration of classifier-free guidance with Diffusion Loss for enhanced control over the generation process."]}}
{"id": "96925", "url": "https://nips.cc/virtual/2024/poster/96925", "title": "The Road Less Scheduled", "authors": [], "abstract": "Abstract:Existing learning rate schedules that do not require specification of the optimization stopping step $T$ are greatly out-performed by learning rate schedules that depend on $T$. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available at https://github.com/facebookresearch/schedule_free. Schedule-Free AdamW is the core algorithm behind our winning entry to the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/facebookresearch/schedule_free"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 NVIDIA GPU (e.g. GTX 1080 or equivalent)", "memory": "8 GB RAM", "API_calls": "none"}, "time_requirements": "2", "code_quality": "well-documented code", "difficulty": 2, "stars": 2105}, "research_ideas": {"problem_statements": ["Existing learning rate schedules that do not require specification of the optimization stopping step T are greatly out-performed by learning rate schedules that depend on T.", "There is a significant gap between theoretical convergence guarantees and practical empirical performance in optimization, particularly for methods like Polyak-Ruppert averaging.", "Learning rate schedules have a critical disadvantage as they require setting the optimization stopping time T in advance."], "main_takeaways": ["The proposed Schedule-Free approach achieves state-of-the-art performance without needing a predefined stopping time T.", "Schedule-Free methods introduce no additional hyper-parameters over standard optimizers with momentum.", "A new theory unifying scheduling and iterate averaging is developed, offering worst-case convergence rate guarantees.", "Schedule-Free AdamW won the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track, validating its effectiveness.", "The method allows for larger learning rates than traditional approaches, potentially leading to faster convergence."], "testable_hypotheses": [{"hypothesis": "Schedule-Free methods perform comparably or better than traditional cosine learning rate schedules across a variety of machine learning tasks.", "method": "Conduct experiments on diverse datasets using Schedule-Free methods and compare their performance against cosine learning rate schedules.", "expected_outcome": "Schedule-Free methods will match or exceed the performance of cosine learning rate schedules."}, {"hypothesis": "The alternative form of momentum used in Schedule-Free methods is worst-case optimal in convex Lipschitz settings.", "method": "Theoretically analyze the convergence of Schedule-Free methods with varying momentum parameters in convex Lipschitz settings.", "expected_outcome": "The momentum form will show optimal convergence rates for any choice of momentum parameter."}, {"hypothesis": "Using large learning rates with Schedule-Free methods maintains optimal convergence rates.", "method": "Experiment with different learning rates on benchmark tasks to observe the stability and convergence of Schedule-Free methods.", "expected_outcome": "Schedule-Free methods will remain stable and converge optimally even with large learning rates."}, {"hypothesis": "Schedule-Free methods can be generalized to arbitrary online optimization algorithms.", "method": "Apply the Schedule-Free approach to different online optimization algorithms and evaluate their performance.", "expected_outcome": "Schedule-Free approach will unify various online-to-batch conversions and show optimal performance."}, {"hypothesis": "Incorporating the momentum parameter \u03b2 allows for convergence despite using larger learning rates on quadratic problems.", "method": "Test Schedule-Free methods with varying \u03b2 values and large learning rates on quadratic minimization problems.", "expected_outcome": "Schedule-Free methods will converge successfully with larger learning rates due to the momentum parameter."}], "follow_up_work_ideas": ["Investigate the application of Schedule-Free methods to other types of optimizers beyond SGD and Adam, such as RMSProp or AdaGrad.", "Explore the potential of combining Schedule-Free methods with other state-of-the-art adaptive learning rate techniques.", "Analyze the impact of Schedule-Free methods in non-convex optimization settings, particularly on large-scale neural network training.", "Develop theoretical insights into the conditions under which large learning rates can be effectively utilized in stochastic problems.", "Examine the effectiveness of Schedule-Free methods in distributed or parallel training environments to assess scalability."]}}
{"id": "95715", "url": "https://nips.cc/virtual/2024/poster/95715", "title": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization", "authors": [], "abstract": "Abstract:Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks. Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM. However, current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs. To address this, we propose accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM. Specifically, we quantize each weight matrix into binary matrices paired with group-wise scaling factors. The associated multiplications are reparameterized into (1) shifts between activations and scaling factors and (2) queries and adds according to the binary matrices. To reduce accuracy loss, we present a multi-objective optimization method to minimize both weight and output activation reparameterization errors. Additionally, based on varying sensitivity across layers to reparameterization, we develop an automated bit allocation strategy to further reduce memory usage and latency. Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity reductions of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3- and 2-bit precision, respectively, and more than 80% memory and energy reductions over the original LLMs. Codes and models are available at https://github.com/GATECH-EIC/ShiftAddLLM.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/GATECH-EIC/ShiftAddLLM"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires conda and setup of environment.yml)", "resource_requirements": {"GPUs": "1 or more GPUs (exact type not specified)", "memory": "Not specified", "API_call": "Hugging Face model inference"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 103}, "research_ideas": {"problem_statements": ["Large language models (LLMs) face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, which result in high memory demands and latency bottlenecks.", "Current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs.", "How can we effectively reparameterize pretrained LLMs with shifts and adds in a post-training manner?", "How can we mitigate the accuracy drop from shift-and-add reparameterization?", "How can we handle varying sensitivities to reparameterization across different layers and blocks in LLMs?"], "main_takeaways": ["ShiftAddLLM is proposed to accelerate pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models.", "The method quantizes each weight matrix into binary matrices paired with group-wise scaling factors, replacing multiplications with shifts and adds.", "A multi-objective optimization method is introduced to minimize both weight and output activation reparameterization errors, achieving lower perplexity and better task accuracy.", "An automated bit allocation strategy is developed to reduce memory usage and latency by allocating bits based on layer sensitivity to reparameterization.", "ShiftAddLLM achieves significant memory and energy reductions over the original LLMs, while also reducing latency."], "testable_hypotheses": [{"hypothesis": "Does replacing multiplications with bitwise shifts and adds in LLMs reduce energy consumption and area usage?", "method": "Compare the energy consumption and area usage of LLMs with traditional multiplication operations versus the ShiftAddLLM approach using hardware simulations.", "expected_outcome": "ShiftAddLLM will show up to 31x energy and 26x area reductions compared to traditional multiplication operations."}, {"hypothesis": "Can a multi-objective optimization method minimize overall reparameterization error in LLMs?", "method": "Evaluate the perplexity and task accuracy of LLMs optimized using the multi-objective optimization method versus those optimized using single-objective methods.", "expected_outcome": "The multi-objective optimization method will achieve lower perplexity and better task accuracy."}, {"hypothesis": "Does automated bit allocation based on layer sensitivity improve the accuracy-latency trade-off in LLMs?", "method": "Experiment with LLMs using static bit allocation versus automated bit allocation and compare their accuracy and latency.", "expected_outcome": "Automated bit allocation will result in better accuracy-latency trade-offs."}, {"hypothesis": "Can ShiftAddLLM achieve comparable or better accuracy at 3- and 2-bit precision compared to competitive quantized LLMs?", "method": "Evaluate the perplexity and accuracy of ShiftAddLLM at 3- and 2-bit precision on multiple LLM families and tasks.", "expected_outcome": "ShiftAddLLM will achieve average perplexity reductions of 5.6 and 22.7 points at 3- and 2-bit precision, respectively."}, {"hypothesis": "Is the memory savings of ShiftAddLLM over FP16 format consistent across different LLM architectures?", "method": "Measure and compare the memory usage of ShiftAddLLM and FP16 LLMs across various architectures and sizes.", "expected_outcome": "ShiftAddLLM will save more than 80% memory over FP16 LLMs."}], "follow_up_work_ideas": ["Explore the application of ShiftAddLLM to other types of neural networks such as CNNs and ViTs to evaluate its generalizability.", "Investigate the integration of ShiftAddLLM with other model compression techniques like pruning to further enhance efficiency.", "Develop fast CUDA kernels for ShiftAddLLM with column-wise scaling factors to achieve GPU speedup without sacrificing accuracy.", "Examine the potential of ShiftAddLLM in real-time applications where latency is critical, such as edge computing.", "Evaluate the robustness of ShiftAddLLM against adversarial attacks and explore methods to improve its security."]}}
{"id": "96507", "url": "https://nips.cc/virtual/2024/poster/96507", "title": "DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation", "authors": [], "abstract": "Abstract:Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets.To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model.GenIR, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models. GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation \\& filtering. This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing a cost-effective, privacy-safe solution for IR dataset construction. The result is a large-scale dataset of one million high-quality images.Our second contribution,DreamClear, is a DiT-based image restoration model. It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration. To boost the model's adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM). It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address.Our exhaustive experiments confirm DreamClear's superior performance, underlining the efficacy of our dual strategy for real-world image restoration. Code and pre-trained models are available at: https://github.com/shallowdream204/DreamClear.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/shallowdream204/DreamClear"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": "2", "memory": "Not specified", "API_calls": "Not specified"}, "time_requirements": 12, "code_quality": "Well-documented code", "difficulty": 3, "stars": 984}, "research_ideas": {"problem_statements": ["How can we obtain a large-scale dataset that accurately represents real-world image restoration (IR)?", "How can we construct powerful models tailored for real-world IR scenarios?", "Addressing the acquisition challenges of high-quality (HQ) images due to copyright and privacy concerns.", "Improving the generalization ability of image restoration models to diverse real-world degradations."], "main_takeaways": ["The paper presents GenIR, a privacy-conscious data curation pipeline that generates a large-scale dataset of one million high-quality images for IR tasks.", "DreamClear, a Diffusion Transformer-based image restoration model, is introduced to handle diverse real-world degradations using generative and degradation priors.", "The Mixture of Adaptive Modulator (MoAM) dynamically integrates various restoration experts, enhancing the model's adaptability to different degradation severities.", "DreamClear achieves state-of-the-art performance in handling real-world image restoration tasks."], "testable_hypotheses": [{"hypothesis": "Does the use of GenIR-generated datasets improve the performance of image restoration models compared to traditional datasets?", "method": "Compare the performance of an image restoration model trained on the GenIR-generated dataset versus traditional datasets using metrics like PSNR and SSIM.", "expected_outcome": "The GenIR-generated datasets will lead to improved performance in image restoration models."}, {"hypothesis": "Can DreamClear outperform existing state-of-the-art models in real-world image restoration tasks?", "method": "Conduct experiments comparing DreamClear to other state-of-the-art image restoration models on standard benchmarks.", "expected_outcome": "DreamClear will achieve superior performance across various real-world benchmarks."}, {"hypothesis": "Does integrating token-wise degradation priors improve the adaptability of image restoration models to diverse degradations?", "method": "Evaluate the performance of DreamClear with and without the Mixture of Adaptive Modulator (MoAM) on datasets with varying degradation types.", "expected_outcome": "The integration of MoAM will enhance the model's adaptability to diverse degradations."}, {"hypothesis": "Will dual-prompt learning in GenIR improve the quality of synthesized datasets for image restoration?", "method": "Generate datasets using GenIR with and without dual-prompt learning and assess their impact on training image restoration models.", "expected_outcome": "Dual-prompt learning will result in higher quality synthesized datasets."}, {"hypothesis": "Does the use of detailed text prompts generated by MLLMs improve the semantic accuracy of image restorations?", "method": "Conduct experiments comparing the semantic accuracy of image restorations with and without detailed text prompts.", "expected_outcome": "Detailed text prompts will enhance the semantic accuracy of restored images."}], "follow_up_work_ideas": ["Investigate the application of GenIR and DreamClear in other domains such as video restoration.", "Explore the potential of integrating more advanced language models for generating even more contextually rich prompts in GenIR.", "Develop real-time versions of DreamClear by optimizing the inference speed for practical applications.", "Study the effects of varying the number of experts in the Mixture of Adaptive Modulator on model performance.", "Examine the ethical implications and potential biases introduced by synthetic datasets in image restoration models."]}}
{"id": "96428", "url": "https://nips.cc/virtual/2024/poster/96428", "title": "Invisible Image Watermarks Are Provably Removable Using Generative AI", "authors": [], "abstract": "Abstract:Invisible watermarks safeguard images' copyrights by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models.We propose a family of regeneration attacks to remove these invisible watermarks. The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image. This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and extensive empirical evaluations, we demonstrate that pixel-level invisible watermarks are vulnerable to this regeneration attack.Our results reveal that, across four different pixel-level watermarking schemes, the proposed method consistently achieves superior performance compared to existing attack techniques, with lower detection rates and higher image quality.However, watermarks that keep the image semantically similar can be an alternative defense against our attacks.Our finding underscores the need for a shift in research/industry emphasis from invisible watermarks to semantic-preserving watermarks. Code is available at https://github.com/XuandongZhao/WatermarkAttacker", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/XuandongZhao/WatermarkAttacker"}, "reproduce_difficulty": {"environment_setup": "Easy (pip install -r requirements.txt)", "resource_requirements": "1 GPU (NVIDIA recommended), 8GB RAM", "time_requirements": "2 hours", "code_quality": "Well-documented code", "difficulty": 2, "stars": 206}, "research_ideas": {"problem_statements": ["Can invisible watermarks be robustly removed from images using generative AI?", "Is there a fundamental trade-off between the invisibility of a watermark and its resilience to attacks?", "What are the limitations of existing watermarking methods in terms of vulnerability to attacks?"], "main_takeaways": ["The proposed regeneration attack can effectively remove invisible watermarks from images.", "Pixel-level invisible watermarks are vulnerable to the regeneration attack, which combines destructive and constructive approaches.", "The attack method maintains high image quality while removing watermarks, outperforming existing methods.", "Semantic watermarks offer a potential defense against the attack but may make the watermark somewhat visible."], "testable_hypotheses": [{"hypothesis": "Does the proposed regeneration attack consistently outperform existing watermark removal techniques?", "method": "Conduct experiments comparing the proposed attack to existing methods across different watermark schemes and datasets.", "expected_outcome": "The regeneration attack will achieve lower detection rates and higher image quality compared to baselines."}, {"hypothesis": "Is there a direct correlation between the noise level added during the attack and the effectiveness of watermark removal?", "method": "Vary the noise levels in the regeneration attack and measure the watermark detection rates and image quality.", "expected_outcome": "Higher noise levels will result in more effective watermark removal but may reduce image quality."}, {"hypothesis": "Will semantic watermarks provide a robust defense against the regeneration attack?", "method": "Apply the regeneration attack to images with semantic watermarks and assess detection rates.", "expected_outcome": "Semantic watermarks will show resilience to the attack but may alter image appearance."}, {"hypothesis": "Can the regeneration attack be adapted to remove watermarks from other media types, such as video?", "method": "Extend the attack method to video watermarking schemes and evaluate its effectiveness.", "expected_outcome": "The attack will be effective in removing video watermarks with some adaptation."}, {"hypothesis": "Does the attack's effectiveness depend on the type of generative model used for image reconstruction?", "method": "Test the attack using different generative models, such as VAEs and diffusion models, and compare results.", "expected_outcome": "Different generative models will have varying effectiveness, with diffusion models likely performing best."}], "follow_up_work_ideas": ["Explore the effectiveness of the regeneration attack on semantic watermarks and develop improved defenses.", "Adapt the regeneration attack for real-time applications or video watermarking.", "Investigate the use of the regeneration attack on other forms of digital content, such as audio or text.", "Develop new watermarking schemes that balance visibility and robustness against the regeneration attack.", "Expand the theoretical analysis to explore the trade-offs in watermarking schemes beyond pixel-level watermarks."]}}
{"id": "95449", "url": "https://nips.cc/virtual/2024/poster/95449", "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation", "authors": [], "abstract": "Abstract:A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is that while increasing the number of vision tokens generally enhances visual understanding, it also significantly raises memory and computational costs, especially in long-term, dense video frame streaming scenarios. Although learnable approaches like Q-Former and Perceiver Resampler have been developed to reduce the vision token burden, they overlook the context causally modeled by LLMs (i.e., key-value cache), potentially leading to missed visual cues when addressing user queries. In this paper, we introduce a novel approach to reduce vision compute by leveraging redundant vision tokens ``skipping layers'' rather than decreasing the number of vision tokens. Our method, VideoLLM-MoD, is inspired by mixture-of-depths LLMs and addresses the challenge of numerous vision tokens in long-term or streaming video. Specifically, for certain transformer layer, we learn to skip the computation for a high proportion (e.g., 80\\%) of vision tokens, passing them directly to the next layer. This approach significantly enhances model efficiency, achieving approximately 42% time and 30% memory savings for the entire training. Moreover, our method reduces the computation in the context and avoid decreasing the vision tokens, thus preserving or even improving performance compared to the vanilla model. We conduct extensive experiments to demonstrate the effectiveness of VideoLLM-MoD, showing its state-of-the-art results on multiple benchmarks, including narration, forecasting, and summarization tasks in COIN, Ego4D, and Ego-Exo4D datasets. The code and checkpoints will be made available at github.com/showlab/VideoLLM-online.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/showlab/VideoLLM-online"}, "reproduce_difficulty": {"environment_setup": "Moderate (uses conda and pip for installation)", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1, "recommended": "A100 or 3090"}, "memory": "At least 16 GB", "additional_resources": "ffmpeg for video processing"}, "time_requirements": "Approximately 3 hours for setup and running initial commands", "code_quality": "Well-documented code", "difficulty": 3, "stars": 325}, "research_ideas": {"problem_statements": ["The paper addresses the dilemma in large vision-language models where increasing the number of vision tokens enhances visual understanding but raises memory and computational costs, especially in dense video frame streaming scenarios.", "Current methods to reduce vision token burden overlook the context modeled by LLMs, leading to potential missed visual cues.", "Existing video-based LMMs primarily operate offline and provide event-level responses, which are inadequate for online settings requiring real-time, frame-aligned answers."], "main_takeaways": ["VIDEO LLM-MoD is introduced to reduce vision computation by learning to skip a high proportion of vision tokens at certain transformer layers, significantly enhancing efficiency while preserving or improving performance.", "The approach achieves approximately 42% time and 30% memory savings during training.", "VIDEO LLM-MoD performs state-of-the-art on multiple benchmarks, demonstrating its effectiveness and generalizability in both online and offline video settings."], "testable_hypotheses": [{"hypothesis": "Does skipping a high proportion of vision tokens in certain transformer layers preserve or improve performance compared to processing all tokens?", "method": "Compare model performance with and without the skipping mechanism on benchmarks like COIN and Ego4D.", "expected_outcome": "Skipping tokens will preserve or even improve performance due to reduced redundancy and maintained context."}, {"hypothesis": "Does VIDEO LLM-MoD achieve more efficient computation than traditional full-computation baselines?", "method": "Measure and compare the training time and memory usage of VIDEO LLM-MoD against full-computation models.", "expected_outcome": "VIDEO LLM-MoD will show approximately 42% time and 30% memory savings."}, {"hypothesis": "Does the method perform better on tasks requiring complex spatial context understanding compared to models using only CLS tokens?", "method": "Evaluate the performance on tasks such as EgoExo4D Fine-grained Keystep recognition.", "expected_outcome": "VIDEO LLM-MoD will outperform models using only CLS tokens due to improved spatial capability."}, {"hypothesis": "Can the proposed LayerExpert module effectively identify and process critical vision tokens?", "method": "Analyze performance variations when using different vision token selection strategies (random, uniform, learnable).", "expected_outcome": "Learnable selection of vision tokens will outperform random and uniform strategies, proving the effectiveness of LayerExpert."}, {"hypothesis": "Does the mixture-of-depths approach improve model robustness and reduce overfitting?", "method": "Assess the model's performance on unseen data and compare it to a non-MoD version.", "expected_outcome": "MoD approach will show improved robustness and reduced overfitting indicators."}], "follow_up_work_ideas": ["Investigate the application of VIDEO LLM-MoD to exo-centric video datasets to explore its generalizability.", "Explore the integration of VIDEO LLM-MoD with additional modalities such as audio to enhance multimodal understanding.", "Develop methods to dynamically adjust the vision token skipping ratio based on video content complexity.", "Analyze the potential of VIDEO LLM-MoD in real-time applications like augmented reality and autonomous driving.", "Investigate the impact of different configurations of LayerExpert on various benchmarks to optimize its performance further."]}}
{"id": "94507", "url": "https://nips.cc/virtual/2024/poster/94507", "title": "NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing", "authors": [], "abstract": "Abstract:We propose a video editing framework, NaRCan, which integrates a hybrid deformation field and diffusion prior to generate high-quality natural canonical images to represent the input video. Our approach utilizes homography to model global motion and employs multi-layer perceptrons (MLPs) to capture local residual deformations, enhancing the model\u2019s ability to handle complex video dynamics. By introducing a diffusion prior from the early stages of training, our model ensures that the generated images retain a high-quality natural appearance, making the produced canonical images suitable for various downstream tasks in video editing, a capability not achieved by current canonical-based methods. Furthermore, we incorporate low-rank adaptation (LoRA) fine-tuning and introduce a noise and diffusion prior update scheduling technique that accelerates the training process by 14 times. Extensive experimental results show that our method outperforms existing approaches in various video editing tasks and produces coherent and high-quality edited video sequences. See our project page for video results:koi953215.github.io/NaRCan_page.", "pdf_url": "", "supplementary_url": "", "code_url": "https://koi953215.github.io/NaRCan_page/", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/koi953215/NaRCan"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "1 or more (specific type not mentioned)", "memory": "large amount of GPU memory required"}, "time_requirements": 4, "code_quality": "well-documented", "difficulty": 3, "stars": 162}, "research_ideas": {"problem_statements": ["The paper addresses the challenge of maintaining temporal consistency in video-to-video tasks using diffusion models.", "Existing canonical-based methods focus on reconstruction quality, often neglecting the naturalness of canonical image generation.", "Diffusion-based methods struggle with precise localized editing tasks and temporal consistency.", "Incorporating diffusion priors into canonical image generation has not been effectively explored for video editing."], "main_takeaways": ["NaRCan integrates a hybrid deformation field and diffusion prior to generate natural canonical images for video editing.", "The method uses homography for global motion modeling and MLPs for local residual deformations.", "NaRCan achieves superior performance in video editing tasks, maintaining temporal consistency and high image quality.", "The introduction of diffusion priors and a noise scheduling technique accelerates the training process by 14 times compared to previous methods."], "testable_hypotheses": [{"hypothesis": "Integrating diffusion priors into the training pipeline results in more natural canonical images compared to methods without diffusion priors.", "method": "Compare the naturalness of canonical images generated with and without diffusion priors using a qualitative user study.", "expected_outcome": "Canonical images generated with diffusion priors will be rated as more natural by participants."}, {"hypothesis": "Using a hybrid deformation field improves the temporal consistency of the edited video compared to using only homography or MLPs.", "method": "Evaluate temporal consistency metrics on videos edited using the hybrid deformation field versus only homography or MLP approaches.", "expected_outcome": "The hybrid deformation field will yield higher temporal consistency scores."}, {"hypothesis": "Low-rank adaptation (LoRA) fine-tuning significantly reduces the training time while maintaining model performance.", "method": "Measure training time and performance metrics (e.g., PSNR, SSIM) with and without LoRA fine-tuning.", "expected_outcome": "Training time will be reduced by 14 times with LoRA fine-tuning, while performance metrics remain comparable."}, {"hypothesis": "NaRCan's canonical images facilitate better style transfer and dynamic segmentation compared to existing methods.", "method": "Conduct style transfer and dynamic segmentation tasks using NaRCan and baseline methods, comparing the quality and consistency of results.", "expected_outcome": "NaRCan will outperform baseline methods in both tasks in terms of quality and consistency."}, {"hypothesis": "The proposed noise and diffusion prior update scheduling technique results in faster convergence compared to a static approach.", "method": "Compare convergence rates and final performance metrics between dynamic noise scheduling and static noise levels during training.", "expected_outcome": "Dynamic scheduling will lead to faster convergence and comparable final performance metrics."}], "follow_up_work_ideas": ["Explore the application of NaRCan in different domains such as virtual reality and augmented reality scenarios.", "Investigate the integration of NaRCan with other generative models to enhance its video editing capabilities.", "Develop methods to further reduce the computational overhead of LoRA fine-tuning.", "Examine the potential for NaRCan to improve temporal consistency in other video processing tasks beyond editing.", "Explore ways to enhance the robustness of NaRCan against extreme video motions and transformations."]}}
{"id": "96055", "url": "https://nips.cc/virtual/2024/poster/96055", "title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "authors": [], "abstract": "Abstract:We propose Pure and Lightning ID customization (PuLID), a novel tuning-free ID customization method for text-to-image generation. By incorporating a Lightning T2I branch with a standard diffusion one, PuLID introduces both contrastive alignment loss and accurate ID loss, minimizing disruption to the original model and ensuring high ID fidelity. Experiments show that PuLID achieves superior performance in both ID fidelity and editability. Another attractive property of PuLID is that the image elements (\\eg, background, lighting, composition, and style) before and after the ID insertion are kept as consistent as possible. Codes and models are available at https://github.com/ToTheBeginning/PuLID", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/ToTheBeginning/PuLID"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA A100 or RTX 30 series", "amount": 1}, "memory": "16GB", "other": "Requires Python >= 3.9 and PyTorch >= 2.0"}, "time_requirements": "1-2", "code_quality": "well-documented", "difficulty": 3, "stars": 3134}, "research_ideas": {"problem_statements": ["The need for efficient and effective ID customization in text-to-image (T2I) generation without disrupting the original model's behavior.", "Existing tuning-free methods insert ID information but often disrupt the model's original behavior, affecting non-ID related elements such as background and style.", "Maintaining high ID fidelity while ensuring the model's ability to follow prompts and edit ID attributes is challenging.", "Calculating accurate ID loss is difficult due to the noisy nature of diffusion models, leading to lower ID fidelity.", "The personalization process in current methods is economically expensive and time-consuming."], "main_takeaways": ["PuLID introduces a novel method for ID customization in text-to-image generation, focusing on contrastive alignment and tuning-free approaches.", "The method ensures high ID fidelity by incorporating a Lightning T2I branch, which minimizes disruption to the original model's behavior.", "Contrastive alignment loss and accurate ID loss are used to align ID insertion with the model's original behavior, maintaining consistency in non-ID related image elements.", "Experiments demonstrate that PuLID achieves superior performance in terms of ID fidelity and editability compared to existing methods.", "PuLID's approach reduces the economic and temporal costs associated with ID customization, making it more practical for widespread use."], "testable_hypotheses": [{"hypothesis": "Does using a Lightning T2I branch improve ID fidelity without disrupting the original model's behavior compared to conventional methods?", "method": "Conduct experiments comparing ID fidelity and model behavior consistency with and without the Lightning T2I branch.", "expected_outcome": "The Lightning T2I branch will improve ID fidelity and maintain model behavior consistency better than conventional methods."}, {"hypothesis": "Will contrastive alignment loss reduce the contamination of ID information on the original model's behavior?", "method": "Evaluate the impact of contrastive alignment loss on model behavior by comparing image element consistency before and after ID insertion.", "expected_outcome": "Contrastive alignment loss will significantly reduce the contamination of ID information on the model's behavior."}, {"hypothesis": "Does the accurate ID loss calculated in the Lightning T2I branch lead to higher ID similarity compared to naive ID loss methods?", "method": "Compare ID similarity metrics using both accurate ID loss and naive ID loss methods in different settings.", "expected_outcome": "Accurate ID loss will result in higher ID similarity than naive ID loss methods."}, {"hypothesis": "Can PuLID maintain the ability to follow prompts and edit ID attributes after ID insertion?", "method": "Test the model's prompt-following and ID attribute editing capabilities before and after ID insertion using various prompts.", "expected_outcome": "PuLID will maintain the ability to follow prompts and edit ID attributes effectively after ID insertion."}, {"hypothesis": "Is the PuLID method more economically efficient for ID customization compared to tuning-based methods?", "method": "Measure the time and computational resources required for ID customization using PuLID versus tuning-based methods.", "expected_outcome": "PuLID will require less time and computational resources, proving more economically efficient than tuning-based methods."}], "follow_up_work_ideas": ["Explore different configurations or variations of the Lightning T2I branch to further enhance ID fidelity and model behavior consistency.", "Investigate the potential of applying contrastive alignment and accurate ID loss techniques to other domains of image customization, such as style customization.", "Examine the scalability of PuLID in handling a larger variety of ID features and more complex prompts.", "Develop strategies to reduce the computational demands of the Lightning T2I branch, potentially integrating more advanced fast sampling techniques.", "Study the long-term effects of using PuLID for continuous model updates and personalization in real-time applications."]}}
{"id": "95747", "url": "https://nips.cc/virtual/2024/poster/95747", "title": "Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image Generation", "authors": [], "abstract": "Abstract:Controllable text-to-image (T2I) diffusion models have shown impressive performance in generating high-quality visual content through the incorporation of various conditions. Current methods, however, exhibit limited performance when guided by skeleton human poses, especially in complex pose conditions such as side or rear perspectives of human figures. To address this issue, we present Stable-Pose, a novel adapter model that introduces a coarse-to-fine attention masking strategy into a vision Transformer (ViT) to gain accurate pose guidance for T2I models. Stable-Pose is designed to adeptly handle pose conditions within pre-trained Stable Diffusion, providing a refined and efficient way of aligning pose representation during image synthesis. We leverage the query-key self-attention mechanism of ViTs to explore the interconnections among different anatomical parts in human pose skeletons. Masked pose images are used to smoothly refine the attention maps based on target pose-related features in a hierarchical manner, transitioning from coarse to fine levels. Additionally, our loss function is formulated to allocate increased emphasis to the pose region, thereby augmenting the model's precision in capturing intricate pose details.    We assessed the performance of Stable-Pose across five public datasets under a wide range of indoor and outdoor human pose scenarios. Stable-Pose achieved an AP score of 57.1 in the LAION-Human dataset, marking around 13\\% improvement over the established technique ControlNet.  The project link and code is available at https://github.com/ai-med/StablePose.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/ai-med/StablePose"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 2}, "memory": "16GB", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 100}, "research_ideas": {"problem_statements": ["Current text-to-image diffusion models struggle with generating images guided by skeleton human poses, particularly in complex pose conditions such as side or rear perspectives.", "Traditional pose-guided image generation methods require source images for style guidance, limiting flexibility and diversity in the output.", "Existing methods often fail to maintain accurate body proportions, leading to unnatural appearances in generated images."], "main_takeaways": ["Stable-Pose introduces a coarse-to-fine attention masking strategy using vision Transformers to improve pose guidance in text-to-image generation.", "The model captures long-range relationships among anatomical parts in human poses, enhancing the alignment of pose representation during image synthesis.", "Stable-Pose significantly improves pose adherence and image fidelity, achieving higher accuracy and robustness compared to state-of-the-art methods."], "testable_hypotheses": [{"hypothesis": "Does the coarse-to-fine attention masking strategy improve pose adherence in text-to-image generation?", "method": "Compare the pose adherence metrics of the model with and without the coarse-to-fine attention strategy on a standard dataset.", "expected_outcome": "The model with coarse-to-fine attention masking will show improved pose adherence."}, {"hypothesis": "Will incorporating additional pose masks into ControlNet improve pose alignment?", "method": "Integrate additional pose masks into ControlNet and evaluate pose alignment metrics against the baseline ControlNet.", "expected_outcome": "Additional pose masks will provide marginal improvements, indicating that the main improvements are due to PMSA."}, {"hypothesis": "Does the hierarchical integration of pose masks enhance the generation fidelity of images?", "method": "Evaluate image fidelity metrics for models with hierarchical and non-hierarchical integration of pose masks.", "expected_outcome": "Hierarchical integration will provide better image fidelity scores."}, {"hypothesis": "Does Stable-Pose maintain accurate body proportions better than other methods when generating images in rare poses?", "method": "Evaluate and compare body proportion accuracy in generated images using Stable-Pose and other methods on a dataset with rare poses.", "expected_outcome": "Stable-Pose will maintain more accurate body proportions, especially in rare poses."}, {"hypothesis": "Does the use of Stable-Pose reduce the need for computational resources compared to T2I-Adapter?", "method": "Measure and compare the computational resources (GPU hours) required for training Stable-Pose and T2I-Adapter on a large dataset.", "expected_outcome": "Stable-Pose will require significantly fewer computational resources."}], "follow_up_work_ideas": ["Explore the application of Stable-Pose in generating images guided by other conditions, such as edge maps, to assess its versatility.", "Investigate the robustness of Stable-Pose in generating images of non-human figures guided by pose, such as animals or fictional beings.", "Enhance Stable-Pose by integrating additional contextual information, like background or lighting conditions, to improve overall scene coherence.", "Develop a lightweight version of Stable-Pose to reduce inference time while maintaining high pose accuracy.", "Apply the Stable-Pose framework to video generation tasks to check its effectiveness in maintaining pose consistency across frames."]}}
{"id": "95671", "url": "https://nips.cc/virtual/2024/poster/95671", "title": "RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance", "authors": [], "abstract": "Abstract:Customizing diffusion models to generate identity-preserving images from user-provided reference images is an intriguing new problem. The prevalent approaches typically require training on extensive domain-specific images to achieve identity preservation, which lacks flexibility across different use cases. To address this issue, we exploit classifier guidance, a training-free technique that steers diffusion models using an existing classifier, for personalized image generation. Our study shows that based on a recent rectified flow framework, the major limitation of vanilla classifier guidance in requiring a special classifier can be resolved with a simple fixed-point solution, allowing flexible personalization with off-the-shelf image discriminators. Moreover, its solving procedure proves to be stable when anchored to a reference flow trajectory, with a convergence guarantee. The derived method is implemented on rectified flow with different off-the-shelf image discriminators, delivering advantageous personalization results for human faces, live subjects, and certain objects. Code is available at https://github.com/feifeiobama/RectifID.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/feifeiobama/RectifID"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires installation of several dependencies including deep learning libraries and models)", "resource_requirements": "A GPU with a minimum of 12.5GB memory is required.", "time_requirements": "Approximately 1 hour to run the main commands.", "code_quality": "Well-documented code with clear instructions for setup and usage.", "difficulty": 3, "stars": 124}, "research_ideas": {"problem_statements": ["The prevalent approaches for customizing diffusion models to generate identity-preserving images require extensive domain-specific training, lacking flexibility.", "Existing personalization methods face challenges in preserving subject identity and being adaptable to various personalization needs without extensive pre-training.", "The original classifier guidance is limited due to reliance on a special classifier trained on noised inputs."], "main_takeaways": ["A training-free approach using classifier guidance for diffusion models can address identity preservation without the need for additional training.", "The proposed method reformulates classifier guidance into a fixed-point problem, allowing the use of off-the-shelf image discriminators.", "Anchoring the classifier-guided flow trajectory to a reference trajectory improves stability and provides convergence guarantees.", "The method achieves advantageous personalization results across tasks involving human faces, live subjects, and certain objects."], "testable_hypotheses": [{"hypothesis": "Does using a fixed-point solution for classifier guidance improve the personalization of diffusion models?", "method": "Implement the fixed-point formulation for classifier guidance and evaluate identity preservation and flexibility.", "expected_outcome": "The fixed-point solution enhances flexibility and identity preservation in personalized image generation."}, {"hypothesis": "Will anchoring the classifier-guided flow to a reference trajectory improve convergence stability?", "method": "Compare the stability and convergence of the guided flow with and without anchoring to a reference trajectory.", "expected_outcome": "Anchoring provides improved stability and convergence in the solving process."}, {"hypothesis": "Can the proposed method work effectively with different off-the-shelf discriminators?", "method": "Test the method using various existing image discriminators on personalization tasks.", "expected_outcome": "The method achieves flexible personalization results with different discriminators."}, {"hypothesis": "Will the training-free approach outperform existing methods that require domain-specific training?", "method": "Conduct a comparative evaluation against methods requiring domain-specific training on personalization tasks.", "expected_outcome": "The training-free approach shows competitive or superior performance without needing extensive data."}, {"hypothesis": "Does the piecewise linear assumption of rectified flow hold in practice for different types of images?", "method": "Evaluate the performance of the method on various image types while assuming piecewise linear flow.", "expected_outcome": "The piecewise linear assumption is valid for diverse image types, yielding effective results."}], "follow_up_work_ideas": ["Explore the application of the proposed method to other domains beyond faces and common subjects, such as medical imaging or satellite photos.", "Investigate the integration of advanced discriminators or multi-modal discriminators to further enhance personalization capabilities.", "Develop more sophisticated numerical solvers for the fixed-point problem to improve computational efficiency.", "Address limitations related to object shape and structure by combining with other image prompt techniques.", "Assess the method's robustness to adversarial attacks or data perturbations in personalized image generation."]}}
{"id": "95573", "url": "https://nips.cc/virtual/2024/poster/95573", "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models", "authors": [], "abstract": "Abstract:Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at https://github.com/NVlabs/MaskLLM.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/NVlabs/MaskLLM"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA GPUs", "amount": 8}, "memory": "Approximately 40GB per GPU"}, "time_requirements": "4", "code_quality": "Well-documented code", "difficulty": 3, "stars": 152}, "research_ideas": {"problem_statements": ["Large Language Models (LLMs) have massive parameter counts leading to significant redundancy, making them challenging and resource-intensive to deploy in real-world applications.", "Existing pruning methods like SparseGPT and Wanda use small calibration sets and handcrafted importance criteria, which are insufficient for representing the comprehensive knowledge in LLMs.", "There is a gap between the real discrepancy induced by pruning and the existing importance indicators, such as gradient information, weight magnitude, and Hessian Matrix.", "Finding the optimal mask set for semi-structured pruning in LLMs is exceedingly challenging due to the vast search space and non-differentiability of mask selection."], "main_takeaways": ["MaskLLM is proposed as a learnable pruning method that establishes semi-structured (N:M) sparsity in LLMs to reduce computational overhead during inference.", "MaskLLM models N:M patterns as a learnable distribution through Gumbel Softmax sampling, allowing end-to-end training on large datasets.", "The method effectively scales to large datasets, learns high-quality masks, and enables transfer learning of sparsity across domains or tasks.", "MaskLLM outperforms state-of-the-art methods in terms of perplexity while keeping the model weights frozen.", "Learnable masks allow for lossless compression of LLMs on some downstream tasks, offering significant storage and computational efficiency."], "testable_hypotheses": [{"hypothesis": "Does the learnable mask from MaskLLM provide better perplexity performance than existing methods like SparseGPT on LLaMA-2 7B?", "method": "Train MaskLLM on the LLaMA-2 7B model and compare the perplexity scores on the Wikitext dataset against SparseGPT.", "expected_outcome": "MaskLLM achieves a lower perplexity score of 6.72 compared to SparseGPT's 10.42."}, {"hypothesis": "Can MaskLLM's learnable mask be effectively transferred to new tasks without loss of performance?", "method": "Apply MaskLLM's learned masks to new downstream tasks and evaluate performance metrics like perplexity or accuracy.", "expected_outcome": "MaskLLM can achieve lossless compression and maintain performance close to the dense model on new tasks."}, {"hypothesis": "Does increasing the sample size used in MaskLLM's end-to-end training improve the quality of the learned masks?", "method": "Vary the number of samples used during MaskLLM training and measure the impact on mask quality through perplexity scores.", "expected_outcome": "Increasing the sample size consistently improves mask quality with observable improvements up to 512k samples."}, {"hypothesis": "Is the incorporation of sparse weight regularization crucial for the effective transfer of learned masks to downstream tasks?", "method": "Train MaskLLM with and without sparse weight regularization and compare their performance on downstream tasks.", "expected_outcome": "Sparse weight regularization helps maintain large gradients, leading to better transfer learning performance."}, {"hypothesis": "Does the Gumbel Softmax scaling factor influence the convergence speed and quality of the learned masks in MaskLLM?", "method": "Experiment with different scaling factors for Gumbel Softmax and evaluate their impact on mask convergence and performance metrics.", "expected_outcome": "An appropriate scaling factor is crucial; too small leads to slow convergence, while too large suppresses exploration."}], "follow_up_work_ideas": ["Explore the application of MaskLLM to other domains beyond language models, such as vision models or multimodal models.", "Investigate methods to further improve the training efficiency of learnable masks to reduce computational resources.", "Develop techniques to dynamically adjust the N:M sparsity ratio for different parts of the model based on task-specific needs.", "Extend MaskLLM to support more complex sparsity patterns or combinations of patterns to explore their impact on efficiency and performance.", "Study the impact of learnable sparsity on the robustness and generalization of models under adversarial settings or noisy data environments."]}}
{"id": "95465", "url": "https://nips.cc/virtual/2024/poster/95465", "title": "AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning", "authors": [], "abstract": "Abstract:Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demon- strate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/zou-group/avatar"}, "reproduce_difficulty": {"environment_setup": "Moderate (uses conda and pip for installation)", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "at least 16GB", "API_calls": "requires API keys for OpenAI and Anthropic"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 181}, "research_ideas": {"problem_statements": ["Developing effective prompting techniques for LLM agents to utilize external tools remains heuristic and labor-intensive.", "Current prompting methods often result in brittle implementations with suboptimal accuracy.", "There is a need for an automated framework that optimizes LLM agents for effective tool utilization."], "main_takeaways": ["AVATAR is an automated framework that optimizes LLM agents for tool usage through contrastive reasoning.", "The comparator module in AVATAR generates holistic prompts by contrasting positive and negative examples.", "AVATAR consistently outperforms state-of-the-art approaches across multiple datasets.", "The framework shows strong generalization ability and achieves significant improvements in task performance metrics."], "testable_hypotheses": [{"hypothesis": "Does the use of a comparator module improve the LLM agent's performance more than using per-sample instructions?", "method": "Compare the performance of AVATAR with and without the comparator module across the same datasets.", "expected_outcome": "The use of a comparator module will result in better performance due to more generalized prompt instructions."}, {"hypothesis": "Can AVATAR's optimization framework improve the Hit@1 metric for multimodal retrieval tasks?", "method": "Evaluate AVATAR on various multimodal retrieval datasets and measure changes in the Hit@1 metric.", "expected_outcome": "AVATAR will show a significant improvement in the Hit@1 metric compared to baseline methods."}, {"hypothesis": "Does iterative optimization with AVATAR lead to improved performance over time?", "method": "Track performance metrics across multiple iterations of optimization on the same dataset.", "expected_outcome": "Performance metrics will improve with each iteration, demonstrating the effectiveness of iterative optimization."}, {"hypothesis": "Will AVATAR's framework generalize well to new and unseen queries in complex retrieval tasks?", "method": "Test AVATAR on a set of queries that were not used during the training phase.", "expected_outcome": "AVATAR will maintain high performance metrics, showing strong generalization capabilities."}, {"hypothesis": "Can the optimized actions from AVATAR be applied effectively to real-world tasks beyond retrieval and QA?", "method": "Apply AVATAR to a different domain or application and measure task performance.", "expected_outcome": "AVATAR's optimized actions will show improved performance in the new domain, indicating versatility."}], "follow_up_work_ideas": ["Explore the application of AVATAR's framework to other AI tasks involving multi-modal data.", "Investigate the development of dynamic tool libraries that can evolve with agent experience.", "Study the integration of specialized LLMs as auxiliary agents to improve scalability.", "Develop better memory banks for storing knowledge and experiences from past training.", "Apply AVATAR's approach to visual reasoning tasks to assess its adaptability to different problem types."]}}
{"id": "95327", "url": "https://nips.cc/virtual/2024/poster/95327", "title": "Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration", "authors": [], "abstract": "Abstract:Membership Inference Attacks (MIA) aim to infer whether a target data record has been utilized for model training or not. Existing MIAs designed for large language models (LLMs) can be bifurcated into two types: reference-free and reference-based attacks. Although reference-based attacks appear promising performance by calibrating the probability measured on the target model with reference models, this illusion of privacy risk heavily depends on a reference dataset that closely resembles the training set. Both two types of attacks are predicated on the hypothesis that training records consistently maintain a higher probability of being sampled. However, this hypothesis heavily relies on the overfitting of target models, which will be mitigated by multiple regularization methods and the generalization of LLMs. Thus, these reasons lead to high false-positive rates of MIAs in practical scenarios.We propose a Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA). Specifically, we introduce a self-prompt approach, which constructs the dataset to fine-tune the reference model by prompting the target LLM itself. In this manner, the adversary can collect a dataset with a similar distribution from public APIs.Furthermore, we introduce probabilistic variation, a more reliable membership signal based on LLM memorization rather than overfitting, from which we rediscover the neighbour attack with theoretical grounding. Comprehensive evaluation conducted on three datasets and four exemplary LLMs shows that SPV-MIA raises the AUC of MIAs from 0.7 to a significantly high level of 0.9. Our code and dataset are available at: https://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": "1 or more GPUs (recommended multi-GPU setup), 16GB+ RAM", "time_requirements": "2-4 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 175}, "research_ideas": {"problem_statements": ["How can Membership Inference Attacks (MIA) better reveal privacy risks in fine-tuned Large Language Models (LLMs) when existing methods show high false-positive rates due to assumptions of overfitting and the dependence on high-quality reference datasets?", "Can the concept of probabilistic variation provide a more reliable membership signal than traditional overfitting-based methods in the context of MIAs?", "What are the privacy concerns associated with fine-tuned LLMs, and how can these be quantified through membership inference attacks?"], "main_takeaways": ["The paper proposes SPV-MIA, a self-calibrated probabilistic variation-based membership inference attack that improves MIA performance by utilizing a self-prompt reference model.", "SPV-MIA significantly raises the AUC of MIAs to 0.9 compared to existing methods by using the target LLM to generate reference datasets, thereby removing the dependency on similar distribution reference datasets.", "The study highlights the importance of LLM memorization in MIAs, suggesting that memorization, rather than overfitting, is a more reliable signal for membership inference.", "The proposed method demonstrates robustness across different datasets and LLMs, indicating its practical applicability in real-world scenarios."], "testable_hypotheses": [{"hypothesis": "Does the self-prompt approach lead to a higher quality of reference datasets than external domain-specific datasets?", "method": "Compare the performance of SPV-MIA using self-prompt-generated datasets against domain-specific datasets in MIA tasks.", "expected_outcome": "Self-prompt-generated datasets will yield a higher attack performance, as indicated by the AUC scores."}, {"hypothesis": "Is probabilistic variation a more reliable membership signal than traditional overfitting-based signals?", "method": "Conduct experiments comparing the performance of MIAs using probabilistic variation versus overfitting-based metrics.", "expected_outcome": "Probabilistic variation will show lower false-positive rates and higher true-positive rates."}, {"hypothesis": "Does SPV-MIA outperform existing MIAs in detecting members in fine-tuned LLMs?", "method": "Evaluate SPV-MIA and existing MIA methods on multiple datasets and LLMs to compare detection performance.", "expected_outcome": "SPV-MIA will achieve higher AUC scores and TPR@1%FPR compared to baselines."}, {"hypothesis": "Can SPV-MIA maintain performance with only irrelevant prompt texts?", "method": "Test SPV-MIA using irrelevant texts as prompts for the self-prompt reference model and measure the impact on performance.", "expected_outcome": "The performance of SPV-MIA will only slightly decrease when using irrelevant texts."}, {"hypothesis": "Does the scale of self-prompt texts affect the performance of SPV-MIA?", "method": "Assess the impact of varying the number of self-prompt texts on the performance of SPV-MIA.", "expected_outcome": "SPV-MIA will maintain stable performance even with a reduced number of self-prompt texts."}], "follow_up_work_ideas": ["Extend SPV-MIA to other types of LLMs, such as masked language models (MLMs) and sequence-to-sequence models, to assess its generalizability.", "Investigate the impact of different paraphrasing techniques on the effectiveness of probabilistic variation in SPV-MIA.", "Develop defense mechanisms against SPV-MIA by exploring differential privacy techniques or other regularization methods.", "Explore the applicability of SPV-MIA in other domains beyond NLP, such as computer vision or recommendation systems, to evaluate its versatility.", "Optimize the computational efficiency of SPV-MIA, particularly the probabilistic variation assessment, to make it more feasible for large-scale deployments."]}}
{"id": "95294", "url": "https://nips.cc/virtual/2024/poster/95294", "title": "Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers", "authors": [], "abstract": "Abstract:One of the roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. This work studies the problem of learning policy representations through heterogeneous pre-training on robot data across different embodiments and tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation. This general architecture aligns the specific proprioception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks. Leveraging the recent large-scale multi-embodiment real-world robotic datasets as well as simulation, deployed robots, and human video datasets, we investigate pre-training policies across heterogeneity. We conduct experiments to investigate the scaling behaviors of training objectives, to the extent of 52 datasets. HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20% on unseen tasks in multiple simulator benchmarks and real-world settings. See the project website (liruiw.github.io/hpt) for code and videos.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/liruiw/hpt"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_call": "Hugging Face models"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 465}, "research_ideas": {"problem_statements": ["How can we leverage heterogeneous robot data to pre-train robotic foundation models efficiently?", "How can pre-trained models generalize across different robotic embodiments and tasks?", "Can we develop a scalable architecture that supports diverse data from different sensor inputs and environments?", "What is the impact of dataset diversity and size on the performance of pre-trained robotic models?", "How can we address the challenges posed by the heterogeneity in robot data during pre-training?"], "main_takeaways": ["Heterogeneous Pre-trained Transformers (HPT) can effectively pre-train on diverse robot data, outperforming several baselines by over 20% on unseen tasks.", "Pre-trained models using HPT require minimal fine-tuning and generalize well across different robotic tasks and embodiments.", "Scaling behaviors of HPT were investigated, showing improvements with increased data and model size.", "HPT's architecture modularizes into stems, trunk, and heads, allowing it to process inputs from different sensor types and map them to a shared latent space.", "The study demonstrates the potential to use heterogeneous data, including simulations and human videos, for pre-training robotic models."], "testable_hypotheses": [{"hypothesis": "Does pre-training on heterogeneous data improve generalization to new tasks compared to single-task pre-training?", "method": "Compare the performance of models pre-trained on heterogeneous datasets with those pre-trained on a single task dataset using unseen tasks in simulation benchmarks.", "expected_outcome": "Models pre-trained on heterogeneous data will show better generalization and higher success rates on new tasks."}, {"hypothesis": "Does increasing the diversity of datasets improve the scalability of the pre-trained model?", "method": "Analyze the validation loss and task success rates as the diversity of pre-training datasets is varied.", "expected_outcome": "Greater dataset diversity will lead to lower validation losses and improved task success rates."}, {"hypothesis": "Does the inclusion of proprioception data in pre-training improve policy performance?", "method": "Evaluate policy performance on tasks with and without proprioception data included during pre-training.", "expected_outcome": "Inclusion of proprioception data will result in better performance due to richer input information."}, {"hypothesis": "Will a larger model size improve the performance of HPT in terms of task success rates?", "method": "Train models of varying sizes on the same dataset and compare their task success rates across benchmarks.", "expected_outcome": "Larger models will achieve higher task success rates due to their capacity to learn more complex functions."}, {"hypothesis": "Can HPT effectively transfer knowledge from simulation and video datasets to real-world robotic tasks?", "method": "Pre-train HPT using simulation and video datasets and evaluate its performance in real-world tasks.", "expected_outcome": "HPT will successfully transfer knowledge and perform well on real-world tasks, demonstrating versatility."}], "follow_up_work_ideas": ["Investigate the impact of different pre-training objectives, such as reinforcement learning or self-supervised learning, on the performance of HPT.", "Explore the use of more diverse sensor modalities, such as tactile or 3D sensors, in the HPT framework.", "Develop techniques to further reduce the amount of fine-tuning required after pre-training with HPT.", "Apply HPT to long-horizon tasks and evaluate its performance in more complex environments.", "Study the potential of using HPT in other domains, such as autonomous driving or drone navigation, where heterogeneous data is prevalent."]}}
{"id": "93727", "url": "https://nips.cc/virtual/2024/poster/93727", "title": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs", "authors": [], "abstract": "Abstract:Quantization of large language models (LLMs) faces significant challenges, particularly due to the presence of outlier activations that impede efficient low-bit representation. Traditional approaches predominantly address Normal Outliers, which are activations across all tokens with relatively large magnitudes. However, these methods struggle with smoothing Massive Outliers that display significantly larger values, which leads to significant performance degradation in low-bit quantization. In this paper, we introduce DuQuant, a novel approach that utilizes rotation and permutation transformations to more effectively mitigate both massive and normal outliers. First, DuQuant starts by constructing the rotation matrix, using specific outlier dimensions as prior knowledge, to redistribute outliers to adjacent channels by block-wise rotation. Second, We further employ a zigzag permutation to balance the distribution of outliers across blocks, thereby reducing block-wise variance. A subsequent rotation further smooths the activation landscape, enhancing model performance. DuQuant simplifies the quantization process and excels in managing outliers, outperforming the state-of-the-art baselines across various sizes and types of LLMs on multiple tasks, even with 4-bit weight-activation quantization. Our code is available at https://github.com/Hsu1023/DuQuant.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Hsu1023/DuQuant"}, "reproduce_difficulty": {"environment_setup": "Moderate - Requires conda for environment management and pip for package installation.", "resource_requirements": "1 GPU (NVIDIA recommended), with at least 16GB of memory for effective performance.", "time_requirements": "Approximately 2 hours to run the main command, including setup time.", "code_quality": "Well-documented code with clear instructions for installation and usage.", "difficulty": 3, "stars": 143}, "research_ideas": {"problem_statements": ["Quantization of large language models faces significant challenges due to the presence of outlier activations that impede efficient low-bit representation.", "Existing methods struggle to handle Massive Outliers, which leads to significant performance degradation in low-bit quantization.", "There is a pressing need for an LLM quantization approach that effectively addresses both Normal and Massive Outliers."], "main_takeaways": ["DuQuant is a novel approach that utilizes rotation and permutation transformations to mitigate both massive and normal outliers in quantized LLMs.", "DuQuant uses a rotation matrix to redistribute outliers to adjacent channels and a zigzag permutation to balance the distribution of outliers across blocks.", "DuQuant outperforms state-of-the-art baselines in various benchmarks, achieving significant improvements in both efficiency and performance of quantized LLMs.", "The method achieves up to 2.08x speedup and 3.50x memory usage reduction during decoding with minimal impact on performance, compared to FP16 models."], "testable_hypotheses": [{"hypothesis": "Does the zigzag permutation significantly reduce activation variance across blocks?", "method": "Evaluate activation variance across blocks before and after applying the zigzag permutation on various LLM models.", "expected_outcome": "The zigzag permutation will reduce activation variance, leading to more uniform distribution of outliers."}, {"hypothesis": "Will DuQuant outperform existing LLM quantization methods in reducing perplexity on benchmark datasets?", "method": "Compare perplexity scores of models quantized using DuQuant and other state-of-the-art methods on WikiText2 and C4 datasets.", "expected_outcome": "DuQuant will show lower perplexity scores compared to other quantization methods."}, {"hypothesis": "Does DuQuant maintain competitive accuracy in zero-shot QA tasks compared to FP16 models?", "method": "Conduct zero-shot QA tasks using LLaMA models quantized with DuQuant and compare with FP16 models.", "expected_outcome": "DuQuant will maintain competitive accuracy, close to FP16 models."}, {"hypothesis": "Can DuQuant effectively manage massive outliers in FFN down-projection layers across different LLM architectures?", "method": "Visualize and analyze activation distributions in FFN down-projection layers before and after applying DuQuant on multiple LLM architectures.", "expected_outcome": "DuQuant will show effective reduction of massive outliers in these layers across different architectures."}, {"hypothesis": "Is DuQuant's rotation matrix more effective than random or Hadamard matrices in smoothing activations?", "method": "Compare the effectiveness of DuQuant's rotation matrix with random and Hadamard matrices in terms of activation smoothing in quantized LLMs.", "expected_outcome": "DuQuant's rotation matrix will demonstrate superior performance in smoothing activations compared to the other matrices."}], "follow_up_work_ideas": ["Investigate the impact of different calibration data on the performance of DuQuant to enhance its robustness and adaptability.", "Explore additional permutation strategies to further optimize the distribution of outliers across activation blocks.", "Apply DuQuant to new domains such as vision-language models to assess its effectiveness in multi-modal settings.", "Develop an efficient strategy for selecting optimal calibration data that enhances DuQuant's performance without increasing computational overhead.", "Explore the integration of DuQuant with other model compression techniques like pruning to further enhance LLM deployment in resource-constrained environments."]}}
{"id": "95195", "url": "https://nips.cc/virtual/2024/poster/95195", "title": "RealCompo: Balancing Realism and Compositionality Improves Text-to-Image Diffusion Models", "authors": [], "abstract": "Abstract:Diffusion models have achieved remarkable advancements in text-to-image generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we proposeRealCompo, a newtraining-freeandtransferred-friendlytext-to-image generation framework, which aims to leverage the respective advantages of text-to-image models and spatial-aware image diffusion models (e.g., layout, keypoints and segmentation maps) to enhance both realism and compositionality of the generated images. An intuitive and novelbalanceris proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and spatial-aware image diffusion models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Notably, our RealCompo can be seamlessly extended with a wide range of spatial-aware image diffusion models and stylized diffusion models. Code is available at: https://github.com/YangLing0818/RealCompo", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/YangLing0818/RealCompo"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "1x NVIDIA GPU (e.g., GTX 1080 or better)", "memory": "At least 16 GB RAM", "API_calls": "Required for certain functionalities"}, "time_requirements": 1, "code_quality": "well-documented code", "difficulty": 3, "stars": 114}, "research_ideas": {"problem_statements": ["Existing text-to-image diffusion models struggle with generating images that accurately align with compositional prompts involving multiple objects or complex relationships.", "There is a significant trade-off between realism and compositionality in generated images when using text-to-image and layout-to-image models.", "Current methods require additional training and lack flexibility, making it difficult to generalize to other models and conditions."], "main_takeaways": ["RealCompo is a new training-free framework that balances realism and compositionality in text-to-image generation by integrating text-to-image models with spatial-aware image diffusion models.", "A novel balancer is introduced to dynamically adjust the influence of text-to-image and spatial-aware models during the denoising process, enhancing the quality of generated images.", "RealCompo outperforms state-of-the-art models in generating multiple objects and complex relationships while maintaining high realism and compositionality.", "The framework is flexible and can be extended to various spatial-aware conditions and stylized text-to-image models without additional training."], "testable_hypotheses": [{"hypothesis": "RealCompo will outperform existing text-to-image models in scenarios with multiple objects and complex relationships.", "method": "Conduct experiments comparing the performance of RealCompo and existing models on complex prompts involving multiple objects.", "expected_outcome": "RealCompo will show superior performance in maintaining both realism and compositionality."}, {"hypothesis": "The dynamic balancer in RealCompo effectively maintains a trade-off between realism and compositionality.", "method": "Analyze the denoising process with and without the dynamic balancer to assess changes in image quality and alignment with prompts.", "expected_outcome": "The dynamic balancer will show improved results in terms of image realism and compositional accuracy."}, {"hypothesis": "RealCompo can be extended to keypoint-based and segmentation-based models effectively.", "method": "Test the RealCompo framework using keypoint and segmentation maps as spatial-aware conditions and evaluate performance.", "expected_outcome": "RealCompo will achieve high-quality compositional generation with keypoint and segmentation-based models."}, {"hypothesis": "Replacing T2I models with stylized models in RealCompo will preserve the style while maintaining compositional quality.", "method": "Use RealCompo with various stylized T2I models and evaluate the style preservation and compositional quality of the generated images.", "expected_outcome": "RealCompo will maintain the intended style and achieve strong compositional quality."}, {"hypothesis": "The influence of layout control parameters directly affects the realism of the generated images.", "method": "Conduct experiments varying the layout control parameters and measure the impact on image realism and aesthetics.", "expected_outcome": "An increase in layout control parameters will lead to a decrease in image realism."}], "follow_up_work_ideas": ["Investigate more efficient computational methods to reduce the computational cost of using RealCompo.", "Explore the application of RealCompo to other domains, such as text-to-video and text-to-3D generation.", "Develop fixed-coefficient strategies to further enhance the capabilities of RealCompo.", "Adapt RealCompo for real-time applications or interactive systems to leverage its compositional strengths.", "Explore the potential of RealCompo in generating images that adhere to specific cultural or artistic styles more closely."]}}
{"id": "95100", "url": "https://nips.cc/virtual/2024/poster/95100", "title": "U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers", "authors": [], "abstract": "Abstract:Diffusion Transformers (DiTs) introduce the transformer architecture to diffusion tasks for latent-space image generation. With an isotropic architecture that chains a series of transformer blocks, DiTs demonstrate competitive performance and good scalability; but meanwhile, the abandonment of U-Net by DiTs and their following improvements is worth rethinking. To this end, we conduct a simple toy experiment by comparing a U-Net architectured DiT with an isotropic one. It turns out that the U-Net architecture only gain a slight advantage amid the U-Net inductive bias, indicating potential redundancies within the U-Net-style DiT. Inspired by the discovery that U-Net backbone features are low-frequency-dominated, we perform token downsampling on the query-key-value tuple for self-attention and bring further improvements despite a considerable amount of reduction in computation. Based on self-attention with downsampled tokens, we propose a series of U-shaped DiTs (U-DiTs) in the paper and conduct extensive experiments to demonstrate the extraordinary performance of U-DiT models. The proposed U-DiT could outperform DiT-XL with only 1/6 of its computation cost. Codes are available at https://github.com/YuchuanTian/U-DiT.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/YuchuanTian/U-DiT"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"count": 1, "type": "NVIDIA", "memory": "8GB or more"}, "RAM": "16GB or more", "disk_space": "21GB for VAE features"}, "time_requirements": 5, "code_quality": "well-documented", "difficulty": 3, "stars": 185}, "research_ideas": {"problem_statements": ["DiTs have discarded the U-Net architecture that is universally applied in manifold previous works, triggering curiosity about whether the U-Net inductive bias truly helps denoising.", "The U-Net architecture is widely used for diffusion applications, but its absence in recent transformer-oriented works for latent-space diffusion is counter-intuitive.", "There is a need to explore the potential of Transformer-backboned U-Net on latent-space diffusion.", "The abandonment of U-Net by DiTs and their improvements raises questions about potential redundancies in U-Net-style DiT.", "Investigating whether downsampling tokens for self-attention in DiT-UNet reduces redundancy and improves performance."], "main_takeaways": ["U-DiTs, the proposed U-shaped diffusion transformers, outperform traditional isotropic DiTs with significantly reduced computation costs.", "Incorporating self-attention with downsampled tokens into DiT-UNet achieves better results on latent U-Net diffusers with reduced computation.", "U-DiT models could outperform DiT-XL/2 with only 1/6 of its computation cost, demonstrating extraordinary performance and scalability.", "The inductive bias of U-Net is not fully leveraged when simply combined with plain transformer blocks.", "The proposed method of token downsampling for self-attention acts as a natural low-pass filter, enhancing performance while reducing computational demands."], "testable_hypotheses": [{"hypothesis": "Does token downsampling in self-attention improve the performance of DiT-UNet compared to full-scale self-attention?", "method": "Conduct experiments comparing DiT-UNet with and without token downsampling in self-attention, measuring performance using FID scores.", "expected_outcome": "Token downsampling will improve performance with reduced computational costs."}, {"hypothesis": "Will U-DiTs outperform isotropic DiTs in terms of computational efficiency and image generation quality?", "method": "Compare U-DiTs with isotropic DiTs across various metrics such as FID, sFID, and IS on image generation tasks.", "expected_outcome": "U-DiTs will demonstrate better performance and efficiency than isotropic DiTs."}, {"hypothesis": "Is the U-Net inductive bias beneficial when combined with downsampled self-attention in latent diffusion transformers?", "method": "Evaluate the performance of U-Net-style DiTs with and without downsampled self-attention on latent diffusion tasks.", "expected_outcome": "U-Net combined with downsampled self-attention will show enhanced performance over standard U-Net or isotropic architectures."}, {"hypothesis": "Can U-DiT models achieve comparable performance to larger isotropic DiT models with substantially fewer FLOPs?", "method": "Scale U-DiT models to various sizes and compare their performance to larger isotropic DiTs.", "expected_outcome": "U-DiT models will outperform larger DiTs with fewer FLOPs."}, {"hypothesis": "Does extending training iterations improve the performance of U-DiT models significantly?", "method": "Train U-DiT models for extended iterations and measure performance improvements over time.", "expected_outcome": "Extended training will lead to consistent improvements in generation quality."}], "follow_up_work_ideas": ["Investigate the potential of applying U-DiT models to other domains such as video generation or 3D image synthesis.", "Explore the integration of token downsampling with other architectures like CNNs to see if similar performance gains can be achieved.", "Conduct in-depth analysis on the impact of different downsampler designs on U-DiT performance and computational efficiency.", "Examine the effects of different positional encoding methods in U-DiT models to further optimize performance.", "Develop methods to dynamically adjust the degree of token downsampling during training to balance performance and computation."]}}
{"id": "94872", "url": "https://nips.cc/virtual/2024/poster/94872", "title": "SGLang: Efficient Execution of Structured Language Model Programs", "authors": [], "abstract": "Abstract:Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to $6.4\\times$ higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/sgl-project/sglang"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": "1x NVIDIA A100", "memory": "16GB", "API_calls": "Yes, requires integration with external APIs"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 11236}, "research_ideas": {"problem_statements": ["How can the efficiency of executing complex language model programs be improved given the lack of efficient systems for programming and executing these applications?", "What are the challenges associated with efficient use of language model programs and how can they be addressed to reduce redundant computation and memory usage?", "How can the reuse of the Key-Value (KV) cache be optimized across multiple generation calls in language model programs?"], "main_takeaways": ["SGLang is introduced as a system for efficient execution of language model programs, consisting of a frontend language and a runtime with optimizations.", "SGLang achieves up to 6.4\u00d7 higher throughput compared to state-of-the-art inference systems across various tasks and models.", "The system introduces RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding.", "SGLang's frontend simplifies programming with primitives for generation and parallelism control, and its runtime accelerates execution with novel optimizations.", "SGLang supports both open-weight models and API-access-only models, demonstrating its versatility across different model types and applications."], "testable_hypotheses": [{"hypothesis": "Does the implementation of RadixAttention improve throughput by optimizing KV cache reuse in language model programs?", "method": "Compare the execution time and throughput of language model programs with and without RadixAttention implemented.", "expected_outcome": "RadixAttention will show a marked improvement in throughput by reducing redundant KV cache computations."}, {"hypothesis": "Can compressed finite state machines significantly decrease decoding time for structured outputs?", "method": "Measure the decoding time for structured outputs with standard finite state machines versus compressed finite state machines.", "expected_outcome": "Compressed finite state machines will decrease decoding time by enabling multi-token decoding."}, {"hypothesis": "Will speculative execution reduce latency and API costs in multi-call SGLang programs using API-access-only models?", "method": "Test speculative execution on multi-call SGLang programs and measure latency and API usage compared to standard execution.", "expected_outcome": "Speculative execution will reduce both latency and API costs by effectively predicting and reusing earlier generation outputs."}, {"hypothesis": "Does the use of the SGLang frontend language simplify the programming of complex workflows involving LLMs?", "method": "Conduct a user study comparing the development time and code complexity for LLM workflows using SGLang versus traditional methods.", "expected_outcome": "SGLang will result in reduced development time and simplified code for complex LLM workflows."}, {"hypothesis": "Can SGLang maintain high throughput with increased model size and complexity?", "method": "Test SGLang's performance on larger models and more complex tasks, such as multi-modal inputs, compared to smaller models.", "expected_outcome": "SGLang will maintain high throughput and efficiency across different model sizes and complexities, demonstrating scalability."}], "follow_up_work_ideas": ["Explore the extension of RadixAttention to operate across multiple levels of the memory hierarchy, such as DRAM and disk.", "Enhance SGLang to support additional output modalities, including audio and video, expanding its applicability.", "Investigate fuzzy semantic matching within RadixAttention to improve cache reuse even when prefixes are not identical.", "Develop higher-level primitives atop SGLang to further simplify complex language model workflows and reduce coding overhead.", "Address potential issues of starvation in cache-aware scheduling to ensure fair resource allocation among requests."]}}
{"id": "94868", "url": "https://nips.cc/virtual/2024/poster/94868", "title": "Pipeline Parallelism with Controllable Memory", "authors": [], "abstract": "Abstract:Pipeline parallelism has been widely explored, but most existing schedules lack a systematic methodology. In this paper, we propose a framework to decompose pipeline schedules as repeating a building block, and show that the lifespan of the building block decides the peak activation memory of the pipeline schedule. Guided by the observations, we find that almost all existing pipeline schedules, to the best of our knowledge, are memory inefficient. To address this, we introduce a family of memory efficient building blocks with controllable activation memory, which can reduce the peak activation memory to 1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable throughput. We can also achieve almost zero pipeline bubbles while maintaining the same activation memory as 1F1B. Our evaluations demonstrate that in pure pipeline parallelism settings, our methods outperform 1F1B by from 7\\% to 55\\% in terms of throughput. When employing a grid search over hybrid parallelism hyperparameters in practical scenarios, our methods demonstrate a 16\\% throughput improvement over the 1F1B baseline for large language models. The implementation is open-sourced at https://github.com/sail-sg/zero-bubble-pipeline-parallelism.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/sail-sg/zero-bubble-pipeline-parallelism"}, "reproduce_difficulty": {"environment_setup": "Moderate difficulty (requires installations via pip or building from source)", "resource_requirements": {"GPUs": "1-8 NVIDIA GPUs (recommended for optimal performance)", "memory": "At least 16GB RAM recommended"}, "time_requirements": "2-4 hours for setup and initial runs", "code_quality": "Well-documented code with examples and usage instructions", "difficulty": 3, "stars": 344}, "research_ideas": {"problem_statements": ["Existing pipeline parallelism schedules are memory inefficient and lack a systematic methodology.", "Pipeline parallelism suffers from large activation memory and pipeline bubbles.", "There is a need for memory-efficient pipeline schedules that maintain or improve throughput."], "main_takeaways": ["The paper proposes a framework to design pipeline schedules using building blocks, which can control the activation memory by adjusting the lifespan of these blocks.", "The proposed V-Shape building blocks can reduce peak activation memory significantly compared to existing methods like 1F1B.", "Experimental results show that the proposed methods outperform existing schedules in terms of throughput and memory efficiency."], "testable_hypotheses": [{"hypothesis": "The V-Shape building blocks can reduce the peak activation memory to 1/2 or 1/3 of 1F1B while maintaining throughput.", "method": "Implement pipeline schedules using V-Shape building blocks and measure peak activation memory and throughput compared to 1F1B.", "expected_outcome": "V-Shape will achieve the described memory reduction and maintain or improve throughput."}, {"hypothesis": "Zero pipeline bubbles can be achieved with V-ZB using the same activation memory as 1F1B.", "method": "Evaluate the V-ZB schedule and compare the pipeline bubble rate and activation memory with 1F1B.", "expected_outcome": "V-ZB will show zero pipeline bubbles without increasing activation memory."}, {"hypothesis": "Increasing the number of microbatches will not lead to increased bubbles for V-Half in most empirical cases.", "method": "Test V-Half with varying numbers of microbatches and measure bubble rates.", "expected_outcome": "V-Half will maintain low bubble rates regardless of the number of microbatches."}, {"hypothesis": "The proposed V-Shape schedules will lie on the Pareto frontier when comparing memory and throughput with other methods.", "method": "Conduct experiments with various pipeline schedules and plot memory vs throughput to identify Pareto efficiency.", "expected_outcome": "V-Shape schedules will be on the Pareto frontier, showing optimal trade-offs."}, {"hypothesis": "The performance of V-Min degrades when F, B, and W times differ significantly.", "method": "Profile the performance of V-Min under varying F, B, W runtimes.", "expected_outcome": "V-Min will show decreased performance with significant differences in runtimes."}], "follow_up_work_ideas": ["Explore more memory-efficient pipeline schedules using continuous offsets or finer-granularity discretization.", "Investigate the application of V-Shape building blocks in other domains such as distributed data processing.", "Develop adaptive schedulers that dynamically adjust building blocks based on real-time memory and throughput demands.", "Extend the proposed methods to support heterogeneous device environments where device capabilities differ.", "Explore integration of V-Shape building blocks with other parallelism strategies like model parallelism."]}}
{"id": "94514", "url": "https://nips.cc/virtual/2024/poster/94514", "title": "Don't Look Twice: Faster Video Transformers with Run-Length Tokenization", "authors": [], "abstract": "Abstract:Video transformers are slow to train due to extremely large numbers of input tokens, even though many video tokens are repeated over time. Existing methods to remove uninformative tokens either have significant overhead, negating any speedup, or require tuning for different datasets and examples. We present Run-Length Tokenization (RLT), a simple approach to speed up video transformers inspired by run-length encoding for data compression. RLT efficiently finds and removes `runs' of patches that are repeated over time before model inference, then replaces them with a single patch and a positional encoding to represent the resulting token's new length. Our method is content-aware, requiring no tuning for different datasets, and fast, incurring negligible overhead. RLT yields a large speedup in training, reducing the wall-clock time to fine-tune a video transformer by 30% while matching baseline model performance. RLT also works without training, increasing model throughput by 35% with only 0.1% drop in accuracy.RLT speeds up training at 30 FPS by more than 100%, and on longer video datasets, can reduce the token count by up to 80\\%. Our project page is at  rccchoudhury.github.io/projects/rlt.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/rccchoudhury/RLT"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "datasets": ["Kinetics-400", "Something-Something V2"]}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 195}, "research_ideas": {"problem_statements": ["Training transformers on videos is slow due to the large number of input tokens, many of which are redundant.", "Existing methods to remove redundant video tokens either have significant overhead or require dataset-specific tuning.", "There is a need for a content-aware tokenization method to efficiently reduce the number of input tokens based on video content."], "main_takeaways": ["Run-Length Tokenization (RLT) is proposed to speed up video transformers by efficiently removing redundant video tokens.", "RLT reduces the wall-clock time to fine-tune a video transformer by 30% while maintaining baseline performance.", "RLT can increase model throughput by 35% with only a 0.1% drop in accuracy, and can reduce token count by up to 80% in longer video datasets.", "RLT is content-aware and requires no tuning for different datasets, working effectively without incurring significant overhead."], "testable_hypotheses": [{"hypothesis": "RLT will reduce training time for video transformers by at least 30% compared to standard tokenization.", "method": "Compare the training time of video transformers using RLT against those using standard tokenization on a set of benchmark datasets.", "expected_outcome": "RLT reduces training time by approximately 30% as shown in the experiments."}, {"hypothesis": "Using RLT will maintain baseline model performance with a negligible drop in accuracy.", "method": "Evaluate the accuracy of video transformers using RLT and compare it to the baseline performance on standard datasets.", "expected_outcome": "RLT maintains performance with only a 0.1% drop in accuracy."}, {"hypothesis": "RLT will work effectively without any training, providing a significant increase in throughput.", "method": "Implement RLT as a drop-in replacement during inference and measure throughput improvements.", "expected_outcome": "RLT increases throughput by 35% during inference."}, {"hypothesis": "RLT will effectively reduce token count by up to 80% in datasets with longer videos or higher FPS.", "method": "Apply RLT to datasets with varying video lengths and FPS, and measure the reduction in token count.", "expected_outcome": "RLT reduces token count by up to 80% in longer video datasets."}, {"hypothesis": "RLT can be combined with random masking or other token pruning strategies to further improve training speed without degrading performance.", "method": "Evaluate the combined effect of RLT and random masking on both training speed and model performance.", "expected_outcome": "Combining RLT with random masking yields further speed improvements while maintaining performance."}], "follow_up_work_ideas": ["Explore the application of RLT to other domains such as video generation or dense vision tasks.", "Investigate adaptive thresholds for RLT to improve its robustness in scenarios with significant camera motion.", "Develop methods to handle dynamically changing input sizes more efficiently during both training and inference.", "Study the integration of RLT with other transformer architectures to assess its general applicability.", "Analyze the impact of RLT on model interpretability and the ability to capture fine-grained motion details."]}}
{"id": "94482", "url": "https://nips.cc/virtual/2024/poster/94482", "title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos", "authors": [], "abstract": "Abstract:We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designedtoken, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/showlab/VideoLISA"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "A10", "amount": 64}, "memory": "Not specified", "API_call": "Yes, requires Hugging Face API for transformers"}, "time_requirements": 5, "code_quality": "Well-documented code", "difficulty": 3, "stars": 105}, "research_ideas": {"problem_statements": ["Existing image-based methods like LISA struggle with video tasks due to the additional temporal dimension, requiring consistent segmentation across frames.", "The challenge in video object segmentation (VOS) stems from the need to capture and comprehend temporal dynamics in videos and predict temporally consistent segmentation masks.", "There is a need for models that can perform language-instructed reasoning segmentation in videos, which requires scene understanding, temporal comprehension, and implicit reasoning.", "Current methods lack the ability to segment and track objects across multiple frames consistently."], "main_takeaways": ["VideoLISA is a multimodal large language model designed for language-instructed reasoning segmentation in videos.", "The Sparse Dense Sampling strategy balances temporal context and spatial detail within computational constraints.", "The One-Token-Seg-All approach uses a <TRK> token to achieve temporal consistency in segmentation across multiple frames.", "VideoLISA outperforms existing methods on various video object segmentation benchmarks, showing strong performance in complex reasoning and temporal understanding tasks."], "testable_hypotheses": [{"hypothesis": "Sparse Dense Sampling strategy improves temporal context understanding without losing spatial detail.", "method": "Compare segmentation accuracy of VideoLISA with and without the Sparse Dense Sampling strategy on video benchmarks.", "expected_outcome": "VideoLISA with Sparse Dense Sampling will show improved temporal context understanding and segmentation accuracy."}, {"hypothesis": "The One-Token-Seg-All approach provides better temporal consistency than using separate tokens for each frame.", "method": "Evaluate segmentation consistency across frames using the One-Token-Seg-All approach compared to a baseline with separate tokens.", "expected_outcome": "One-Token-Seg-All will show higher temporal consistency in segmentation results."}, {"hypothesis": "VideoLISA can generalize well to image segmentation tasks despite being optimized for videos.", "method": "Test VideoLISA on standard image segmentation benchmarks and compare performance with image-optimized models.", "expected_outcome": "VideoLISA will perform comparably to image-optimized models, demonstrating good generalization to image tasks."}, {"hypothesis": "A single <TRK> token can effectively segment and track objects across an entire video.", "method": "Use the <TRK> token to track objects across multiple frames and evaluate tracking accuracy.", "expected_outcome": "The <TRK> token will maintain high tracking accuracy across frames."}, {"hypothesis": "VideoLISA's reasoning capabilities improve segmentation accuracy in scenarios requiring complex reasoning.", "method": "Evaluate VideoLISA's performance on newly introduced ReasonVOS benchmark focusing on complex reasoning.", "expected_outcome": "VideoLISA will outperform other models on the ReasonVOS benchmark due to its enhanced reasoning capabilities."}], "follow_up_work_ideas": ["Explore integrating a dedicated video backbone to enhance temporal dynamics understanding in VideoLISA.", "Investigate methods to reduce computational costs while maintaining the reasoning and segmentation capabilities of VideoLISA.", "Develop strategies to preserve text generation capabilities while enhancing segmentation performance in multimodal models.", "Apply VideoLISA's approach to other domains requiring temporal consistency, such as action recognition or event detection in videos.", "Enhance the reasoning capabilities of VideoLISA by incorporating additional world knowledge databases to support more complex queries."]}}
{"id": "93038", "url": "https://nips.cc/virtual/2024/poster/93038", "title": "Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving", "authors": [], "abstract": "Abstract:Autonomous driving has advanced significantly due to sensors, machine learning, and artificial intelligence improvements. However, prevailing methods struggle with intricate scenarios and causal relationships, hindering adaptability and interpretability in varied environments. To address the above problems, we introduce LeapAD, a novel paradigm for autonomous driving inspired by the human cognitive process. Specifically, LeapAD emulates human attention by selecting critical objects relevant to driving decisions, simplifying environmental interpretation, and mitigating decision-making complexities. Additionally, LeapAD incorporates an innovative dual-process decision-making module, which consists of an Analytic Process (System-II) for thorough analysis and reasoning, along with a Heuristic Process (System-I) for swift and empirical processing. The Analytic Process leverages its logical reasoning to accumulate linguistic driving experience, which is then transferred to the Heuristic Process by supervised fine-tuning. Through reflection mechanisms and a growing memory bank, LeapAD continuously improves itself from past mistakes in a closed-loop environment. Closed-loop testing in CARLA shows that LeapAD outperforms all methods relying solely on camera input, requiring 1-2 orders of magnitude less labeled data. Experiments also demonstrate that as the memory bank expands, the Heuristic Process with only 1.8B parameters can inherit the knowledge from a GPT-4 powered Analytic Process and achieve continuous performance improvement. Project page: https://pjlab-adg.github.io/LeapAD", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/pjlab-adg/LeapAD"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires Docker and a mid-end GPU)", "resource_requirements": "1 mid-end GPU, 8GB RAM minimum, CARLA simulator", "time_requirements": "Approximately 2-4 hours to set up and run main commands", "code_quality": "Well-documented code with detailed installation and usage instructions", "difficulty": 3, "stars": 115}, "research_ideas": {"problem_statements": ["Prevailing autonomous driving methods struggle with intricate scenarios and causal relationships, hindering adaptability and interpretability in varied environments.", "There is a need for a system capable of reasoning about unseen scenarios and utilizing knowledge in a human cognition manner.", "Existing knowledge-based methods in autonomous driving perform open-loop testing, failing to reflect the dynamic interactions between ego car and the real-world environment.", "The need to develop autonomous driving systems that can continuously interact, learn, and adapt in closed-loop environments, similar to human drivers."], "main_takeaways": ["LeapAD introduces a dual-process approach for autonomous driving inspired by human cognitive processes, enhancing decision-making and adaptability.", "LeapAD uses a scene understanding module that focuses on critical objects, simplifying environmental descriptions and decision-making processes.", "The dual-process decision-making module integrates an Analytic Process for rational reasoning and a Heuristic Process for quick, empirical decision-making.", "LeapAD demonstrates superior performance in closed-loop driving tests in CARLA, outperforming other methods using significantly less labeled data.", "The system leverages a reflection mechanism and a memory bank to continuously learn and improve from past experiences."], "testable_hypotheses": [{"hypothesis": "Does the dual-process decision-making module improve driving performance over single-process methods?", "method": "Conduct experiments in CARLA comparing LeapAD to single-process methods under similar conditions.", "expected_outcome": "LeapAD will show improved adaptability and performance due to the dual-process approach."}, {"hypothesis": "Can the Heuristic Process maintain performance improvements with minimal labeled data compared to larger models?", "method": "Test the Heuristic Process's performance with varying amounts of labeled data and compare it to larger models like GPT-4.", "expected_outcome": "The Heuristic Process will perform comparably to larger models but with significantly less data."}, {"hypothesis": "Does the reflection mechanism contribute to continuous improvement in decision-making performance?", "method": "Evaluate the performance of LeapAD with and without the reflection mechanism over multiple driving scenarios.", "expected_outcome": "LeapAD with the reflection mechanism will show continuous improvement in decision-making accuracy."}, {"hypothesis": "Is the memory bank effective in transferring knowledge across different driving scenarios?", "method": "Test LeapAD's performance in new scenarios after training with a memory bank from different environments.", "expected_outcome": "LeapAD will demonstrate effective knowledge transfer and adaptability to new environments."}, {"hypothesis": "Does the scene understanding module improve decision-making by focusing on critical objects?", "method": "Compare LeapAD's decision-making accuracy with and without the scene understanding module in complex scenarios.", "expected_outcome": "The module will enhance decision-making accuracy by reducing cognitive load and focusing on important elements."}], "follow_up_work_ideas": ["Explore integrating temporal information into LeapAD's decision-making processes to improve performance in dynamic scenarios.", "Investigate the application of LeapAD's dual-process system to other domains where continuous learning and adaptability are critical.", "Develop a high-fidelity world simulator to bridge the gap between simulation and real-world scenarios, enhancing LeapAD's applicability.", "Enhance the VLM's participation in the reflection mechanism to further improve LeapAD's learning and decision-making capabilities.", "Experiment with different configurations of the memory bank to optimize the balance between memory size and performance improvement."]}}
{"id": "94431", "url": "https://nips.cc/virtual/2024/poster/94431", "title": "Depth Anything V2", "authors": [], "abstract": "Abstract:This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with sparse depth annotations to facilitate future research. Models are available at https://github.com/DepthAnything/Depth-Anything-V2.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/DepthAnything/Depth-Anything-V2"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "8GB", "api_calls": "None specified"}, "time_requirements": "1", "code_quality": "Well-documented code", "difficulty": 3, "stars": 4755}, "research_ideas": {"problem_statements": ["How to build a more capable monocular depth estimation model that produces robust predictions for complex scenes, fine-grained details, varied model scales, and strong generalization capability?", "Can synthetic images replace real labeled images for training monocular depth estimation models?", "How to address the distribution shift and limited diversity of synthetic images when used for training monocular depth estimation models?", "What role do large-scale unlabeled real images play in improving monocular depth estimation models?"], "main_takeaways": ["Depth Anything V2 significantly outperforms its predecessor in robustness and fine-grained details.", "Replacing all labeled real images with synthetic images results in finer and more robust depth predictions.", "Using large-scale pseudo-labeled real images bridges the domain gap and enhances the scene coverage.", "The model offers varied scales, achieving more than 10x faster inference and higher accuracy compared to Stable Diffusion-based models.", "A versatile evaluation benchmark, DA-2K, was constructed to facilitate future research with precise annotations and diverse scenes."], "testable_hypotheses": [{"hypothesis": "Using synthetic images for training can produce finer depth maps than using real labeled images.", "method": "Train depth estimation models on synthetic images and real labeled images separately, then compare the output depth map details.", "expected_outcome": "Synthetic images result in finer and more precise depth details."}, {"hypothesis": "Large-scale unlabeled real images can improve model generalization to real-world scenes.", "method": "Train models on synthetic data, with and without pseudo-labeled real images, and evaluate on diverse real-world test sets.", "expected_outcome": "Models trained with pseudo-labeled real images show improved generalization."}, {"hypothesis": "Pseudo-labeled real images can bridge the domain gap between synthetic and real images.", "method": "Evaluate model performance on real images after training on synthetic images alone and with additional pseudo-labeled real images.", "expected_outcome": "Pseudo-labeled real images help in bridging the domain gap, improving performance."}, {"hypothesis": "Scaling up the teacher model improves the quality of pseudo labels.", "method": "Compare the quality of pseudo labels generated by teacher models of different sizes.", "expected_outcome": "Larger teacher models produce more accurate pseudo labels."}, {"hypothesis": "A model trained on synthetic images and pseudo-labeled real images can achieve better performance on a new benchmark with diverse scenarios.", "method": "Train models on the proposed data pipeline and evaluate on the DA-2K benchmark.", "expected_outcome": "The model shows high accuracy across diverse scenarios in DA-2K."}], "follow_up_work_ideas": ["Investigate the use of synthetic data from more diverse sources to improve training diversity and robustness.", "Develop methods to further reduce the computational burden of training with large-scale unlabeled datasets.", "Explore the integration of additional modalities, such as semantic segmentation, to enhance depth estimation accuracy.", "Test the applicability of the proposed methods in other domains, such as video depth estimation and 3D scene reconstruction.", "Develop techniques to automatically identify and filter out noisy pseudo-labels during training."]}}
{"id": "93338", "url": "https://nips.cc/virtual/2024/poster/93338", "title": "An Image is Worth 32 Tokens for Reconstruction and Generation", "authors": [], "abstract": "Abstract:Recent advancements in generative models have highlighted the crucial role of image tokenization in the efficient synthesis of high-resolution images. Tokenization, which transforms images into latent representations, reduces computational demands compared to directly processing pixels and enhances the effectiveness and efficiency of the generation process. Prior methods, such as VQGAN, typically utilize 2D latent grids with fixed downsampling factors. However, these 2D tokenizations face challenges in managing the inherent redundancies present in images, where adjacent regions frequently display similarities. To overcome this issue, we introduceTransformer-based 1-DimensionalTokenizer (TiTok), an innovative approach that tokenizes images into 1D latent sequences. TiTok provides a more compact latent representation, yielding substantially more efficient and effective representations than conventional techniques. For example, a 256 \u00d7 256 \u00d7 3 image can be reduced to just32discrete tokens, a significant reduction from the 256 or 1024 tokens obtained by prior methods. Despite its compact nature, TiTok achieves competitive performance to state-of-the-art approaches. Specifically, using the same generator framework, TiTok attains1.97gFID, outperforming MaskGIT baseline significantly by 4.21 at ImageNet 256 \u00d7 256 benchmark. The advantages of TiTok become even more significant when it comes to higher resolution. At ImageNet 512 \u00d7 512 benchmark, TiTok not only outperforms state-of-the-art diffusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image tokens by 64\u00d7, leading to410\u00d7 fastergeneration process. Our best-performing variant can significantly surpasses DiT-XL/2 (gFID2.13vs. 3.04) while still generating high-quality samples74\u00d7 faster. Codes and models are available at https://github.com/bytedance/1d-tokenizer", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/bytedance/1d-tokenizer"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 NVIDIA GPU (minimum)", "memory": "16GB RAM", "API_calls": "None specified"}, "time_requirements": "2 hours", "code_quality": "well-documented code", "difficulty": 2, "stars": 698}, "research_ideas": {"problem_statements": ["How can a 1D tokenizer like TiTok effectively address the redundancy in image tokenization compared to 2D methods?", "Is the 2D structure necessary for image tokenization, or can a 1D sequence provide a more compact and efficient representation?", "Can a significantly reduced number of tokens (e.g., 32 tokens) still achieve competitive performance in image reconstruction and generation?"], "main_takeaways": ["TiTok tokenizes images into 1D sequences, significantly reducing the number of tokens needed compared to traditional 2D methods while maintaining competitive performance.", "The 1D tokenization breaks the grid constraints of 2D methods, enabling a more flexible and semantically rich representation.", "TiTok achieves competitive generative quality with significantly fewer tokens, leading to faster training and inference compared to state-of-the-art methods.", "The model size can be scaled to enable more compact latent representations without compromising performance."], "testable_hypotheses": [{"hypothesis": "Does the use of 32 tokens in TiTok provide comparable image reconstruction quality to 256 tokens in VQGAN?", "method": "Conduct image reconstruction experiments using both TiTok with 32 tokens and VQGAN with 256 tokens, comparing the reconstruction fidelity.", "expected_outcome": "TiTok with 32 tokens will achieve comparable reconstruction quality to VQGAN with 256 tokens."}, {"hypothesis": "Can scaling up the tokenizer model size improve performance with fewer latent tokens?", "method": "Experiment with different tokenizer model sizes (small, base, large) while keeping the number of tokens constant, measuring reconstruction and generation performance.", "expected_outcome": "Larger tokenizer models will show improved performance even with fewer latent tokens."}, {"hypothesis": "Does 1D tokenization lead to faster training and inference times compared to 2D tokenization?", "method": "Measure the training and inference throughput of TiTok and compare it with a 2D tokenization method across various tasks.", "expected_outcome": "1D tokenization will result in significantly faster training and inference times."}, {"hypothesis": "Can TiTok's compact 1D tokens improve the generative model's training efficiency?", "method": "Evaluate the generative model's training efficiency using TiTok's 1D tokens and compare it to models trained with traditional 2D tokens.", "expected_outcome": "TiTok's 1D tokens will enhance training efficiency, leading to faster convergence."}, {"hypothesis": "Does the flexibility of 1D tokenization allow for more semantically rich image representations?", "method": "Perform a linear probing task on the features learned by TiTok's 1D encoder, comparing the semantic richness to a standard 2D encoder.", "expected_outcome": "TiTok's 1D tokenization will learn more semantically rich representations."}], "follow_up_work_ideas": ["Explore the application of TiTok's 1D tokenization in video generation to see if similar efficiencies can be achieved.", "Investigate the performance of TiTok when integrated with other generative frameworks beyond MaskGIT, such as diffusion models.", "Assess the impact of training TiTok on larger and more diverse datasets to improve its generalization capabilities.", "Develop advanced masking and sampling strategies specifically optimized for TiTok's 1D tokens during generative tasks.", "Explore hybrid tokenization approaches that combine 1D and 2D features to potentially leverage the strengths of both."]}}
{"id": "94539", "url": "https://nips.cc/virtual/2024/poster/94539", "title": "SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection", "authors": [], "abstract": "Abstract:Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR object detection. The dataset and code is available at \\url{https://github.com/zcablii/SARDet_100K}.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/zcablii/SARDet_100K"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_calls": "None"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 469}, "research_ideas": {"problem_statements": ["Limited public datasets and inaccessible source code in SAR object detection research.", "Significant domain and model gap when transferring from RGB datasets to SAR datasets.", "High costs and sensitivity associated with annotating high-resolution SAR images."], "main_takeaways": ["Introduction of SARDet-100K, the first large-scale multi-class SAR object detection dataset.", "Proposal of the Multi-Stage with Filter Augmentation (MSFA) pretraining framework to bridge domain and model gaps.", "SARDet-100K dataset and MSFA framework significantly improve SAR object detection performance."], "testable_hypotheses": [{"hypothesis": "Does the MSFA framework outperform traditional RGB pretraining methods on SAR datasets?", "method": "Compare the performance of models pretrained using MSFA versus traditional RGB pretraining on SARDet-100K.", "expected_outcome": "MSFA will outperform traditional pretraining methods."}, {"hypothesis": "Can the WST feature improve SAR object detection performance compared to other handcrafted features?", "method": "Test detection models using different handcrafted features as input, including WST, on SARDet-100K.", "expected_outcome": "WST will enhance detection performance more than other features."}, {"hypothesis": "Does the scale of pretraining datasets affect the performance of SAR object detection models?", "method": "Compare SAR detection performance using models pretrained on different scales of optical remote sensing datasets.", "expected_outcome": "Larger pretraining datasets like DOTA will lead to better performance."}, {"hypothesis": "Can MSFA reduce the domain gap between RGB and SAR images more effectively than other domain adaptation techniques?", "method": "Evaluate domain adaptation performance by comparing MSFA with other techniques using domain gap metrics.", "expected_outcome": "MSFA will show a greater reduction in domain gap."}, {"hypothesis": "Does multi-stage pretraining provide significant performance improvements across different neural network architectures?", "method": "Test the generalizability of MSFA across various models and architectures on SARDet-100K.", "expected_outcome": "MSFA will demonstrate consistent performance improvements across architectures."}], "follow_up_work_ideas": ["Explore semi-supervised or unsupervised learning for domain transfer in SAR object detection.", "Investigate the integration of MSFA with other advanced SAR detection models.", "Apply the MSFA framework to other remote sensing tasks beyond SAR object detection.", "Develop more detailed designs within MSFA to enhance performance and capabilities.", "Examine scalability and efficiency improvements for large-scale SAR image processing."]}}
{"id": "95036", "url": "https://nips.cc/virtual/2024/poster/95036", "title": "One-Step Effective Diffusion Network for Real-World  Image Super-Resolution", "authors": [], "abstract": "Abstract:The pre-trained text-to-image diffusion models have been increasingly employed to tackle the real-world image super-resolution (Real-ISR) problem due to their powerful generative image priors. Most of the existing methods start from random noise to reconstruct the high-quality (HQ) image under the guidance of the given low-quality (LQ) image. While promising results have been achieved, such Real-ISR methods require multiple diffusion steps to reproduce the HQ image, increasing the computational cost. Meanwhile, the random noise introduces uncertainty in the output, which is unfriendly to image restoration tasks. To address these issues, we propose a one-step effective diffusion network, namely OSEDiff, for the Real-ISR problem. We argue that the LQ image contains rich information to restore its HQ counterpart, and hence the given LQ image can be directly taken as the starting point for diffusion, eliminating the uncertainty introduced by random noise sampling. We finetune the pre-trained diffusion network with trainable layers to adapt it to complex image degradations. To ensure that the one-step diffusion model could yield HQ Real-ISR output, we apply variational score distillation in the latent space to conduct KL-divergence regularization. As a result, our OSEDiff model can efficiently and effectively generate HQ images in just one diffusion step. Our experiments demonstrate that OSEDiff achieves comparable or even better Real-ISR results, in terms of both objective metrics and subjective evaluations, than previous diffusion model-based Real-ISR methods that require dozens or hundreds of steps. The source codes are released at https://github.com/cswry/OSEDiff.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/cswry/OSEDiff"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA A100", "amount": 4}, "memory": "16GB", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 333}, "research_ideas": {"problem_statements": ["Existing Real-ISR methods require multiple diffusion steps to reproduce high-quality images, increasing computational cost and introducing uncertainty due to random noise.", "There is a need to ensure that restored images follow the distribution of high-quality natural images.", "How to effectively use the rich information in low-quality images to restore high-quality images without introducing randomness?"], "main_takeaways": ["The proposed OSEDiff model can generate high-quality images in just one diffusion step, significantly reducing computational cost.", "OSEDiff uses the low-quality image as the starting point for diffusion, eliminating the randomness introduced by random noise.", "The model is fine-tuned with trainable layers to adapt to complex image degradations, and uses variational score distillation for KL-divergence regularization.", "OSEDiff achieves comparable or superior Real-ISR results compared to existing methods requiring multiple diffusion steps."], "testable_hypotheses": [{"hypothesis": "Does starting the diffusion process from the low-quality image itself reduce randomness in the output?", "method": "Compare the variability in outputs when starting diffusion from random noise versus starting from the low-quality image.", "expected_outcome": "Starting from the low-quality image reduces randomness in the output."}, {"hypothesis": "Will variational score distillation improve the perceptual quality of the generated images?", "method": "Evaluate the perceptual quality of images with and without using variational score distillation.", "expected_outcome": "Variational score distillation improves the perceptual quality of the generated images."}, {"hypothesis": "Can the one-step diffusion model produce high-quality images comparable to multi-step models?", "method": "Compare the quality of images produced by one-step and multi-step diffusion models using objective metrics.", "expected_outcome": "The one-step model produces images of comparable quality to multi-step models."}, {"hypothesis": "Does fine-tuning with trainable layers improve adaptation to complex image degradations?", "method": "Test the model on various image degradations with and without fine-tuning.", "expected_outcome": "Fine-tuning with trainable layers improves adaptation to complex image degradations."}, {"hypothesis": "Will using text prompts extracted from low-quality images enhance image generation quality?", "method": "Compare image quality with and without using text prompts extracted from low-quality images.", "expected_outcome": "Using text prompts enhances image generation quality."}], "follow_up_work_ideas": ["Explore the integration of other types of image priors to further improve detail generation.", "Investigate the reconstruction of fine-scale structures such as small scene texts using the OSEDiff framework.", "Apply OSEDiff to other image restoration tasks beyond super-resolution, such as denoising or deblurring.", "Examine the effect of different types of trainable layers and architectures on the performance of the model.", "Study the impact of varying the rank of LoRA layers in different parts of the network on the efficiency and quality of results."]}}
{"id": "95524", "url": "https://nips.cc/virtual/2024/poster/95524", "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering", "authors": [], "abstract": "Abstract:Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our \\textit{G-Retriever} method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, \\textit{G-Retriever} performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/G-Retriever}}", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/XiaoxinHe/G-Retriever"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": "1 x A GPU with CUDA support (e.g., NVIDIA)", "memory": "At least 16 GB RAM", "additional_requirements": "Hugging Face account for model access"}, "time_requirements": 3, "code_quality": "Well-documented", "difficulty": 3, "stars": 409}, "research_ideas": {"problem_statements": ["How can we enable users to interact with graphs containing textual attributes in a conversational interface?", "What are the limitations of existing methods that combine large language models (LLMs) and graph neural networks (GNNs) for understanding real-world textual graphs?", "How can we mitigate hallucination in graph LLMs while handling large textual graphs that exceed an LLM's context window size?", "What is needed to create a comprehensive benchmark for graph question answering (GraphQA) that addresses real-world applications?"], "main_takeaways": ["G-Retriever is a new framework that allows users to interact with real-world textual graphs via a conversational interface, enhancing graph understanding.", "The proposed method uses retrieval-augmented generation (RAG) to mitigate hallucination and efficiently scale to large graphs.", "A new GraphQA benchmark is introduced, focusing on diverse, real-world applications for graph question answering.", "Empirical evaluations show that G-Retriever outperforms existing baselines, scales well with larger graphs, and reduces hallucination."], "testable_hypotheses": [{"hypothesis": "Does retrieval-augmented generation (RAG) reduce hallucination in graph LLMs?", "method": "Compare the hallucination frequency in LLMs using RAG against those that do not use RAG on a graph QA task.", "expected_outcome": "RAG will significantly reduce hallucination compared to non-RAG methods."}, {"hypothesis": "Can G-Retriever outperform baseline models across multiple domains in GraphQA tasks?", "method": "Evaluate G-Retriever and baseline models on GraphQA tasks using the introduced benchmark datasets.", "expected_outcome": "G-Retriever will achieve higher performance metrics than baseline models."}, {"hypothesis": "Does employing the Prize-Collecting Steiner Tree (PCST) optimization improve retrieval efficiency in large graphs?", "method": "Compare retrieval efficiency and accuracy of G-Retriever with and without PCST optimization on large graph datasets.", "expected_outcome": "PCST optimization will improve retrieval efficiency and accuracy."}, {"hypothesis": "Is G-Retriever's performance robust to variations in graph encoder architectures?", "method": "Test G-Retriever with different graph encoders (e.g., GCN, GAT) and compare performance on GraphQA tasks.", "expected_outcome": "G-Retriever will maintain competitive performance across different graph encoders."}, {"hypothesis": "Will increasing the size of the LLM used in G-Retriever enhance its performance on GraphQA tasks?", "method": "Evaluate the performance of G-Retriever using LLMs of different sizes (e.g., Llama2-7b vs. Llama2-13b) on the benchmark datasets.", "expected_outcome": "Larger LLMs will lead to improved performance of G-Retriever."}], "follow_up_work_ideas": ["Explore dynamic retrieval methods that adaptively learn to retrieve graph segments based on query characteristics.", "Apply G-Retriever to other types of structured data such as tabular or multi-modal datasets for broader applicability.", "Investigate the integration of other advanced graph algorithms to further enhance retrieval efficiency and accuracy.", "Develop methods to automatically tune the parameters (e.g., k in k-nearest neighbors) based on graph characteristics to optimize performance.", "Examine the use of G-Retriever in real-time applications where graph data is continuously updated and requires immediate processing."]}}
{"id": "95851", "url": "https://nips.cc/virtual/2024/poster/95851", "title": "DOGS: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction  Via Gaussian Consensus", "authors": [], "abstract": "Abstract:The recent advances in 3D Gaussian Splatting (3DGS) show promising results on the novel view synthesis (NVS) task. With its superior rendering performance and high-fidelity rendering quality, 3DGS is excelling at its previous NeRF counterparts. The most recent 3DGS method focuses either on improving the instability of rendering efficiency or reducing the model size. On the other hand, the training efficiency of 3DGS on large-scale scenes has not gained much attention. In this work, we propose DoGaussian, a method that trains 3DGS distributedly. Our method first decomposes a scene into $K$ blocks and then introduces the Alternating Direction Method of Multipliers (ADMM) into the training procedure of 3DGS. During training, our DoGaussian maintains one global 3DGS model on the master node and $K$ local 3DGS models on the slave nodes. The $K$ local 3DGS models are dropped after training and we only query the global 3DGS model during inference. The training time is reduced by scene decomposition, and the training convergence and stability are guaranteed through the consensus on the shared 3D Gaussians. Our method accelerates the training of 3DGS by $6+$ times when evaluated on large-scale scenes while concurrently achieving state-of-the-art rendering quality. Our code is publicly available at [https://github.com/AIBluefisher/DOGS](https://github.com/AIBluefisher/DOGS).", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/AIBluefisher/DOGS"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "storage": "Minimum 10GB free space required for datasets"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 195}, "research_ideas": {"problem_statements": ["The training efficiency of 3D Gaussian Splatting (3DGS) on large-scale scenes has not gained much attention.", "3DGS often requires larger memory during training compared to NeRF methods, especially for larger scenes.", "The training of a huge number of 3D Gaussians on larger scenes leads to longer training time.", "Dispatching 3D Gaussians into different compute nodes is difficult due to the customized rasterization procedure of 3DGS.", "Existing methods require querying multiple sub-models during inference, which slows down rendering efficiency."], "main_takeaways": ["DOGS accelerates the training of 3DGS by over 6 times on large-scale scenes while achieving state-of-the-art rendering quality.", "DOGS decomposes a scene into blocks and introduces the Alternating Direction Method of Multipliers (ADMM) for distributed training.", "During inference, only a global 3DGS model is queried, maintaining efficiency.", "The method ensures training convergence and stability through consensus on shared 3D Gaussians.", "DOGS demonstrates superior performance on large-scale datasets like Mill19 and UrbanScene3D compared to state-of-the-art methods."], "testable_hypotheses": [{"hypothesis": "Does splitting scenes into balanced blocks reduce memory usage during training?", "method": "Measure memory usage while training with and without scene splitting.", "expected_outcome": "Splitting scenes into balanced blocks reduces memory usage."}, {"hypothesis": "Does DOGS improve training time compared to the original 3DGS?", "method": "Compare training durations of DOGS and original 3DGS on large-scale datasets.", "expected_outcome": "DOGS reduces training time by over 6 times."}, {"hypothesis": "Does the use of ADMM ensure convergence to a global 3DGS model?", "method": "Analyze convergence metrics during training using ADMM versus without.", "expected_outcome": "ADMM ensures convergence to a consistent global model."}, {"hypothesis": "Does the consensus step improve rendering quality?", "method": "Evaluate rendering quality with and without the consensus step using metrics like PSNR.", "expected_outcome": "Consensus step improves rendering quality."}, {"hypothesis": "Can DOGS maintain rendering performance with just the global model during inference?", "method": "Compare rendering performance using only the global model versus multiple sub-models.", "expected_outcome": "DOGS maintains rendering performance using only the global model."}], "follow_up_work_ideas": ["Investigate the integration of level-of-detail techniques to reduce the number of 3D Gaussians and memory usage.", "Explore the application of DOGS to different types of large-scale datasets beyond urban scenes.", "Enhance the data transmission efficiency between nodes to further reduce training time.", "Investigate the impact of different scene splitting algorithms on training efficiency and rendering quality.", "Extend the DOGS methodology to incorporate real-time adjustments during dynamic scene changes."]}}
{"id": "95975", "url": "https://nips.cc/virtual/2024/poster/95975", "title": "AutoTimes: Autoregressive Time Series Forecasters via Large Language Models", "authors": [], "abstract": "Abstract:Foundation models of time series have not been fully developed due to the limited availability of time series corpora and the underexploration of scalable pre-training. Based on the similar sequential formulation of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, the inherent autoregressive property and decoder-only architecture of LLMs have not been fully considered, resulting in insufficient utilization of LLM abilities. To fully revitalize the general-purpose token transition and multi-step generation capability of large language models, we propose AutoTimes to repurpose LLMs as autoregressive time series forecasters, which projects time series into the embedding space of language tokens and autoregressively generates future predictions with arbitrary lengths. Compatible with any decoder-only LLMs, the consequent forecaster exhibits the flexibility of the lookback length and scalability with larger LLMs. Further, we formulate time series as prompts, extending the context for prediction beyond the lookback window, termed in-context forecasting. By introducing LLM-embedded textual timestamps, AutoTimes can utilize chronological information to align multivariate time series. Empirically, AutoTimes achieves state-of-the-art with 0.1% trainable parameters and over $5\\times$ training/inference speedup compared to advanced LLM-based forecasters. Code is available at this repository: https://github.com/thuml/AutoTimes.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/thuml/AutoTimes"}, "reproduce_difficulty": {"environment_setup": "Moderate difficulty; requires installing dependencies via pip and downloading models from Hugging Face.", "resource_requirements": "1 GPU (recommended RTX 3090 with 24GB), memory sufficient for model and dataset, and access to large language models (LLAMA-7B preferred).", "time_requirements": "Approximately 15 minutes to run the main commands after setup.", "code_quality": "Well-documented code with clear instructions in the README.", "difficulty": 3, "stars": 156}, "research_ideas": {"problem_statements": ["Limited availability of large-scale time series pre-training datasets hampers the development of foundation models for time series.", "Existing LLM4TS methods do not fully utilize the autoregressive capabilities of large language models.", "There is a need for a general-purpose time series forecasting model that can handle variable-length predictions and multimodal inputs."], "main_takeaways": ["AutoTimes adapts large language models as autoregressive time series forecasters, utilizing their inherent token transition capabilities.", "The proposed approach achieves state-of-the-art performance with a significant reduction in trainable parameters and faster training/inference speeds.", "AutoTimes introduces 'in-context forecasting,' where time series can be self-prompted by relevant contexts to improve forecasting performance.", "LLM-embedded textual timestamps are used to align multivariate time series, enhancing chronological awareness in predictions."], "testable_hypotheses": [{"hypothesis": "Does the autoregressive structure of LLMs improve multi-step time series forecasting performance compared to non-autoregressive approaches?", "method": "Compare forecasting performance of AutoTimes with non-autoregressive LLM4TS methods on benchmark datasets.", "expected_outcome": "AutoTimes will outperform non-autoregressive methods due to better utilization of LLM's autoregressive capabilities."}, {"hypothesis": "Can the use of LLM-embedded textual timestamps improve the alignment and forecasting accuracy of multivariate time series?", "method": "Evaluate forecasting accuracy with and without LLM-embedded textual timestamps on multivariate datasets.", "expected_outcome": "The use of LLM-embedded timestamps will show improved alignment and forecasting accuracy."}, {"hypothesis": "Does in-context forecasting with time series prompts enhance prediction accuracy over traditional lookback methods?", "method": "Conduct experiments comparing in-context forecasting with traditional methods using the same datasets and prediction tasks.", "expected_outcome": "In-context forecasting will show improved prediction accuracy due to the enriched context from time series prompts."}, {"hypothesis": "Is the scaling behavior of LLM-based forecasters consistent with model size, leading to improved performance with larger models?", "method": "Evaluate forecasting performance using different sizes of LLMs adapted by AutoTimes.", "expected_outcome": "Larger LLMs will demonstrate improved forecasting performance due to better model capacity."}, {"hypothesis": "Does the adaptation of LLMs via lightweight token embedding and projection layers maintain high efficiency in training and inference?", "method": "Measure the training and inference time of AutoTimes compared to other LLM4TS methods with full LLM tuning.", "expected_outcome": "AutoTimes will show significantly faster training and inference times due to lightweight adaptation."}], "follow_up_work_ideas": ["Explore the use of advanced low-rank adaptation techniques to further fine-tune LLM token transitions for time series forecasting.", "Investigate the application of AutoTimes in multimodal scenarios where time series data is combined with other data types like text or images.", "Develop methods to handle probabilistic forecasting within the AutoTimes framework to provide uncertainty estimates in predictions.", "Apply AutoTimes to real-world domains with complex dependencies, such as finance or healthcare, to validate its practical utility.", "Study the effects of different types of time series prompts in in-context forecasting to refine prompt engineering strategies."]}}
{"id": "96101", "url": "https://nips.cc/virtual/2024/poster/96101", "title": "Segment Any Change", "authors": [], "abstract": "Abstract:Visual foundation models have achieved remarkable results in zero-shot image classification and segmentation, but zero-shot change detection remains an open problem. In this paper, we propose the segment any change models (AnyChange), a new type of change detection model that supports zero-shot prediction and generalization on unseen change types and data distributions.AnyChange is built on the segment anything model (SAM) via our training-free adaptation method, bitemporal latent matching.By revealing and exploiting intra-image and inter-image semantic similarities in SAM's latent space, bitemporal latent matching endows SAM with zero-shot change detection capabilities in a training-free way. We also propose a point query mechanism to enable AnyChange's zero-shot object-centric change detection capability.We perform extensive experiments to confirm the effectiveness of AnyChange for zero-shot change detection.AnyChange sets a new record on the SECOND benchmark for unsupervised change detection, exceeding the previous SOTA by up to 4.4\\% F$_1$ score, and achieving comparable accuracy with negligible manual annotations (1 pixel per image) for supervised change detection. Code is available at https://github.com/Z-Zheng/pytorch-change-models.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Z-Zheng/pytorch-change-models"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires Python and dependencies installation via pip)", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "8GB or more"}, "time_requirements": "2", "code_quality": "Well-documented code", "difficulty": 3, "stars": 106}, "research_ideas": {"problem_statements": ["The paper addresses the open problem of zero-shot change detection in remote sensing, where current models cannot generalize to unseen change types and data distributions.", "The challenge is to adapt the Segment Anything Model (SAM) for zero-shot change detection without requiring large-scale change detection labels for training.", "Current deep change detection models are limited to specific scenarios and cannot generalize to new geographic areas or change types.", "There is a need for models capable of object-centric change detection in real-world applications, such as disaster damage assessment."], "main_takeaways": ["AnyChange is the first model to enable zero-shot change detection by leveraging the latent space of SAM through bitemporal latent matching.", "Bitemporal latent matching allows SAM to detect changes by exploiting intra-image and inter-image semantic similarities without training.", "AnyChange achieves superior performance on the SECOND benchmark for unsupervised change detection, setting a new record.", "The model can perform both instance-level and pixel-level change detection, either automatically or interactively using a point query mechanism."], "testable_hypotheses": [{"hypothesis": "Does the bitemporal latent matching method improve zero-shot change detection performance compared to SAM without adaptation?", "method": "Compare zero-shot change detection accuracy of AnyChange using bitemporal latent matching with a version of SAM without this adaptation.", "expected_outcome": "AnyChange with bitemporal latent matching will outperform SAM without adaptation."}, {"hypothesis": "Can the point query mechanism improve object-centric change detection accuracy compared to class-agnostic methods?", "method": "Evaluate change detection accuracy on datasets with and without using the point query mechanism for object-centric detection.", "expected_outcome": "The point query mechanism will increase precision in detecting object-centric changes."}, {"hypothesis": "Will AnyChange outperform traditional unsupervised change detection methods on a variety of datasets?", "method": "Benchmark AnyChange against state-of-the-art unsupervised change detection models on multiple datasets.", "expected_outcome": "AnyChange will achieve higher performance metrics than traditional unsupervised methods."}, {"hypothesis": "Does the size of SAM's backbone (ViT-B, ViT-L, ViT-H) significantly affect the zero-shot change detection performance of AnyChange?", "method": "Compare the performance of AnyChange with different backbone sizes on the same dataset.", "expected_outcome": "Larger backbones will generally improve performance, but the increase will be less significant than the initial adaptation impact."}, {"hypothesis": "Can AnyChange's pseudo-labels effectively train a supervised change detection model with minimal manual annotations?", "method": "Train a supervised change detection model using pseudo-labels generated by AnyChange and compare its performance to models trained with full annotations.", "expected_outcome": "The model trained with AnyChange's pseudo-labels will achieve competitive performance with minimal manual annotations."}], "follow_up_work_ideas": ["Explore the application of AnyChange to other domains beyond remote sensing, such as medical imaging or industrial inspection.", "Investigate the integration of temporal information to enhance change detection capabilities over longer time spans.", "Develop methods to improve the robustness of AnyChange against variations in lighting and imaging conditions.", "Extend AnyChange to handle multi-class change detection by incorporating additional semantic information.", "Evaluate the potential of combining AnyChange with active learning techniques to further reduce annotation costs."]}}
{"id": "96571", "url": "https://nips.cc/virtual/2024/poster/96571", "title": "Text2CAD: Generating Sequential CAD Designs from Beginner-to-Expert Level Text Prompts", "authors": [], "abstract": "Abstract:Prototyping complex computer-aided design (CAD) models in modern softwares can be very time-consuming. This is due to the lack of intelligent systems that can quickly generate simpler intermediate parts. We propose Text2CAD, the first AI framework for generating text-to-parametric CAD models using designer-friendly instructions for all skill levels. Furthermore, we introduce a data annotation pipeline for generating text prompts based on natural language instructions for the DeepCAD dataset using Mistral and LLaVA-NeXT. The dataset contains $\\sim170$K models and $\\sim660$K text annotations, from abstract CAD descriptions (e.g., _generate two concentric cylinders_) to detailed specifications (e.g., _draw two circles with center_ $(x,y)$ and _radius_ $r_{1}$, $r_{2}$, \\textit{and extrude along the normal by} $d$...). Within the Text2CAD framework, we propose an end-to-end transformer-based auto-regressive network to generate parametric CAD models from input texts. We evaluate the performance of our model through a mixture of metrics, including visual quality, parametric precision, and geometrical accuracy. Our proposed framework shows great potential in AI-aided design applications. Project page is available at https://sadilkhan.github.io/text2cad-project/.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/sadilkhan/text2cad"}, "reproduce_difficulty": {"environment_setup": "Easy (using conda with an environment.yml file)", "resource_requirements": {"GPUs": "1 NVIDIA GPU (CUDA compatible)", "memory": "At least 8 GB RAM recommended", "API_calls": "Depends on the model used for inference"}, "time_requirements": "Approximately 2-3 hours for training depending on the dataset size", "code_quality": "Well-documented code", "difficulty": 2, "stars": 141}, "research_ideas": {"problem_statements": ["The lack of intelligent systems that can quickly generate simpler intermediate parts in CAD software leads to time-consuming prototyping of complex CAD models.", "There is no existing system capable of generating parametric CAD models from textual design descriptions.", "Defining suitable textual descriptions for parametric CAD generation is a challenge, making it difficult to create accurate deep learning methods for CAD models."], "main_takeaways": ["Text2CAD is the first AI framework to generate parametric 3D CAD models from text prompts, aiming to assist designers of various skill levels.", "A data annotation pipeline was introduced to generate text prompts with varying complexity for the DeepCAD dataset.", "The Text2CAD framework includes a transformer-based autoregressive network that generates CAD models from text inputs.", "Text2CAD demonstrated superior performance over existing methods, particularly in handling detailed parametric descriptions."], "testable_hypotheses": [{"hypothesis": "Does the inclusion of an adaptive layer in the Text2CAD transformer improve the accuracy of generated CAD sequences?", "method": "Compare the performance of Text2CAD with and without the adaptive layer using F1 scores for primitives and extrusion parameters.", "expected_outcome": "The inclusion of the adaptive layer will show improved F1 scores for line, arc, and circle primitives."}, {"hypothesis": "Can Text2CAD generate accurate CAD models from prompts with varying levels of detail?", "method": "Evaluate the performance of Text2CAD using prompts of different levels (abstract, beginner, intermediate, expert) and compare geometric alignment.", "expected_outcome": "Text2CAD will perform better with more detailed prompts, especially at the intermediate and expert levels."}, {"hypothesis": "Does the use of LLaV A-NeXT improve the quality of shape descriptions in the annotation pipeline?", "method": "Assess the accuracy of shape descriptions generated by LLaV A-NeXT compared to other vision language models.", "expected_outcome": "LLaV A-NeXT will provide more accurate and descriptive shape annotations."}, {"hypothesis": "Does Text2CAD outperform DeepCAD in terms of geometric and parametric accuracy of generated models?", "method": "Perform quantitative and qualitative evaluations of CAD models generated by Text2CAD and DeepCAD using metrics like chamfer distance.", "expected_outcome": "Text2CAD will outperform DeepCAD in terms of lower chamfer distance and higher geometric accuracy."}, {"hypothesis": "Can the annotation pipeline reduce the hallucination rate in LLM-generated prompts?", "method": "Measure the hallucination rate in prompts before and after implementing the minimal metadata and annotation pipeline.", "expected_outcome": "The annotation pipeline will significantly reduce hallucinations in the generated prompts."}], "follow_up_work_ideas": ["Investigate the integration of real-time feedback mechanisms within Text2CAD to improve user interaction and design refinement.", "Expand the dataset to include a broader range of shapes and complexities to improve the model's robustness and generalization.", "Develop a standardized benchmark for evaluating text-to-CAD model generation to facilitate comparative analysis and improvements.", "Explore the potential of multi-modal inputs (e.g., combining text with sketches) to enhance the accuracy and flexibility of CAD model generation.", "Optimize the computational efficiency of Text2CAD to make it more accessible for real-time applications in CAD environments."]}}
{"id": "96788", "url": "https://nips.cc/virtual/2024/poster/96788", "title": "SMART: Scalable Multi-agent Real-time Motion Generation via Next-token Prediction", "authors": [], "abstract": "Abstract:Data-driven autonomous driving motion generation tasks are frequently impacted by the limitations of dataset size and the domain gap between datasets, which precludes their extensive application in real-world scenarios. To address this issue, we introduce SMART, a novel autonomous driving motion generation paradigm that models vectorized map and agent trajectory data into discrete sequence tokens. These tokens are then processed through a decoder-only transformer architecture to train for the next token prediction task across spatial-temporal series. This GPT-style method allows the model to learn the motion distribution in real driving scenarios. SMART achieves state-of-the-art performance across most of the metrics on the generative Sim Agents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset (WOMD), demonstrating remarkable inference speed. Moreover, SMART represents the generative model in the autonomous driving motion domain, exhibiting zero-shot generalization capabilities: Using only the NuPlan dataset for training and WOMD for validation, SMART achieved a competitive score of 0.72 on the Sim Agents challenge. Lastly, we have collected over 1 billion motion tokens from multiple datasets, validating the model's scalability. These results suggest that SMART has initially emulated two important properties: scalability and zero-shot generalization, and preliminarily meets the needs of large-scale real-time simulation applications. We have released all the code to promote the exploration of models for motion generation in the autonomous driving field. The source code is available at https://github.com/rainmaker22/SMART.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/rainmaker22/SMART"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_calls": "No external API calls required"}, "time_requirements": 5, "code_quality": "Well-documented code", "difficulty": 3, "stars": 109}, "research_ideas": {"problem_statements": ["The limitations of dataset size and domain gaps between datasets hinder the application of data-driven autonomous driving motion generation in real-world scenarios.", "Existing models struggle with inconsistent scene-level forecasting due to not representing future interactions between different agents' motions.", "Current models are limited in generalizing across different datasets, requiring new data collection for training in new urban environments or maps.", "Scalability and zero-shot generalization remain unaddressed challenges in existing NTP-based motion models for autonomous driving."], "main_takeaways": ["SMART introduces a novel autonomous driving motion generation paradigm using a decoder-only transformer for next-token prediction, showing state-of-the-art performance.", "SMART exhibits zero-shot generalization capabilities, achieving competitive scores on the WOMD test dataset despite training only on NuPlan.", "The model's scalability is demonstrated by collecting over 1 billion motion tokens and validating its performance across datasets.", "SMART provides a new framework for motion generation with a focus on zero-shot generalizability and scalability, crucial for real-time simulation applications."], "testable_hypotheses": [{"hypothesis": "Does SMART's zero-shot generalization capability outperform previous models when tested across different datasets?", "method": "Train SMART on the NuPlan dataset and evaluate its performance on the WOMD validation dataset, comparing with existing models.", "expected_outcome": "SMART will achieve comparable or better performance than existing models, demonstrating effective zero-shot generalization."}, {"hypothesis": "Will increasing the size of SMART's model parameters improve its performance on generative tasks?", "method": "Conduct scaling experiments by training SMART models of varying sizes and evaluate their performance on the WOMD benchmark.", "expected_outcome": "Larger model sizes will show a predictable decrease in test loss, following scaling laws."}, {"hypothesis": "Does the inclusion of road vector tokenization enhance SMART's generalization capabilities?", "method": "Compare the performance of SMART with and without road vector tokenization on multiple datasets.", "expected_outcome": "The model with road vector tokenization will show improved generalization across different datasets."}, {"hypothesis": "Can SMART maintain real-time performance with an inference time under 15ms for multi-agent motion generation?", "method": "Measure the inference time of SMART during simulation tasks and compare it against real-time thresholds.", "expected_outcome": "SMART's inference time will consistently be under 15ms, meeting real-time requirements."}, {"hypothesis": "Does the noise introduced during tokenization improve SMART's robustness in handling accumulated errors?", "method": "Evaluate the interaction and map-based metrics of SMART with and without noise during tokenization.", "expected_outcome": "The version with noise will show better performance in interaction and map-based metrics, indicating improved robustness."}], "follow_up_work_ideas": ["Investigate advanced tokenization techniques or sampling methods to further enhance SMART's performance.", "Explore the application of SMART in planning and prediction tasks within autonomous driving.", "Conduct hyperparameter optimization experiments to identify the best configurations for SMART's model architecture.", "Test SMART's applicability and performance in other domains beyond autonomous driving, such as robotics or spatial-temporal prediction tasks.", "Develop methods to reduce computational costs while maintaining or improving SMART's scalability and performance."]}}
{"id": "96021", "url": "https://nips.cc/virtual/2024/poster/96021", "title": "SHMT: Self-supervised Hierarchical Makeup Transfer via Latent Diffusion Models", "authors": [], "abstract": "Abstract:This paper studies the challenging task of makeup transfer, which aims to apply diverse makeup styles precisely and naturally to a given facial image.  Due to the absence of paired data, current methods typically synthesize sub-optimal pseudo ground truths to guide the model training, resulting in low makeup fidelity. Additionally, different makeup styles generally have varying effects on the person face, but existing methods struggle to deal with this diversity. To address these issues, we propose a novel Self-supervised Hierarchical Makeup Transfer (SHMT) method via latent diffusion models. Following a \"decoupling-and-reconstruction\" paradigm, SHMT works in a self-supervised manner, freeing itself from the misguidance of imprecise pseudo-paired data. Furthermore, to accommodate a variety of makeup styles, hierarchical texture details are decomposed via a Laplacian pyramid and selectively introduced to the content representation. Finally, we design a novel Iterative Dual Alignment (IDA) module that dynamically adjusts the injection condition of the diffusion model, allowing the alignment errors caused by the domain gap between content and makeup representations to be corrected. Extensive quantitative and qualitative analyses demonstrate the effectiveness of our method. Our code is available at https://github.com/Snowfallingplum/SHMT.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Snowfallingplum/SHMT"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires conda environment setup)", "resource_requirements": {"GPUs": {"type": "NVIDIA GPU", "amount": 1}, "memory": "4GB or more recommended", "api_calls": "N/A"}, "time_requirements": "2 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 163}, "research_ideas": {"problem_statements": ["The task of makeup transfer is challenging due to the absence of paired data, leading current methods to synthesize sub-optimal pseudo ground truths that result in low makeup fidelity.", "Existing methods struggle to handle the diversity of different makeup styles, which have varying effects on the person\u2019s face.", "Makeup transfer is essentially an unsupervised task without real transferred images as labeled targets for model training.", "There is ambiguity in preserving source content details in different makeup styles, where some details should be preserved in simple styles but not in complex ones."], "main_takeaways": ["SHMT proposes a novel self-supervised hierarchical makeup transfer method using latent diffusion models, eliminating the need for pseudo-paired data.", "The method follows a 'decoupling-and-reconstruction' paradigm, extracting and reconstructing content and makeup representations from face images.", "A Laplacian pyramid is used to hierarchically decompose texture information, providing flexible control over content detail preservation for various makeup styles.", "The Iterative Dual Alignment (IDA) module dynamically adjusts the injection condition to correct alignment errors in the diffusion model.", "Extensive qualitative and quantitative analyses show that SHMT outperforms other state-of-the-art makeup transfer methods, demonstrating robustness and generalization ability."], "testable_hypotheses": [{"hypothesis": "Does the self-supervised learning strategy of SHMT improve makeup fidelity compared to pseudo-paired data methods?", "method": "Compare the makeup fidelity of SHMT with methods that use pseudo-paired data on a diverse set of makeup styles.", "expected_outcome": "SHMT will show higher makeup fidelity due to its self-supervised learning strategy."}, {"hypothesis": "Can hierarchical texture details via Laplacian pyramid improve the flexibility of makeup style adaptation?", "method": "Evaluate SHMT's performance on makeup styles ranging from simple to complex using different levels of texture detail.", "expected_outcome": "Hierarchical texture details allow SHMT to adapt more flexibly to varying makeup styles."}, {"hypothesis": "Does the Iterative Dual Alignment (IDA) module effectively correct alignment errors in makeup transfer?", "method": "Perform makeup transfer with and without the IDA module and compare the alignment accuracy and makeup fidelity.", "expected_outcome": "The IDA module will improve alignment accuracy and enhance makeup fidelity."}, {"hypothesis": "Will SHMT maintain content preservation better than existing methods when transferring complex makeup styles?", "method": "Compare content preservation metrics of SHMT with existing methods on complex makeup styles.", "expected_outcome": "SHMT will show better content preservation due to its hierarchical handling of texture details."}, {"hypothesis": "Can SHMT's approach be generalized to non-facial makeup transfer tasks?", "method": "Apply SHMT to a different domain, such as object style transfer, and evaluate its performance.", "expected_outcome": "SHMT's methodology will generalize effectively, demonstrating adaptability to new domains."}], "follow_up_work_ideas": ["Explore the application of SHMT to non-facial domains, such as artwork or fashion style transfer, to test its adaptability.", "Investigate the integration of real-time feedback mechanisms to dynamically adjust makeup styles based on user input.", "Develop methods to extend SHMT to work with lower-quality or occluded facial images, improving robustness to real-world scenarios.", "Enhance the computational efficiency of SHMT for faster inference times, potentially through accelerated sampling techniques.", "Study the potential of combining SHMT with augmented reality applications for virtual try-on systems in beauty and fashion industries."]}}
{"id": "94183", "url": "https://nips.cc/virtual/2024/poster/94183", "title": "Harmonizing Visual Text Comprehension and Generation", "authors": [], "abstract": "Abstract:In this work, we present TextHarmony, a unified and versatile multimodal generative model proficient in comprehending and generating visual text. Simultaneously generating images and texts typically results in performance degradation due to the inherent inconsistency between vision and language modalities. To overcome this challenge, existing approaches resort to modality-specific data for supervised fine-tuning, necessitating distinct model instances. We propose Slide-LoRA, which dynamically aggregates modality-specific and modality-agnostic LoRA experts, partially decoupling the multimodal generation space. Slide-LoRA harmonizes the generation of vision and language within a singular model instance, thereby facilitating a more unified generative process. Additionally, we develop a high-quality image caption dataset, DetailedTextCaps-100K, synthesized with a sophisticated closed-source MLLM to enhance visual text generation capabilities further. Comprehensive experiments across various benchmarks demonstrate the effectiveness of the proposed approach. Empowered by Slide-LoRA, TextHarmony achieves comparable performance to modality-specific fine-tuning results with only a 2% increase in parameters and shows an average improvement of 2.5% in visual text comprehension tasks and 4.0% in visual text generation tasks. Our work delineates the viability of an integrated approach to multimodal generation within the visual text domain, setting a foundation for subsequent inquiries. Code is available at https://github.com/bytedance/TextHarmony.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/bytedance/TextHarmony"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": "1 or more (specific type not mentioned)", "memory": "Not specified", "API_calls": "Not specified"}, "time_requirements": "2-3 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 112}, "research_ideas": {"problem_statements": ["How can we harmonize the generation of vision and language within a single model instance without performance degradation?", "What strategies can be employed to overcome the inconsistency between vision and language modalities in multimodal generative models?", "Can a single multimodal generative model achieve comparable performance to modality-specific fine-tuning with minimal parameter increase?"], "main_takeaways": ["TextHarmony uses Slide-LoRA to harmonize visual and text generation within a single model instance, achieving comparable results to modality-specific fine-tuning.", "The proposed model shows an average improvement of 2.5% in visual text comprehension tasks and 4.0% in visual text generation tasks.", "DetailedTextCaps-100K, a new high-quality image caption dataset, is created to improve visual text generation capabilities.", "Slide-LoRA involves dynamic aggregation of modality-specific and modality-agnostic LoRA experts to partially decouple the multimodal generation space.", "TextHarmony effectively addresses the inherent inconsistencies between different modalities, providing a unified approach to multimodal generation."], "testable_hypotheses": [{"hypothesis": "Does the Slide-LoRA mechanism improve the consistency between vision and language modalities in multimodal generation?", "method": "Conduct experiments comparing multimodal generation performance with and without Slide-LoRA using metrics for both text and image generation.", "expected_outcome": "Slide-LoRA will show a noticeable improvement in consistency between modalities, reflected in increased performance metrics."}, {"hypothesis": "Can TextHarmony achieve similar performance to modality-specific models with only a 2% increase in parameters?", "method": "Compare the parameter efficiency and performance of TextHarmony against leading modality-specific models across multiple benchmarks.", "expected_outcome": "TextHarmony will achieve comparable performance with only a slight increase in parameter size."}, {"hypothesis": "Does using DetailedTextCaps-100K improve the quality of visual text generation?", "method": "Evaluate the image generation quality using models trained with and without DetailedTextCaps-100K on standard benchmarks.", "expected_outcome": "The model trained with DetailedTextCaps-100K will generate higher quality visual text content."}, {"hypothesis": "Is the performance of TextHarmony affected by the number of LoRA modules used in Slide-LoRA?", "method": "Experiment with different configurations of LoRA modules (e.g., 3, 6, 9) and measure the impact on performance metrics.", "expected_outcome": "Performance will vary slightly with different LoRA configurations, but significant degradation is expected only with extreme changes."}, {"hypothesis": "Can Slide-LoRA be effectively integrated into other multimodal generative models to improve their performance?", "method": "Integrate Slide-LoRA into different existing multimodal models and evaluate their performance improvements on standard tasks.", "expected_outcome": "Models with Slide-LoRA integration will exhibit improved performance in handling multimodal tasks."}], "follow_up_work_ideas": ["Investigate the scalability of Slide-LoRA in larger multimodal generative models to further enhance performance across diverse tasks.", "Explore integrating Slide-LoRA with other types of generative models, such as GANs or VAEs, to assess its general applicability.", "Develop methods to optimize the gating network in Slide-LoRA for better separation of modality-specific parameters.", "Apply TextHarmony to new domains, such as medical imaging or autonomous driving, to test its versatility and robustness in different contexts.", "Conduct a study on the impact of varying levels of detailed captions in datasets like DetailedTextCaps-100K on generation quality."]}}
{"id": "94617", "url": "https://nips.cc/virtual/2024/poster/94617", "title": "VMamba: Visual State Space Model", "authors": [], "abstract": "Abstract:Designing computationally efficient network architectures remains an ongoing necessity in computer vision. In this paper, we adapt Mamba, a state-space language model, into VMamba, a vision backbone with linear time complexity. At the core of VMamba is a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D bridges the gap between the ordered nature of 1D selective scan and the non-sequential structure of 2D vision data, which facilitates the collection of contextual information from various sources and perspectives. Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements. Extensive experiments demonstrate VMamba\u2019s promising performance across diverse visual perception tasks, highlighting its superior input scaling efficiency compared to existing benchmark models. Source code is available at https://github.com/MzeroMiko/VMamba", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/MzeroMiko/VMamba"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": {"type": "NVIDIA A100", "amount": 1}, "memory": "16GB", "API_calls": "not specified"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 2419}, "research_ideas": {"problem_statements": ["The paper addresses the challenge of designing computationally efficient network architectures for computer vision.", "Existing Vision Transformers suffer from quadratic complexity in self-attention, making them inefficient for large spatial resolutions.", "Previous efforts to improve attention computation efficiency either limit the effective receptive field size or result in performance degradation."], "main_takeaways": ["VMamba, a vision backbone based on State Space Models (SSMs), is introduced as a computationally efficient alternative to Vision Transformers.", "The 2D Selective Scan (SS2D) module is proposed to adapt the 1D selective scan operation of Mamba for 2D vision data.", "VMamba achieves linear time complexity in visual representation learning, maintaining competitive performance with state-of-the-art models.", "Extensive experiments demonstrate VMamba's superior performance across various visual tasks, including image classification, object detection, and semantic segmentation."], "testable_hypotheses": [{"hypothesis": "Does VMamba outperform existing vision models in terms of computational efficiency?", "method": "Compare VMamba's throughput and FLOPs with existing vision models like Swin and ConvNeXt on ImageNet-1K.", "expected_outcome": "VMamba will show higher throughput and comparable or lower FLOPs."}, {"hypothesis": "Can the 2D Selective Scan (SS2D) module effectively capture 2D spatial information?", "method": "Conduct ablation studies by replacing SS2D with alternative scanning mechanisms and evaluate performance on benchmark datasets.", "expected_outcome": "SS2D will outperform alternative scanning mechanisms in capturing 2D spatial information."}, {"hypothesis": "Does reducing the ssm-ratio negatively impact VMamba's performance?", "method": "Alter the ssm-ratio in VMamba's configuration and evaluate the impact on model performance and efficiency.", "expected_outcome": "A lower ssm-ratio will reduce performance but increase computational efficiency."}, {"hypothesis": "Will VMamba maintain performance advantages on tasks with larger input resolutions?", "method": "Evaluate VMamba's performance on inputs with progressively increased spatial resolutions and compare with other models.", "expected_outcome": "VMamba will maintain stable performance and linear growth in computational complexity."}, {"hypothesis": "Does the choice of activation function in SS2D significantly affect VMamba's performance?", "method": "Compare VMamba's performance using different activation functions such as SiLU, ReLU, and GELU.", "expected_outcome": "Performance will be robust across different activation functions."}], "follow_up_work_ideas": ["Investigate the compatibility of existing pre-training methods with SSM-based architectures and explore tailored pre-training techniques for VMamba.", "Explore the potential of VMamba for integration into more generalized tasks, such as video processing or multi-modal data.", "Conduct fine-grained hyperparameter searches to further enhance VMamba's performance.", "Scale VMamba's architecture to larger models and evaluate its performance on even larger datasets.", "Extend the SS2D mechanism to other types of non-sequential data beyond vision, such as spatial-temporal data."]}}
{"id": "94779", "url": "https://nips.cc/virtual/2024/poster/94779", "title": "MVInpainter: Learning Multi-View Consistent Inpainting to Bridge 2D and 3D Editing", "authors": [], "abstract": "Abstract:Novel View Synthesis (NVS) and 3D generation have recently achieved prominent improvements. However, these works mainly focus on confined categories or synthetic 3D assets, which are discouraged from generalizing to challenging in-the-wild scenes and fail to be employed with 2D synthesis directly. Moreover, these methods heavily depended on camera poses, limiting their real-world applications. To overcome these issues, we propose MVInpainter, re-formulating the 3D editing as a multi-view 2D inpainting task. Specifically, MVInpainter partially inpaints multi-view images with the reference guidance rather than intractably generating an entirely novel view from scratch, which largely simplifies the difficulty of in-the-wild NVS and leverages unmasked clues instead of explicit pose conditions. To ensure cross-view consistency, MVInpainter is enhanced by video priors from motion components and appearance guidance from concatenated reference key\\&value attention. Furthermore, MVInpainter incorporates slot attention to aggregate high-level optical flow features from unmasked regions to control the camera movement with pose-free training and inference. Sufficient scene-level experiments on both object-centric and forward-facing datasets verify the effectiveness of MVInpainter, including diverse tasks, such as multi-view object removal, synthesis, insertion, and replacement. The project page is https://ewrfcas.github.io/MVInpainter/.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/ewrfcas/MVInpainter"}, "reproduce_difficulty": {"environment_setup": "Moderate - Requires conda and pip for installation, along with specific commands to set up the environment.", "resource_requirements": {"GPUs": "8 GPUs required (CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7)", "memory": "Not specified but requires substantial memory for processing.", "API_calls": "None specified."}, "time_requirements": "Approximately 10 hours for training and setup.", "code_quality": "Well-documented code with clear instructions.", "difficulty": 3, "stars": 105}, "research_ideas": {"problem_statements": ["How to achieve multi-view consistent scene editing by inserting, removing, replacing objects in 3D scenes?", "3D object generation struggles to generalize to scene-level editing due to factors like illumination and shadows.", "NVS methods have difficulty generalizing across various categories and often fail in diverse or unseen scenes.", "Instance-level 3D editing is time-consuming and requires costly dataset updates.", "Heavy reliance on explicit camera poses limits scalability and applicability in scenarios like short video editing."], "main_takeaways": ["MVInpainter re-formulates 3D editing as a multi-view 2D inpainting task, bridging 2D and 3D editing.", "The method leverages unmasked clues instead of explicit pose conditions, simplifying the difficulty of NVS.", "Video priors and reference guidance ensure cross-view consistency.", "Slot attention aggregates optical flow features without explicit pose requirements, enabling pose-free training and inference.", "MVInpainter is effective in various tasks such as multi-view object removal, synthesis, insertion, and replacement."], "testable_hypotheses": [{"hypothesis": "Does MVInpainter outperform existing NVS methods in generating consistent multi-view scenes without explicit pose data?", "method": "Compare MVInpainter's performance with that of existing NVS methods using consistency metrics across diverse datasets.", "expected_outcome": "MVInpainter will show improved consistency and generalization across various scenes."}, {"hypothesis": "Can the slot attention mechanism effectively replace explicit camera pose data in maintaining object position consistency?", "method": "Evaluate the consistency of object positions across views in scenes processed with and without slot attention.", "expected_outcome": "Slot attention will provide comparable consistency to methods using explicit pose data."}, {"hypothesis": "Does incorporating video priors from motion models improve the structural consistency of inpainted scenes?", "method": "Run experiments with and without video prior incorporation and measure structural consistency using LPIPS.", "expected_outcome": "Video priors will significantly enhance the structural consistency of generated scenes."}, {"hypothesis": "Is the proposed mask adaption strategy effective in adapting object masks across novel views without pose conditions?", "method": "Test the mask adaption strategy across a variety of scenes and measure the accuracy of mask alignment.", "expected_outcome": "The mask adaption strategy will effectively align masks across novel views, maintaining object consistency."}, {"hypothesis": "Does MVInpainter's performance degrade with increasing scene complexity or object variety?", "method": "Evaluate MVInpainter's performance on datasets of varying complexity and object variety.", "expected_outcome": "MVInpainter will maintain performance across different complexities, showing robustness to scene variations."}], "follow_up_work_ideas": ["Explore the integration of more advanced attention mechanisms to handle complex scene inpainting.", "Investigate the application of MVInpainter for real-time video editing and its scalability.", "Develop a version of MVInpainter that can handle extreme viewpoint changes by enhancing its robustness.", "Expand MVInpainter's capabilities to include other forms of generative tasks such as texture synthesis or dynamic scene generation.", "Examine the impact of using different types of video priors and their effectiveness in various editing tasks."]}}
