{"id": "93022", "url": "https://nips.cc/virtual/2024/poster/93022", "title": "Generative Modeling of Molecular Dynamics Trajectories", "authors": [], "abstract": "Abstract:Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show preliminary results on scaling to protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself. Code is available at https://github.com/bjing2016/mdgen.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/478b06f60662d3cdc1d4f15d4587173a-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/bjing2016/mdgen"}, "reproduce_difficulty": {"environment_setup": "Requires pip for package installation with specific versioning.", "resource_requirements": "GPUs not specified, but PyTorch and related libraries suggest a need for a CUDA-compatible GPU. Memory requirements unspecified.", "time_requirements": "Estimated 10 hours for training depending on dataset size and hardware.", "code_quality": "Well-documented code with clear installation and usage instructions.", "difficulty": 3, "stars": 144}}
{"id": "93431", "url": "https://nips.cc/virtual/2024/poster/93431", "title": "Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs", "authors": [], "abstract": "Abstract:We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. AutoDiff frameworks, like PyTorch, enable efficient end-to-end optimization of differentiable systems. However, general computational workflows can be non-differentiable and involve rich feedback (e.g. console output or user\u2019s responses), heterogeneous parameters (e.g. prompts, codes), and intricate objectives (beyond maximizing a score). We investigate end-to-end generative optimization \u2013 using generative models such as LLMs within the optimizer for automatic updating of general computational workflows. We discover that workflow execution traces are akin to back-propagated gradients in AutoDiff and can provide key information to interpret feedback for efficient optimization. Formally, we frame a new mathematical setup, Optimization with Trace Oracle (OPTO). In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. We provide a Python library, Trace, that efficiently converts a workflow optimization problem into an OPTO instance using PyTorch-like syntax. Using Trace, we develop a general LLM-based generative optimizer called OptoPrime. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We envision Trace as an open research platform for devising novel generative optimizers and developing the next generation of interactive learning agents. Website: https://microsoft.github.io/Trace/.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/83ba7056bce2c3c3c27e17397cf3e1f0-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/microsoft/Trace"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 x NVIDIA GPU recommended", "memory": "8 GB RAM minimum", "API_call": "OpenAI API key required"}, "time_requirements": "1-2", "code_quality": "well-documented code", "difficulty": 2, "stars": 492}}
{"id": "98316", "url": "https://nips.cc/virtual/2024/poster/98316", "title": "Causal-learn: Causal Discovery in Python", "authors": [], "abstract": "Abstract:Causal discovery aims at revealing causal relations from observational data, which is a fundamental task in science and engineering. We describe causal-learn, an open-source Python library for causal discovery. This library focuses on bringing a comprehensive collection of causal discovery methods to both practitioners and researchers. It provides easy-to-use APIs for non-specialists, modular building blocks for developers, detailed documentation for learners, and comprehensive methods for all. Different from previous packages in R or Java, causal-learn is fully developed in Python, which could be more in tune with the recent preference shift in programming languages within related communities. The library is available at https://github.com/py-why/causal-learn.", "pdf_url": "https://jmlr.org/papers/volume25/23-0970/23-0970.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/py-why/causal-learn"}, "reproduce_difficulty": {"environment_setup": "Easy (using pip)", "resource_requirements": {"GPUs": "Not required", "memory": "Standard (depends on the dataset)", "API_calls": "None"}, "time_requirements": "1-2 hours", "code_quality": "Well-documented code", "difficulty": 2, "stars": 1287}}
{"id": "95333", "url": "https://nips.cc/virtual/2024/poster/95333", "title": "3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors", "authors": [], "abstract": "Abstract:Novel-view synthesis aims to generate novel views of a scene from multiple inputimages or videos, and recent advancements like 3D Gaussian splatting (3DGS)have achieved notable success in producing photorealistic renderings with efficientpipelines. However, generating high-quality novel views under challenging settings,such as sparse input views, remains difficult due to insufficient information inunder-sampled areas, often resulting in noticeable artifacts. This paper presents3DGS-Enhancer, a novel pipeline for enhancing the representation quality of3DGS representations. We leverage 2D video diffusion priors to address thechallenging 3D view consistency problem, reformulating it as achieving temporalconsistency within a video generation process. 3DGS-Enhancer restores view-consistent latent features of rendered novel views and integrates them with theinput views through a spatial-temporal decoder. The enhanced views are thenused to fine-tune the initial 3DGS model, significantly improving its renderingperformance. Extensive experiments on large-scale datasets of unbounded scenesdemonstrate that 3DGS-Enhancer yields superior reconstruction performance andhigh-fidelity rendering results compared to state-of-the-art methods. The projectwebpage is https://xiliu8006.github.io/3DGS-Enhancer-project.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/f0b42291ddab77dcb2ef8a3488301b62-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/xiliu8006/3DGS-Enhancer"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_call": "Yes"}, "time_requirements": "2", "code_quality": "Well-documented code", "difficulty": 3, "stars": 170}}
{"id": "98326", "url": "https://nips.cc/virtual/2024/poster/98326", "title": "TorchOpt: An Efficient Library for Differentiable Optimization", "authors": [], "abstract": "Abstract:Differentiable optimization algorithms often involve expensive computations of various meta-gradients. To address this, we design and implement TorchOpt, a new PyTorch-based differentiable optimization library. TorchOpt provides an expressive and unified programming interface that simplifies the implementation of explicit, implicit, and zero-order gradients. Moreover, TorchOpt has a distributed execution runtime capable of parallelizing diverse operations linked to differentiable optimization tasks across CPU and GPU devices. Experimental results demonstrate that TorchOpt achieves a 5.2\u00d7 training time speedup in a cluster. TorchOpt is open-sourced at https://github.com/metaopt/torchopt and has become a PyTorch Ecosystem project.", "pdf_url": "https://jmlr.org/papers/volume24/23-0191/23-0191.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/metaopt/torchopt"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "At least 8GB RAM", "additional_requirements": "PyTorch installed"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 570}}
{"id": "98318", "url": "https://nips.cc/virtual/2024/poster/98318", "title": "BenchMARL: Benchmarking Multi-Agent Reinforcement Learning", "authors": [], "abstract": "Abstract:The field of Multi-Agent Reinforcement Learning (MARL) is currently facing a reproducibility crisis. While solutions for standardized reporting have been proposed to address the issue, we still lack a benchmarking tool that enables standardization and reproducibility, while leveraging cutting-edge Reinforcement Learning (RL) implementations. In this paper, we introduce BenchMARL, the first MARL training library created to enable standardized benchmarking across different algorithms, models, and environments. BenchMARL uses TorchRL as its backend, granting it high-performance and maintained state-of-the-art implementations while addressing the broad community of MARL PyTorch users. Its design enables systematic configuration and reporting, thus allowing users to create and run complex benchmarks from simple one-line inputs. BenchMARL is open-sourced on GitHub at https://github.com/facebookresearch/BenchMARL", "pdf_url": "https://jmlr.org/papers/volume25/23-1612/23-1612.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/facebookresearch/BenchMARL"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires pip installation and possibly additional packages)", "resource_requirements": {"GPUs": "1x NVIDIA GPU (e.g., GTX 1080 or better)", "memory": "At least 8GB RAM recommended"}, "time_requirements": "1-2 hours", "code_quality": "Well-documented code with clear instructions", "difficulty": 3, "stars": 351}}
{"id": "95818", "url": "https://nips.cc/virtual/2024/poster/95818", "title": "Classification Done Right for Vision-Language Pre-Training", "authors": [], "abstract": "Abstract:We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data. Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised classification labels, without the need for additional text filtering or selection. Due to the absence of the text encoding as contrastive target, SuperClass does not require a text encoder and does not need to maintain a large batch size as CLIP does. SuperClass demonstrated superior performance on various downstream tasks, including classic computer vision benchmarks and vision language downstream tasks. We further explored the scaling behavior of SuperClass on model size, training length, or data size, and reported encouraging results and comparisons to CLIP. https://github.com/x-cls/superclass", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/aee5298251a418aad89618cf6b5e7ccc-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/x-cls/superclass"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA V100", "amount": 1}, "memory": "16 GB", "api_calls": "N/A"}, "time_requirements": 4, "code_quality": "well-documented code", "difficulty": 3, "stars": 200}}
{"id": "95974", "url": "https://nips.cc/virtual/2024/poster/95974", "title": "Reasoning Multi-Agent Behavioral Topology for Interactive Autonomous Driving", "authors": [], "abstract": "Abstract:Autonomous driving system aims for safe and social-consistent driving through the behavioral integration among interactive agents. However, challenges remain due to multi-agent scene uncertainty and heterogeneous interaction. Current dense and sparse behavioral representations struggle with inefficiency and inconsistency in multi-agent modeling, leading to instability of collective behavioral patterns when integrating prediction and planning (IPP). To address this, we initiate a topological formation that serves as a compliant behavioral foreground to guide downstream trajectory generations. Specifically, we introduce Behavioral Topology (BeTop), a pivotal topological formulation that explicitly represents the consensual behavioral pattern among multi-agent future. BeTop is derived from braid theory to distill compliant interactive topology from multi-agent future trajectories. A synergistic learning framework (BeTopNet) supervised by BeTop facilitates the consistency of behavior prediction and planning within the predicted topology priors. Through imitative contingency learning, BeTop also effectively manages behavioral uncertainty for prediction and planning. Extensive verification on large-scale real-world datasets, including nuPlan and WOMD, demonstrates that BeTop achieves state-of-the-art performance in both prediction and planning tasks. Further validations on the proposed interactive scenario benchmark showcase planning compliance in interactive cases. Code and model is available at https://github.com/OpenDriveLab/BeTop.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/a862f5788fd09bb6843c694d8120d50c-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/OpenDriveLab/BeTop"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 NVIDIA GPU recommended", "memory": "8 GB RAM minimum", "API_calls": "N/A"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 2, "stars": 107}}
{"id": "97514", "url": "https://nips.cc/virtual/2024/poster/97514", "title": "HEST-1k: A Dataset For Spatial Transcriptomics and Histology Image Analysis", "authors": [], "abstract": "Abstract:Spatial transcriptomics enables interrogating the molecular composition of tissue with ever-increasing resolution and sensitivity. However, costs, rapidly evolving technology, and lack of standards have constrained computational methods in ST to narrow tasks and small cohorts. In addition, the underlying tissue morphology, as reflected by H&E-stained whole slide images (WSIs), encodes rich information often overlooked in ST studies. Here, we introduce HEST-1k, a collection of 1,229 spatial transcriptomic profiles, each linked to a WSI and extensive metadata. HEST-1k was assembled from 153 public and internal cohorts encompassing 26 organs, two species (Homo Sapiens and Mus Musculus), and 367 cancer samples from 25 cancer types. HEST-1k processing enabled the identification of 2.1 million expression-morphology pairs and over 76 million nuclei. To support its development, we additionally introduce the HEST-Library, a Python package designed to perform a range of actions with HEST samples. We test HEST-1k and Library on three use cases: (1) benchmarking foundation models for pathology (HEST-Benchmark), (2) biomarker exploration, and (3) multimodal representation learning. HEST-1k, HEST-Library, and HEST-Benchmark can be freely accessed at https://github.com/mahmoodlab/hest.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/60a899cc31f763be0bde781a75e04458-Paper-Datasets_and_Benchmarks_Track.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/mahmoodlab/hest"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "2GB", "additional": "Requires additional libraries for WSI manipulation and GPU acceleration."}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 236}}
{"id": "97649", "url": "https://nips.cc/virtual/2024/poster/97649", "title": "JaxMARL: Multi-Agent RL Environments and Algorithms in JAX", "authors": [], "abstract": "Abstract:Benchmarks are crucial in the development of machine learning algorithms, significantly influencing reinforcement learning (RL) research through the available environments. Traditionally, RL environments run on the CPU, which limits their scalability with the computational resources typically available in academia. However, recent advancements in JAX have enabled the wider use of hardware acceleration, enabling massively parallel RL training pipelines and environments. While this has been successfully applied to single-agent RL, it has not yet been widely adopted for multi-agent scenarios. In this paper, we present JaxMARL, the first open-source, easy-to-use code base that combines GPU-enabled efficiency with support for a large number of commonly used MARL environments and popular baseline algorithms. Our experiments show that, in terms of wall clock time, our JAX-based training pipeline is up to 12,500 times faster than existing approaches. This enables efficient and thorough evaluations, potentially alleviating the evaluation crisis in the field. We also introduce and benchmark SMAX, a vectorised, simplified version of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. The code is available at https://github.com/flairox/jaxmarl.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/5aee125f052c90e326dcf6f380df94f6-Paper-Datasets_and_Benchmarks_Track.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/flairox/jaxmarl"}, "reproduce_difficulty": {"environment_setup": "Moderate (pip for installation, Docker available)", "resource_requirements": {"GPUs": "1 NVIDIA GPU (recommended)", "memory": "8GB RAM minimum", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "Well-documented", "difficulty": 3, "stars": 520}}
{"id": "97713", "url": "https://nips.cc/virtual/2024/poster/97713", "title": "WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks", "authors": [], "abstract": "Abstract:The ability of large language models (LLMs) to mimic human-like intelligence has led to a surge in LLM-based autonomous agents. Though recent LLMs seem capable of planning and reasoning given user instructions, their effectiveness in applying these capabilities for autonomous task solving remains underexplored. This is especially true in enterprise settings, where automated agents hold the promise of a high impact. To fill this gap, we propose WorkArena++, a novel benchmark consisting of 682 tasks corresponding to realistic workflows routinely performed by knowledge workers. WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents. Our empirical studies across state-of-the-art LLMs and vision-language models (VLMs), as well as human workers, reveal several challenges for such models to serve as useful assistants in the workplace. In addition to the benchmark, we provide a mechanism to effortlessly generate thousands of ground-truth observation/action traces, which can be used for fine-tuning existing models. Overall, we expect this work to serve as a useful resource to help the community progress towards capable autonomous agents. The benchmark can be found at https://github.com/ServiceNow/WorkArena.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/0b82662b6c32e887bb252a74d8cb2d5e-Paper-Datasets_and_Benchmarks_Track.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/ServiceNow/WorkArena"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "1", "memory": "8GB", "CPU": "4 cores", "API_calls": "Yes"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 164}}
{"id": "93219", "url": "https://nips.cc/virtual/2024/poster/93219", "title": "HAWK: Learning to Understand Open-World Video Anomalies", "authors": [], "abstract": "Abstract:Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs. However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction. Additionally, the prevalent data scarcity in existing datasets restricts their applicability in open-world scenarios.In this paper, we introduce HAWK, a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely. Recognizing the difference in motion information between abnormal and normal videos, HAWK explicitly integrates motion modality to enhance anomaly identification. To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality. Moreover, to improve the interpretation of motion-to-language, we establish a clear supervisory relationship between motion and its linguistic representation. Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users' open-world questions. The final results demonstrate that HAWK achieves SOTA performance, surpassing existing baselines in both video description generation and question-answering. Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/fca83589e85cb061631b7ebc5db5d6bd-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/jqtangust/hawk"}, "reproduce_difficulty": {"environment_setup": "Moderate difficulty, requires conda for environment setup and installation of dependencies from an environment.yml file.", "resource_requirements": {"GPUs": "4 x RTX A6000 48G", "memory": "Not specified, but requires high-performance GPUs for training.", "API_calls": "Not specified"}, "time_requirements": "Approximately 5-10 hours for training based on the dataset size and model complexity.", "code_quality": "Well-documented code with clear instructions.", "difficulty": 3, "stars": 177}}
{"id": "94065", "url": "https://nips.cc/virtual/2024/poster/94065", "title": "NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface Reconstruction", "authors": [], "abstract": "Abstract:Signed Distance Function (SDF)-based volume rendering has demonstrated significant capabilities in surface reconstruction. Although promising, SDF-based methods often fail to capture detailed geometric structures, resulting in visible defects. By comparing SDF-based volume rendering to density-based volume rendering, we identify two main factors within the SDF-based approach that degrade surface quality: SDF-to-density representation and geometric regularization. These factors introduce challenges that hinder the optimization of the SDF field. To address these issues, we introduce NeuRodin, a novel two-stage neural surface reconstruction framework that not only achieves high-fidelity surface reconstruction but also retains the flexible optimization characteristics of density-based methods.   NeuRodin incorporates innovative strategies that facilitate transformation of arbitrary topologies and reduce artifacts associated with density bias.  Extensive evaluations on the Tanks and Temples and ScanNet++ datasets demonstrate the superiority of NeuRodin, showing strong reconstruction capabilities for both indoor and outdoor environments using solely posed RGB captures. Project website:https://open3dvlab.github.io/NeuRodin/", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/bade15648d6c71df84988cfe042be5d8-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/open3dvlab/NeuRodin"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires conda and specific CUDA version)", "resource_requirements": {"GPUs": {"type": "NVIDIA GPU", "amount": "1 or more"}, "memory": "At least 8GB", "additional": "Requires specific datasets from Tanks and Temples and ScanNet++"}, "time_requirements": "2-3 hours", "code_quality": "Well-documented code with clear instructions", "difficulty": 3, "stars": 117}}
{"id": "94388", "url": "https://nips.cc/virtual/2024/poster/94388", "title": "Return of Unconditional Generation: A Self-supervised Representation Generation Method", "authors": [], "abstract": "Abstract:Unconditional generation -- the problem of modeling data distribution without relying on human-annotated labels -- is a long-standing and fundamental challenge in generative models, creating a potential of learning from large-scale unlabeled data. In the literature, the generation quality of an unconditional method has been much worse than that of its conditional counterpart. This gap can be attributed to the lack of semantic information provided by labels. In this work, we show that one can close this gap by generating semantic representations in the representation space produced by a self-supervised encoder. These representations can be used to condition the image generator. This framework, called Representation-Conditioned Generation (RCG), provides an effective solution to the unconditional generation problem without using labels. Through comprehensive experiments, we observe that RCG significantly improves unconditional generation quality: e.g., it achieves a new state-of-the-art FID of 2.15 on ImageNet 256x256, largely reducing the previous best of 5.91 by a relative 64%. Our unconditional results are situated in the same tier as the leading class-conditional ones. We hope these encouraging observations will attract the community's attention to the fundamental problem of unconditional generation. Code is available athttps://github.com/LTH14/rcg.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/LTH14/rcg", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/LTH14/rcg"}, "reproduce_difficulty": {"environment_setup": "conda", "resource_requirements": {"GPUs": {"type": "NVIDIA V100", "amount": 4}, "memory": "not specified", "API_calls": "not specified"}, "time_requirements": 200, "code_quality": "well-documented code", "difficulty": 3, "stars": 905}}
{"id": "96264", "url": "https://nips.cc/virtual/2024/poster/96264", "title": "Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models", "authors": [], "abstract": "Abstract:We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs). Specifically, we propose meta-buffer to store a series of informative high-level thoughts, namely thought-template, distilled from the problem-solving processes across various tasks. Then for each problem, we retrieve a relevant thought-template and adaptively instantiate it with specific reasoning structures to conduct efficient reasoning. To guarantee the scalability and stability, we further propose buffer-manager to dynamically update the meta-buffer, thus enhancing the capacity of meta-buffer as more tasks are solved. We conduct extensive experiments on 10 challenging reasoning-intensive tasks, and achieve significant performance improvements over previous SOTA methods: 11\\% on Game of 24, 20\\% on Geometric Shapes and 51\\% on Checkmate-in-One. Further analysis demonstrate the superior generalization ability and model robustness of our BoT, while requiring only 12\\% of the cost of multi-query prompting methods (e.g., tree/graph of thoughts) on average. Code is available at: https://github.com/YangLing0818/buffer-of-thought-llm", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/cde328b7bf6358f5ebb91fe9c539745e-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/YangLing0818/buffer-of-thought-llm"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_call": true}, "time_requirements": 1, "code_quality": "well-documented code", "difficulty": 3, "stars": 608}}
{"id": "96897", "url": "https://nips.cc/virtual/2024/poster/96897", "title": "BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models", "authors": [], "abstract": "Abstract:This work presents BAdam, an optimization method that leverages the block coordinate descent (BCD) framework with Adam's update rule. BAdam offers a memory efficient approach to the full parameter finetuning of large language models. We conduct   a theoretical convergence analysis for BAdam in the deterministic case. Experimentally, we apply BAdam to finetune the Llama 3-8B and Llama 3-70B models using a single RTX3090-24GB GPU and 4 A100-80GB GPUs, respectively. The results confirm BAdam's efficiency in terms of memory usage, running time, and optimization capability. Furthermore, the downstream performance evaluation based on MT-bench and math benchmarks shows that BAdam outperforms existing memory efficient baselines such as LoRA. It also demonstrates that BAdam can achieve comparable or even superior performance compared to Adam. Finally, the ablation study using SGD's update rule illustrates the suitability of BCD for finetuning LLMs. Our code can be easily integrated into any PyTorch-based codebase and is available at https://github.com/Ledzy/BAdam.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/2c570b0f9938c7a58a612e5b00af9cc0-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Ledzy/BAdam"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "RTX3090", "amount": 1}, "memory": "23.5 GB for Llama 3-8B, 21.8 GB for Llama 2-7B"}, "time_requirements": "3", "code_quality": "well-documented code", "difficulty": 3, "stars": 244}}
{"id": "94480", "url": "https://nips.cc/virtual/2024/poster/94480", "title": "InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory", "authors": [], "abstract": "Abstract:Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents). However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues. Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities. In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning. To this end, we introduce a training-free memory-based method, InfLLM. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences with a limited context window and well capture long-distance dependencies. Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences. Even when the sequence length is scaled to 1,024K, InfLLM still effectively captures long-distance dependencies. Our code can be found at https://github.com/thunlp/InfLLM.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/thunlp/InfLLM"}, "reproduce_difficulty": {"environment_setup": "Moderate difficulty, requires Python packages and configuration files.", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "Minimum 16 GB GPU memory", "other": "Network access for downloading datasets"}, "time_requirements": "Approximately 2 hours for setup and initial tests.", "code_quality": "Well-documented, provides clear instructions and examples.", "difficulty": 3, "stars": 335}}
{"id": "93638", "url": "https://nips.cc/virtual/2024/poster/93638", "title": "Self-playing Adversarial Language Game Enhances LLM Reasoning", "authors": [], "abstract": "Abstract:We explore the potential of self-play training for large language models (LLMs) in a two-player adversarial language game called Adversarial Taboo. In this game, an attacker and a defender communicate around a target word only visible to the attacker. The attacker aims to induce the defender to speak the target word unconsciously, while the defender tries to infer the target word from the attacker's utterances. To win the game, both players must have sufficient knowledge about the target word and high-level reasoning ability to infer and express in this information-reserved conversation. Hence, we are curious about whether LLMs' reasoning ability can be further enhanced by Self-Playing this Adversarial language Game (SPAG). With this goal, we select several open-source LLMs and let each act as the attacker and play with a copy of itself as the defender on an extensive range of target words. Through reinforcement learning on the game outcomes, we observe that the LLMs' performances uniformly improve on a broad range of reasoning benchmarks. Furthermore, iteratively adopting this self-play process can continuously promote LLMs' reasoning abilities. The code is available at https://github.com/Linear95/SPAG.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/e4be7e9867ef163563f4a5e90cec478f-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Linear95/SPAG"}, "reproduce_difficulty": {"environment_setup": "Medium (Requires pip for installation and specific hardware setup)", "resource_requirements": {"GPUs": {"type": "A100", "amount": 32}, "memory": "40G per GPU", "CUDA_version": "11.0"}, "time_requirements": 24, "code_quality": "Well-documented code", "difficulty": 3, "stars": 120}}
{"id": "94328", "url": "https://nips.cc/virtual/2024/poster/94328", "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs", "authors": [], "abstract": "Abstract:We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism, and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4 bits, without any channels identified for retention in higher precision. Our 4-bit quantized LLAMA2-70B model has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the zero-shot performance. We also show that QuaRot can provide lossless 6 and 8 bit LLAMA-2 models without any calibration data using round-to-nearest quantization. Code is available at github.com/spcl/QuaRot.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/b5b939436789f76f08b9d0da5e81af7c-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/spcl/QuaRot"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": {"type": "NVIDIA A100", "amount": 1}, "memory": "16 GB", "API_calls": "None"}, "time_requirements": 2, "code_quality": "well-documented", "difficulty": 2, "stars": 351}}
{"id": "93408", "url": "https://nips.cc/virtual/2024/poster/93408", "title": "ADOPT: Modified Adam Can Converge with Any $\\beta_2$ with the Optimal Rate", "authors": [], "abstract": "Abstract:Adam is one of the most popular optimization algorithms in deep learning. However, it is known that Adam does not converge in theory unless choosing a hyperparameter, i.e., $\\beta_2$, in a problem-dependent manner. There have been many attempts to fix the non-convergence (e.g., AMSGrad), but they require an impractical assumption that the gradient noise is uniformly bounded. In this paper, we propose a new adaptive gradient method named ADOPT, which achieves the optimal convergence rate of $\\mathcal{O} ( 1 / \\sqrt{T} )$ with any choice of $\\beta_2$ without depending on the bounded noise assumption. ADOPT addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate and changing the order of the momentum update and the normalization by the second moment estimate. We also conduct intensive numerical experiments, and verify that our ADOPT achieves superior results compared to Adam and its variants across a wide range of tasks, including image classification, generative modeling, natural language processing, and deep reinforcement learning. The implementation is available at https://github.com/iShohei220/adopt.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/iShohei220/adopt"}, "reproduce_difficulty": {"environment_setup": "low", "resource_requirements": {"GPUs": "1 x GPU (NVIDIA recommended)", "memory": "8 GB RAM minimum", "API_call": "None specified"}, "time_requirements": "1-2 hours for initial training and experimentation", "code_quality": "well-documented code", "difficulty": 2, "stars": 417}}
{"id": "95262", "url": "https://nips.cc/virtual/2024/poster/95262", "title": "MoE Jetpack: From Dense Checkpoints to Adaptive Mixture of Experts for Vision Tasks", "authors": [], "abstract": "Abstract:The sparsely activated mixture of experts (MoE) model presents an effective alternative to densely activated (dense) models, combining improved accuracy with computational efficiency. However, training MoE models from scratch requires extensive data and computational resources, a challenge that limits their widespread adoption. To address this, we introduce MoE Jetpack, a framework designed to fine-tune the abundant and easily accessible dense checkpoints into MoE models. MoE Jetpack incorporates two key techniques: (1)checkpoint recycling, which initializes MoE models with dense checkpoints to accelerate convergence and enhance accuracy, minimizing the need for extensive pre-training; (2) thehyperspherical adaptive MoE (SpheroMoE) layer, which optimizes the MoE architecture to enhance fine-tuning performance and efficiency.Experimental results indicate that MoE Jetpack doubles the convergence speed and enhances accuracy by 2.8% on ImageNet-1K. On smaller datasets, it achieves up to 8-fold faster convergence and over 30% accuracy gains, highlighting its efficiency.The code is available at https://github.com/Adlith/MoE-Jetpack.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/167bcf2af2cd08fcf75b932022db0311-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Adlith/MoE-Jetpack"}, "reproduce_difficulty": {"environment_setup": "Moderate difficulty, requires installation of PyTorch, MMCV, and additional dependencies via pip.", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 4}, "memory": "Not specified, but likely requires significant RAM for training.", "api_calls": "Not specified"}, "time_requirements": 2, "code_quality": "Well-documented code with clear instructions.", "difficulty": 3, "stars": 105}}
{"id": "95559", "url": "https://nips.cc/virtual/2024/poster/95559", "title": "Demystify Mamba in Vision: A Linear Attention Perspective", "authors": [], "abstract": "Abstract:Mamba is an effective state space model with linear computation complexity. It has recently shown impressive efficiency in dealing with high-resolution inputs across various vision tasks. In this paper, we reveal that the powerful Mamba model shares surprising similarities with linear attention Transformer, which typically underperform conventional Transformer in practice. By exploring the similarities and disparities between the effective Mamba and subpar linear attention Transformer, we provide comprehensive analyses to demystify the key factors behind Mamba\u2019s success. Specifically, we reformulate the selective state space model and linear attention within a unified formulation, rephrasing Mamba as a variant of linear attention Transformer with six major distinctions: input gate, forget gate, shortcut, no attention normalization, single-head, and modified block design. For each design, we meticulously analyze its pros and cons, and empirically evaluate its impact on model performance in vision tasks. Interestingly, the results highlight the forget gate and block design as the core contributors to Mamba\u2019s success, while the other four designs are less crucial. Based on these findings, we propose a Mamba-Inspired Linear Attention (MILA) model by incorporating the merits of these two key designs into linear attention. The resulting model outperforms various vision Mamba models in both image classification and high-resolution dense prediction tasks, while enjoying parallelizable computation and fast inference speed. Code is available at https://github.com/LeapLabTHU/MLLA.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/LeapLabTHU/MLLA"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires specific versions of Python packages and PyTorch setup with distributed training)", "resource_requirements": "8 GPUs for distributed training, at least 16 GB of RAM, ImageNet dataset", "time_requirements": "3 hours", "code_quality": "Well-documented code with clear instructions for setup and usage", "difficulty": 3, "stars": 276}}
{"id": "94391", "url": "https://nips.cc/virtual/2024/poster/94391", "title": "CycleNet: Enhancing Time Series Forecasting through Modeling Periodic Patterns", "authors": [], "abstract": "Abstract:The stable periodic patterns present in time series data serve as the foundation for conducting long-horizon forecasts. In this paper, we pioneer the exploration of explicitly modeling this periodicity to enhance the performance of models in long-term time series forecasting (LTSF) tasks. Specifically, we introduce the Residual Cycle Forecasting (RCF) technique, which utilizes learnable recurrent cycles to model the inherent periodic patterns within sequences, and then performs predictions on the residual components of the modeled cycles. Combining RCF with a Linear layer or a shallow MLP forms the simple yet powerful method proposed in this paper, called CycleNet. CycleNet achieves state-of-the-art prediction accuracy in multiple domains including electricity, weather, and energy, while offering significant efficiency advantages by reducing over 90% of the required parameter quantity. Furthermore, as a novel plug-and-play technique, the RCF can also significantly improve the prediction accuracy of existing models, including PatchTST and iTransformer. The source code is available at: https://github.com/ACAT-SCUT/CycleNet.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/bfe7998398779dde03cad7a73b1f81b6-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/ACAT-SCUT/CycleNet"}, "reproduce_difficulty": {"environment_setup": "2", "resource_requirements": {"GPUs": "1 NVIDIA GPU (e.g., Tesla V100 or equivalent)", "memory": "Minimum of 16 GB RAM"}, "time_requirements": "2", "code_quality": "Well-documented code", "difficulty": 3, "stars": 133}}
{"id": "96893", "url": "https://nips.cc/virtual/2024/poster/96893", "title": "SegVol: Universal and Interactive Volumetric Medical Image Segmentation", "authors": [], "abstract": "Abstract:Precise image segmentation provides clinical study with instructive information. Despite the remarkable progress achieved in medical image segmentation, there is still an absence of a 3D foundation segmentation model that can segment a wide range of anatomical categories with easy user interaction. In this paper, we propose a 3D foundation segmentation model, named SegVol, supporting universal and interactive volumetric medical image segmentation. By scaling up training data to 90K unlabeled Computed Tomography (CT) volumes and 6K labeled CT volumes, this foundation model supports the segmentation of over 200 anatomical categories using semantic and spatial prompts. To facilitate efficient and precise inference on volumetric images, we design a zoom-out-zoom-in mechanism. Extensive experiments on 22 anatomical segmentation tasks verify that SegVol outperforms the competitors in 19 tasks, with improvements up to 37.24\\% compared to the runner-up methods. We demonstrate the effectiveness and importance of specific designs by ablation study. We expect this foundation model can promote the development of volumetric medical image analysis. The model and code are publicly available at https://github.com/BAAI-DCAI/SegVol.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/c7c7cf10082e454b9662a686ce6f1b6f-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/BAAI-DCAI/SegVol"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "Well-documented", "difficulty": 3, "stars": 295}}
{"id": "94155", "url": "https://nips.cc/virtual/2024/poster/94155", "title": "Voxel Mamba: Group-Free State Space Models for Point Cloud based 3D Object Detection", "authors": [], "abstract": "Abstract:Serialization-based methods, which serialize the 3D voxels and group them into multiple sequences before inputting to Transformers, have demonstrated their effectiveness in 3D object detection. However, serializing 3D voxels into 1D sequences will inevitably sacrifice the voxel spatial proximity. Such an issue is hard to be addressed by enlarging the group size with existing serialization-based methods due to the quadratic complexity of Transformers with feature sizes. Inspired by the recent advances of state space models (SSMs), we present a Voxel SSM, termed as Voxel Mamba, which employs a group-free strategy to serialize the whole space of voxels into a single sequence. The linear complexity of SSMs encourages our group-free design, alleviating the loss of spatial proximity of voxels. To further enhance the spatial proximity, we propose a Dual-scale SSM Block to establish a hierarchical structure, enabling a larger receptive field in the 1D serialization curve, as well as more complete local regions in 3D space. Moreover, we implicitly apply window partition under the group-free framework by positional encoding, which further enhances spatial proximity by encoding voxel positional information. Our experiments on Waymo Open Dataset and nuScenes dataset show that Voxel Mamba not only achieves higher accuracy than state-of-the-art methods, but also demonstrates significant advantages in computational efficiency. The source code is available at https://github.com/gwenzhang/Voxel-Mamba.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/947b63838c90f1485188b9c673bc3a14-Paper-Conference.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/gwenzhang/Voxel-Mamba"}, "reproduce_difficulty": {"environment_setup": "Moderate (Requires multi-GPU setup and dependencies from OpenPCDet)", "resource_requirements": {"GPUs": {"type": "NVIDIA A100", "amount": 8}, "memory": "Not specified, but high memory usage expected due to multi-GPU training", "API_calls": "Not specified"}, "time_requirements": "Estimated 2-4 hours for training depending on hardware setup", "code_quality": "Well-documented code", "difficulty": 3, "stars": 110}}
{"id": "97431", "url": "https://nips.cc/virtual/2024/poster/97431", "title": "Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs", "authors": [], "abstract": "Abstract:Although Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks in a zero-shot manner, they are susceptible to jailbreak attacks and can be manipulated to produce harmful outputs. Recently, a growing body of research has categorized jailbreak attacks into token-level and prompt-level attacks.  However, previous work primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs. To address these issues, we introduced JailTrickBench to evaluate the impact of various attack settings on LLM performance and provide a baseline for jailbreak attacks, encouraging the adoption of a standardized evaluation framework. Specifically, we evaluate the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives. We further conduct seven representative jailbreak attacks on six defense methods across two widely used datasets, encompassing approximately 354 experiments with about 55,000 GPU hours on A800-80G. Our experimental results highlight the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is available at https://github.com/usail-hkust/JailTrickBench.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/38c1dfb4f7625907b15e9515365e7803-Paper-Datasets_and_Benchmarks_Track.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/usail-hkust/JailTrickBench"}, "reproduce_difficulty": {"environment_setup": "Requires installation via pip; no specific mention of conda or docker.", "resource_requirements": "Requires access to multiple GPUs (50 A800 GPUs recommended); approximately 55,000 GPU hours for experiments.", "time_requirements": "Estimated time for running main commands is significant, likely several hours to days depending on the setup.", "code_quality": "Well-documented code with detailed instructions for setup and running experiments.", "difficulty": 4, "stars": 122}}
{"id": "97605", "url": "https://nips.cc/virtual/2024/poster/97605", "title": "WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark", "authors": [], "abstract": "Abstract:Underwater Object Tracking (UOT) is essential for identifying and tracking submerged objects in underwater videos, but existing datasets are limited in scale, diversity of target categories and scenarios covered, impeding the development of advanced tracking algorithms. To bridge this gap, we take the first step and introduce WebUOT-1M, \\ie, the largest public UOT benchmark to date, sourced from complex and realistic underwater environments. It comprises 1.1 million frames across 1,500 video clips filtered from 408 target categories, largely surpassing previous UOT datasets, \\eg, UVOT400. Through meticulous manual annotation and verification, we provide high-quality bounding boxes for underwater targets. Additionally, WebUOT-1M includes language prompts for video sequences, expanding its application areas, \\eg, underwater vision-language tracking. Given that most existing trackers are designed for open-air conditions and perform poorly in underwater environments due to domain gaps, we propose a novel framework that uses omni-knowledge distillation to train a student Transformer model effectively. To the best of our knowledge, this framework is the first to effectively transfer open-air domain knowledge to the UOT model through knowledge distillation, as demonstrated by results on both existing UOT datasets and the newly proposed WebUOT-1M. We have thoroughly tested WebUOT-1M with 30 deep trackers, showcasing its potential as a benchmark for future UOT research. The complete dataset, along with codes and tracking results, are publicly accessible at \\href{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}{\\color{magenta}{here}}.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/983632847/Awesome-Multimodal-Object-Tracking"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires installation of dependencies, likely with pip/conda)", "resource_requirements": {"GPUs": "1 NVIDIA GPU with at least 8GB memory", "memory": "16GB RAM", "API_call": "Possible for dataset downloads"}, "time_requirements": "3 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 160}}
{"id": "97791", "url": "https://nips.cc/virtual/2024/poster/97791", "title": "The Multimodal Universe: Enabling Large-Scale Machine Learning with 100 TB of Astronomical Scientific Data", "authors": [], "abstract": "Abstract:We present theMultimodal Universe, a large-scale multimodal dataset of scientific astronomical data, compiled specifically to facilitate machine learning research. Overall, our dataset contains hundreds of millions of astronomical observations, constituting 100TB of multi-channel and hyper-spectral images, spectra, multivariate time series, as well as a wide variety of associated scientific measurements and metadata. In addition, we include a range of benchmark tasks representative of standard practices for machine learning methods in astrophysics. This massive dataset will enable the development of large multi-modal models specifically targeted towards scientific applications. All codes used to compile the dataset, and a description of how to access the data is available at https://github.com/MultimodalUniverse/MultimodalUniverse", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/6a57493d35fefea59d06396c7cb69228-Paper-Datasets_and_Benchmarks_Track.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/MultimodalUniverse/MultimodalUniverse"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "2 NVIDIA A100", "memory": "64 GB RAM", "storage": "100 TB disk space"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 379}}
{"id": "97609", "url": "https://nips.cc/virtual/2024/poster/97609", "title": "DrivAerNet++: A Large-Scale Multimodal Car Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks", "authors": [], "abstract": "Abstract:We present DrivAerNet++, the largest and most comprehensive multimodal dataset for aerodynamic car design. DrivAerNet++ comprises 8,000 diverse car designs modeled with high-fidelity computational fluid dynamics (CFD) simulations. The dataset includes diverse car configurations such as fastback, notchback, and estateback, with different underbody and wheel designs to represent both internal combustion engines and electric vehicles. Each entry in the dataset features detailed 3D meshes, parametric models, aerodynamic coefficients, and extensive flow and surface field data, along with segmented parts for car classification and point cloud data. This dataset supports a wide array of machine learning applications including data-driven design optimization, generative modeling, surrogate model training, CFD simulation acceleration, and geometric classification. With more than 39 TB of publicly available engineering data, DrivAerNet++ fills a significant gap in available resources, providing high-quality, diverse data to enhance model training, promote generalization, and accelerate automotive design processes. Along with rigorous dataset validation, we also provide ML benchmarking results on the task of aerodynamic drag prediction, showcasing the breadth of applications supported by our dataset. This dataset is set to significantly impact automotive design and broader engineering disciplines by fostering innovation and improving the fidelity of aerodynamic evaluations. Dataset and code available at: https://github.com/Mohamedelrefaie/DrivAerNet", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/013cf29a9e68e4411d0593040a8a1eb3-Paper-Datasets_and_Benchmarks_Track.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Mohamedelrefaie/DrivAerNet"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": "2 NVIDIA GPUs (e.g., RTX 3090 or equivalent)", "memory": "1000 GB", "storage": "39 TB"}, "time_requirements": "3 million CPU-hours", "code_quality": "Well documented", "difficulty": 4, "stars": 274}}
{"id": "97674", "url": "https://nips.cc/virtual/2024/poster/97674", "title": "Needle In A Multimodal Haystack", "authors": [], "abstract": "Abstract:With the rapid advancement of multimodal large language models (MLLMs), their evaluation has become increasingly comprehensive. However, understanding long multimodal content, as a foundational ability for real-world applications, remains underexplored. In this work, we present Needle In A Multimodal Haystack (MM-NIAH), the first benchmark specifically designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents. Our benchmark includes three types of evaluation tasks: multimodal retrieval, counting, and reasoning. In each task, the model is required to answer the questions according to different key information scattered throughout the given multimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation. We hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs. Code and benchmark are released at https://github.com/OpenGVLab/MM-NIAH.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/24a8968affe71ffe4067d022b9d16566-Paper-Datasets_and_Benchmarks_Track.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/OpenGVLab/MM-NIAH"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 8}, "memory": "not specified", "api_calls": "not specified"}, "time_requirements": 5, "code_quality": "well-documented code", "difficulty": 3, "stars": 112}}
{"id": "97882", "url": "https://nips.cc/virtual/2024/poster/97882", "title": "The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning", "authors": [], "abstract": "Abstract:Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at https://github.com/PolymathicAI/the_well.", "pdf_url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/4f9a5acd91ac76569f2fe291b1f4772b-Paper-Datasets_and_Benchmarks_Track.pdf", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/PolymathicAI/the_well"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA CUDA", "amount": 1}, "memory": "16GB", "disk_space": "15TB for full datasets"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 761}}
{"id": "97594", "url": "https://nips.cc/virtual/2024/poster/97594", "title": "IMDL-BenCo: A Comprehensive Benchmark and Codebase for Image Manipulation Detection & Localization", "authors": [], "abstract": "Abstract:A comprehensive benchmark is yet to be established in the Image Manipulation Detection \\& Localization (IMDL) field. The absence of such a benchmark leads to insufficient and misleading model evaluations, severely undermining the development of this field. However, the scarcity of open-sourced baseline models and inconsistent training and evaluation protocols make conducting rigorous experiments and faithful comparisons among IMDL models challenging. To address these challenges, we introduce IMDL-BenCo, the first comprehensive IMDL benchmark and modular codebase. IMDL-BenCo: i) decomposes the IMDL framework into standardized, reusable components and revises the model construction pipeline, improving coding efficiency and customization flexibility; ii) fully implements or incorporates training code for state-of-the-art models to establish a comprehensive IMDL benchmark; and iii) conducts deep analysis based on the established benchmark and codebase, offering new insights into IMDL model architecture, dataset characteristics, and evaluation standards.Specifically, IMDL-BenCo includes common processing algorithms, 8 state-of-the-art IMDL models (1 of which are reproduced from scratch), 2 sets of standard training and evaluation protocols, 15 GPU-accelerated evaluation metrics, and 3 kinds of robustness evaluation. This benchmark and codebase represent a significant leap forward in calibrating the current progress in the IMDL field and inspiring future breakthroughs.Code is available at: https://github.com/scu-zjz/IMDLBenCo", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/scu-zjz/IMDLBenCo"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 NVIDIA GPU (minimum 4GB VRAM recommended)", "memory": "8GB RAM minimum"}, "time_requirements": "1", "code_quality": "well-documented", "difficulty": 2, "stars": 115}}
{"id": "97868", "url": "https://nips.cc/virtual/2024/poster/97868", "title": "Vript: A Video Is Worth Thousands of Words", "authors": [], "abstract": "Abstract:Advancements in multimodal learning, particularly in video understanding and generation, require high-quality video-text datasets for improved model performance. Vript addresses this issue with a meticulously annotated corpus of 12K high-resolution videos, offering detailed, dense, and script-like captions for over 420K clips. Each clip has a caption of ~145 words, which is over 10x longer than most video-text datasets. Unlike captions only documenting static content in previous datasets, we enhance video captioning to video scripting by documenting not just the content, but also the camera operations, which include the shot types (medium shot, close-up, etc) and camera movements (panning, tilting, etc). By utilizing the Vript, we explore three training paradigms of aligning more text with the video modality rather than clip-caption pairs. This results in Vriptor, a top-performing video captioning model among open-source models, comparable to GPT-4V in performance. Vriptor is also a powerful model capable of end-to-end generation of dense and detailed captions for long videos. Moreover, we introduce Vript-Hard, a benchmark consisting of three video understanding tasks that are more challenging than existing benchmarks: Vript-HAL is the first benchmark evaluating action and object hallucinations in video LLMs, Vript-RR combines reasoning with retrieval resolving question ambiguity in long-video QAs, and Vript-ERO is a new task to evaluate the temporal understanding of events in long videos rather than actions in short videos in previous works. All code, models, and datasets are available in https://github.com/mutonix/Vript.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/mutonix/Vript"}, "reproduce_difficulty": {"environment_setup": "Easy (using pip and conda)", "resource_requirements": {"GPUs": "1 NVIDIA 1080 or equivalent", "memory": "16 GB RAM", "API calls": "Requires access to OpenAI API for model inference"}, "time_requirements": "2 hours", "code_quality": "Well-documented code with clear instructions", "difficulty": 2, "stars": 136}}
{"id": "97746", "url": "https://nips.cc/virtual/2024/poster/97746", "title": "Benchmarking LLMs via Uncertainty Quantification", "authors": [], "abstract": "Abstract:The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves nine LLMs (LLM series) spanning five representative natural language processing tasks. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. These results underscore the significance of incorporating uncertainty in the evaluation of LLMs. Our implementation is available at https://github.com/smartyfh/LLM-Uncertainty-Bench.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/smartyfh/LLM-Uncertainty-Bench"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "NVIDIA GPUs recommended", "amount": "1 or more", "memory": "16 GB or more recommended"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 209}}
{"id": "97425", "url": "https://nips.cc/virtual/2024/poster/97425", "title": "XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX", "authors": [], "abstract": "Abstract:Inspired by the diversity and depth of XLand and the simplicity and minimalism of MiniGrid, we present XLand-MiniGrid, a suite of tools and grid-world environments for meta-reinforcement learning research. Written in JAX, XLand-MiniGrid is designed to be highly scalable and can potentially run on GPU or TPU accelerators, democratizing large-scale experimentation with limited resources. Along with the environments, XLand-MiniGrid provides pre-sampled benchmarks with millions of unique tasks of varying difficulty and easy-to-use baselines that allow users to quickly start training adaptive agents. In addition, we have conducted a preliminary analysis of scaling and generalization, showing that our baselines are capable of reaching millions of steps per second during training and validating that the proposed benchmarks are challenging. XLand-MiniGrid is open-source and available at \\url{https://github.com/corl-team/xland-minigrid}.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/corl-team/xland-minigrid"}, "reproduce_difficulty": {"environment_setup": "moderate", "resource_requirements": {"GPUs": "1 (NVIDIA recommended for JAX)", "memory": "8GB or more recommended", "API_call": "HuggingFace for benchmark datasets"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 265}}
{"id": "97445", "url": "https://nips.cc/virtual/2024/poster/97445", "title": "ProG: A Graph Prompt Learning Benchmark", "authors": [], "abstract": "Abstract:Artificial general intelligence on graphs has shown significant advancements across various applications, yet the traditional `Pre-train \\& Fine-tune' paradigm faces inefficiencies and negative transfer issues, particularly in complex and few-shot settings. Graph prompt learning emerges as a promising alternative, leveraging lightweight prompts to manipulate data and fill the task gap by reformulating downstream tasks to the pretext. However, several critical challenges still remain: how to unify diverse graph prompt models, how to evaluate the quality of graph prompts, and to improve their usability for practical comparisons and selection. In response to these challenges, we introduce the first comprehensive benchmark for graph prompt learning. Our benchmark integratesSIXpre-training methods andFIVEstate-of-the-art graph prompt techniques, evaluated acrossFIFTEENdiverse datasets to assess performance, flexibility, and efficiency. We also present 'ProG', an easy-to-use open-source library that streamlines the execution of various graph prompt models, facilitating objective evaluations. Additionally, we propose a unified framework that categorizes existing graph prompt methods into two main approaches: prompts as graphs and prompts as tokens. This framework enhances the applicability and comparison of graph prompt techniques. The code is available at: https://github.com/sheldonresearch/ProG.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/sheldonresearch/ProG"}, "reproduce_difficulty": {"environment_setup": "Moderate difficulty, requires conda for environment setup and installation of multiple dependencies.", "resource_requirements": {"GPUs": {"type": "CUDA-enabled GPU", "amount": "1"}, "memory": "At least 8GB RAM recommended", "others": "Python 3.9 or higher"}, "time_requirements": "Approximately 5 hours for initial setup and running experiments.", "code_quality": "Well-documented code with clear instructions.", "difficulty": 3, "stars": 529}}
{"id": "97533", "url": "https://nips.cc/virtual/2024/poster/97533", "title": "shapiq: Shapley Interactions for Machine Learning", "authors": [], "abstract": "Abstract:Originally rooted in game theory, the Shapley Value (SV) has recently become an important tool in machine learning research. Perhaps most notably, it is used for feature attribution and data valuation in explainable artificial intelligence. Shapley Interactions (SIs) naturally extend the SV and address its limitations by assigning joint contributions to groups of entities, which enhance understanding of black box machine learning models. Due to the exponential complexity of computing SVs and SIs, various methods have been proposed that exploit structural assumptions or yield probabilistic estimates given limited resources. In this work, we introduce shapiq, an open-source Python package that unifies state-of-the-art algorithms to efficiently compute SVs and any-order SIs in an application-agnostic framework. Moreover, it includes a benchmarking suite containing 11 machine learning applications of SIs with pre-computed games and ground-truth values to systematically assess computational performance across domains. For practitioners, shapiq is able to explain and visualize any-order feature interactions in predictions of models, including vision transformers, language models, as well as XGBoost and LightGBM with TreeSHAP-IQ. With shapiq, we extend shap beyond feature attributions and consolidate the application of SVs and SIs in machine learning that facilitates future research. The source code and documentation are available at https://github.com/mmschlk/shapiq.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/mmschlk/shapiq"}, "reproduce_difficulty": {"environment_setup": "Easy, can be installed using pip.", "resource_requirements": "Standard CPU is sufficient, no specific GPU required.", "time_requirements": "Approximately 1 hour to run the main example.", "code_quality": "Well-documented code.", "difficulty": 2, "stars": 408}}
{"id": "97454", "url": "https://nips.cc/virtual/2024/poster/97454", "title": "CALE: Continuous Arcade Learning Environment", "authors": [], "abstract": "Abstract:We introduce the Continuous Arcade Learning Environment (CALE), an extension of the well-known Arcade Learning Environment (ALE) [Bellemare et al., 2013]. The CALE uses the same underlying emulator of the Atari 2600 gaming system (Stella), but adds support for continuous actions. This enables the benchmarking and evaluation of continuous-control agents (such as PPO [Schulman et al., 2017] and SAC [Haarnoja et al., 2018]) and value-based agents (such as DQN [Mnih et al., 2015] and Rainbow [Hessel et al., 2018]) on the same environment suite. We provide a series of open questions and research directions that CALE enables, as well as initial baseline results using Soft Actor-Critic. CALE is available as part of the ALE athttps://github.com/Farama-Foundation/Arcade-Learning-Environment.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Farama-Foundation/Arcade-Learning-Environment"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "Optional, depends on the complexity of the agents being developed", "memory": "Not specified, but typical requirements for running AI agents in Python", "API_calls": "None specified"}, "time_requirements": "1", "code_quality": "well-documented code", "difficulty": 2, "stars": 2237}}
{"id": "97581", "url": "https://nips.cc/virtual/2024/poster/97581", "title": "UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling", "authors": [], "abstract": "Abstract:Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. Yet, with an ever-growing number of benchmarks,researchers are tasked with the heavy burden of implementing each protocol, bearing a non-trivial computational cost, and making sense of how all these benchmarks translate into meaningful axes of progress.To facilitate a systematic evaluation of VLM progress, we introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a range of carefully categorized vision-centric capabilities from object recognition to spatial awareness, counting, and much more. We showcase the utility of UniBench for measuring progress by evaluating nearly 60 publicly available vision-language models, trained on scales of up to 12.8B samples. We find that while scaling training data or model size can boost many vision-language model capabilities, scaling offers little benefit for reasoning or relations.  Surprisingly, we also discover today's best VLMs struggle on simple digit recognition and counting tasks, e.g. MNIST, which much simpler networks can solve. Where scale falls short, we find that more precise interventions, such as data quality or tailored-learning objectives offer more promise. For practitioners, we also offer guidance on selecting a suitable VLM for a given application. Finally, we release an easy-to-run UniBench code-base with the full set of 50+ benchmarks and comparisons across 59 models as well as a distilled, representative set of benchmarks that runs in 5 minutes on a single GPU. UniBench with model evaluations on all benchmarks are provided as a toolbox at: https://github.com/facebookresearch/unibench", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/facebookresearch/unibench"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires conda for environment setup and specific package installations)", "resource_requirements": "1 GPU (NVIDIA recommended), 8GB RAM minimum", "time_requirements": "1-2 hours (depends on dataset size and model complexity)", "code_quality": "Well-documented code with clear instructions and examples", "difficulty": 3, "stars": 192}}
{"id": "97769", "url": "https://nips.cc/virtual/2024/poster/97769", "title": "MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens", "authors": [], "abstract": "Abstract:Multimodal interleaved datasets featuring free-form interleaved sequences of images and text are crucial for training frontier large multimodal models (LMMs). Despite the rapid progression of open-source LMMs, there remains a pronounced scarcity of large-scale, open-source multimodal interleaved datasets.In response, we introduce MINT-1T, the most extensive and diverse open-source Multimodal INTerleaved dataset to date. MINT-1T comprises of one trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. As scaling multimodal interleaved datasets requires substantial engineering effort, sharing the data curation process and releasing the dataset greatly benefits the community. Our experiments show that LMMs trained on MINT-1T rival the performance of models trained on the previous leading dataset, OBELICS. We release our data at https://github.com/mlfoundations/MINT-1T.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/mlfoundations/MINT-1T"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": "1-2 NVIDIA A100 or equivalent", "memory": "32 GB RAM", "storage": "10 TB disk space for dataset"}, "time_requirements": "8 hours", "code_quality": "Well-documented", "difficulty": 3, "stars": 799}}
{"id": "97532", "url": "https://nips.cc/virtual/2024/poster/97532", "title": "TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series", "authors": [], "abstract": "Abstract:Time series data are essential in a wide range of machine learning (ML) applications. However, temporal data are often scarce or highly sensitive, limiting data sharing and the use of data-intensive ML methods. A possible solution to this problem is the generation of synthetic datasets that resemble real data. In this work, we introduce Time Series Generative Modeling (TSGM), an open-source framework for the generative modeling and evaluation of synthetic time series datasets. TSGM includes a broad repertoire of machine learning methods: generative models, probabilistic, simulation-based approaches, and augmentation techniques. The framework enables users to evaluate the quality of the produced data from different angles: similarity, downstream effectiveness, predictive consistency, diversity, fairness, and privacy. TSGM is extensible and user-friendly, which allows researchers to rapidly implement their own methods and compare them in a shareable environment. The framework has been tested on open datasets and in production and proved to be beneficial in both cases. https://github.com/AlexanderVNikitin/tsgm", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/AlexanderVNikitin/tsgm"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires both pip and conda for some dependencies)", "resource_requirements": {"GPUs": "1 x NVIDIA Tesla V100 or similar", "memory": "At least 16 GB RAM recommended", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "Well-documented code with clear installation and usage instructions", "difficulty": 3, "stars": 152}}
{"id": "97440", "url": "https://nips.cc/virtual/2024/poster/97440", "title": "Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification", "authors": [], "abstract": "Abstract:Graph Transformers (GTs) have recently emerged as popular alternatives to traditional message-passing Graph Neural Networks (GNNs), due to their theoretically superior expressiveness and impressive performance reported on standard node classification benchmarks, often significantly outperforming GNNs. In this paper, we conduct a thorough empirical analysis to reevaluate the performance of three classic GNN models (GCN, GAT, and GraphSAGE) against GTs. Our findings suggest that the previously reported superiority of GTs may have been overstated due to suboptimal hyperparameter configurations in GNNs. Remarkably, with slight hyperparameter tuning, these classic GNN models achieve state-of-the-art performance, matching or even exceeding that of recent GTs across 17 out of the 18 diverse datasets examined. Additionally, we conduct detailed ablation studies to investigate the influence of various GNN configurations\u2014such as normalization, dropout, residual connections, and network depth\u2014on node classification performance. Our study aims to promote a higher standard of empirical rigor in the field of graph machine learning, encouraging more accurate comparisons and evaluations of model capabilities. Our implementation is available at https://github.com/LUOyk1999/tunedGNN.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/LUOyk1999/tunedGNN"}, "reproduce_difficulty": {"environment_setup": "conda", "resource_requirements": {"GPUs": "None specified", "memory": "None specified", "API_calls": "None specified"}, "time_requirements": 1, "code_quality": "well-documented code", "difficulty": 2, "stars": 120}}
{"id": "97779", "url": "https://nips.cc/virtual/2024/poster/97779", "title": "NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking", "authors": [], "abstract": "Abstract:Benchmarking vision-based driving policies is challenging. On one hand, open-loop evaluation with real data is easy, but these results do not reflect closed-loop performance. On the other, closed-loop evaluation is possible in simulation, but is hard to scale due to its significant computational demands. Further, the simulators available today exhibit a large domain gap to real data. This has resulted in an inability to draw clear conclusions from the rapidly growing body of research on end-to-end autonomous driving. In this paper, we present NAVSIM, a middle ground between these evaluation paradigms, where we use large datasets in combination with a non-reactive simulator to enable large-scale real-world benchmarking. Specifically, we gather simulation-based metrics, such as progress and time to collision, by unrolling bird's eye view abstractions of the test scenes for a short simulation horizon. Our simulation is non-reactive, i.e., the evaluated policy and environment do not influence each other. As we demonstrate empirically, this decoupling allows open-loop metric computation while being better aligned with closed-loop evaluations than traditional displacement errors. NAVSIM enabled a new competition held at CVPR 2024, where 143 teams submitted 463 entries, resulting in several new insights. On a large set of challenging scenarios, we observe that simple methods with moderate compute requirements such as TransFuser can match recent large-scale end-to-end driving architectures such as UniAD. Our modular framework can potentially be extended with new datasets, data curation strategies, and metrics, and will be continually maintained to host future challenges. Our code is available at https://github.com/autonomousvision/navsim.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/autonomousvision/navsim"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "8GB RAM", "other": "Python 3.x, various Python packages"}, "time_requirements": "2", "code_quality": "Well-documented", "difficulty": 3, "stars": 382}}
{"id": "93911", "url": "https://nips.cc/virtual/2024/poster/93911", "title": "Large Language Models Play StarCraft II:Benchmarks and A Chain of Summarization Approach", "authors": [], "abstract": "Abstract:With the continued advancement of Large Language Models (LLMs) Agents in reasoning, planning, and decision-making, benchmarks have become crucial in evaluating these skills. However, there is a notable gap in benchmarks for real-time strategic decision-making. StarCraft II (SC2), with its complex and dynamic nature, serves as an ideal setting for such evaluations. To this end, we have developed TextStarCraft II, a specialized environment for assessing LLMs in real-time strategic scenarios within SC2. Addressing the limitations of traditional Chain of Thought (CoT) methods, we introduce the Chain of Summarization (CoS) method, enhancing LLMs' capabilities in rapid and effective decision-making. Our key experiments included:1. LLM Evaluation: Tested 10 LLMs in TextStarCraft II, most of them defeating LV5 build-in AI, showcasing effective strategy skills.2. Commercial Model Knowledge: Evaluated four commercial  models on SC2 knowledge; GPT-4 ranked highest by Grandmaster-level experts.3. Human-AI Matches: Experimental results showed that fine-tuned LLMs performed on par with Gold-level players in real-time matches, demonstrating comparable strategic abilities.All code and data from thisstudy have been made pulicly available at https://github.com/histmeisah/Large-Language-Models-play-StarCraftII", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/histmeisah/Large-Language-Models-play-StarCraftII"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA V100", "amount": 1}, "CPUs": 1, "memory": "8GB", "other": "OpenAI API key required"}, "time_requirements": 7, "code_quality": "Well-documented code", "difficulty": 3, "stars": 249}}
{"id": "93709", "url": "https://nips.cc/virtual/2024/poster/93709", "title": "UniTS: A Unified Multi-Task Time Series Model", "authors": [], "abstract": "Abstract:Although pre-trained transformers and reprogrammed text-based LLMs have shown strong performance on time series tasks, the best-performing architectures vary widely across tasks, with most models narrowly focused on specific areas, such as time series forecasting. Unifying predictive and generative time series tasks within a single model remains challenging. We introduce UniTS, a unified multi-task time series model that utilizes task tokenization to integrate predictive and generative tasks into a single framework. UniTS employs a modified transformer block to capture universal time series representations, enabling transferability from a heterogeneous, multi-domain pre-training dataset\u2014characterized by diverse dynamic patterns, sampling rates, and temporal scales\u2014to a wide range of downstream datasets with varied task specifications and data domains. Tested on 38 datasets across human activity sensors, healthcare, engineering, and finance, UniTS achieves superior performance compared to 12 forecasting models, 20 classification models, 18 anomaly detection models, and 16 imputation models, including adapted text-based LLMs. UniTS also demonstrates strong few-shot and prompt capabilities when applied to new domains and tasks. In single-task settings, UniTS outperforms competitive task-specialized time series models. Code and datasets are available at https://github.com/mims-harvard/UniTS.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/mims-harvard/UniTS"}, "reproduce_difficulty": {"environment_setup": "moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_calls": "none"}, "time_requirements": 2, "code_quality": "well-documented", "difficulty": 3, "stars": 507}}
{"id": "93507", "url": "https://nips.cc/virtual/2024/poster/93507", "title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning", "authors": [], "abstract": "Abstract:Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA. However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets. This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance. Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA. Building on these insights, we have developed HydraLoRA, a LoRA framework with an asymmetric structure that eliminates the need for domain expertise. Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases. Our anonymous codes are submitted with the paper and will be publicly available. Code is available: https://github.com/Clin0212/HydraLoRA.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Clin0212/HydraLoRA"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": [{"type": "NVIDIA", "amount": 1}], "memory": "16GB", "API_calls": "None"}, "time_requirements": 2, "code_quality": "well-documented", "difficulty": 3, "stars": 167}}
{"id": "96675", "url": "https://nips.cc/virtual/2024/poster/96675", "title": "Long-form factuality in large language models", "authors": [], "abstract": "Abstract:Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model\u2019s long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user\u2019s preferred response length (recall).Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotators\u2014on a set of\u223c16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators.  We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/google-deepmind/long-form-factuality"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_call": "OpenAI or Anthropic"}, "time_requirements": "2", "code_quality": "well-documented code", "difficulty": 3, "stars": 586}}
{"id": "96041", "url": "https://nips.cc/virtual/2024/poster/96041", "title": "MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making", "authors": [], "abstract": "Abstract:Foundation models are becoming valuable tools in medicine. Yet despite their promise, the best way to leverage Large Language Models (LLMs) in complex medical tasks remains an open question. We introduce a novel multi-agent framework, named **M**edical **D**ecision-making **Agents** (**MDAgents**) that helps to address this gap by automatically assigning a collaboration structure to a team of LLMs. The assigned solo or group collaboration structure is tailored to the medical task at hand, a simple emulation inspired by the way real-world medical decision-making processes are adapted to tasks of different complexities. We evaluate our framework and baseline methods using state-of-the-art LLMs across a suite of real-world medical knowledge and clinical diagnosis benchmarks, including a comparison ofLLMs\u2019 medical complexity classification against human physicians. MDAgents achieved the **best performance in seven out of ten** benchmarks on tasks requiring an understanding of medical knowledge and multi-modal reasoning, showing a significant **improvement of up to 4.2\\%** ($p$ < 0.05) compared to previous methods' best performances. Ablation studies reveal that MDAgents effectively determines medical complexity to optimize for efficiency and accuracy across diverse medical tasks. Notably, the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy **improvement of 11.8\\%**. Our code can be found at https://github.com/mitmedialab/MDAgents.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/mitmedialab/MDAgents"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "1 x NVIDIA GPU with at least 8GB memory", "memory": "8GB RAM minimum", "API_call": "Requires OpenAI API key and GenAI API key"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 117}}
{"id": "95721", "url": "https://nips.cc/virtual/2024/poster/95721", "title": "Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention", "authors": [], "abstract": "Abstract:Conditional diffusion models have shown remarkable success in visual content generation, producing high-quality samples across various domains, largely due to classifier-free guidance (CFG). Recent attempts to extend guidance to unconditional models have relied on heuristic techniques, resulting in suboptimal generation quality and unintended effects. In this work, we propose Smoothed Energy Guidance (SEG), a novel training- and condition-free approach that leverages the energy-based perspective of the self-attention mechanism to enhance image generation. By defining the energy of self-attention, we introduce a method to reduce the curvature of the energy landscape of attention and use the output as the unconditional prediction. Practically, we control the curvature of the energy landscape by adjusting the Gaussian kernel parameter while keeping the guidance scale parameter fixed. Additionally, we present a query blurring method that is equivalent to blurring the entire attention weights without incurring quadratic complexity in the number of tokens. In our experiments, SEG achieves a Pareto improvement in both quality and the reduction of side effects. The code is available at https://github.com/SusungHong/SEG-SDXL.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/SusungHong/SEG-SDXL"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires installing specific versions of torch and diffusers)", "resource_requirements": "1 GPU (CUDA compatible, ideally NVIDIA), 8GB RAM minimum, torch 2.0.1, diffusers 0.27.2", "time_requirements": "1 hour", "code_quality": "Well-documented code", "difficulty": 3, "stars": 117}}
{"id": "96223", "url": "https://nips.cc/virtual/2024/poster/96223", "title": "MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views", "authors": [], "abstract": "Abstract:We introduce MVSplat360, a feed-forward approach for 360\u00b0 novel view synthesis (NVS) of diverse real-world scenes, using only sparse observations. This setting is inherently ill-posed due to minimal overlap among input views and insufficient visual information provided, making it challenging for conventional methods to achieve high-quality results. Our MVSplat360 addresses this by effectively combining geometry-aware 3D reconstruction with temporally consistent video generation. Specifically, it refactors a feed-forward 3D Gaussian Splatting (3DGS) model to render features directly into the latent space of a pre-trained Stable Video Diffusion (SVD) model, where these features then act as pose and visual cues to guide the denoising process and produce photorealistic 3D-consistent views. Our model is end-to-end trainable and supports rendering arbitrary views with as few as 5 sparse input views. To evaluate MVSplat360's performance, we introduce a new benchmark using the challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual quality compared to state-of-the-art methods on wide-sweeping or even 360\u00b0 NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the effectiveness of our model. Readers are highly recommended to view the video results atdonydchen.github.io/mvsplat360.", "pdf_url": "", "supplementary_url": "", "code_url": "https://donydchen.github.io/mvsplat360", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/donydchen/mvsplat360"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 or more GPUs with at least 22G VRAM for evaluation and 80G for training", "memory": "22G or 80G VRAM depending on the task"}, "time_requirements": "2-4 hours for training depending on the dataset size", "code_quality": "well-documented code", "difficulty": 3, "stars": 231}}
{"id": "96085", "url": "https://nips.cc/virtual/2024/poster/96085", "title": "Are Language Models Actually Useful for Time Series Forecasting?", "authors": [], "abstract": "Abstract:Large language models (LLMs) are being applied to time series forecasting. But are language models actually useful for time series? In a series of ablation studies on three recent and popular LLM-based time series forecasting methods, we find that removing the LLM component or replacing it with a basic attention layer does not degrade forecasting performance---in most cases, the results even improve! We also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings. Additionally, we explore time series encoders and find that patching and attention structures perform similarly to LLM-based forecasters. All resources needed to reproduce our work are available: https://github.com/BennyTMT/LLMsForTimeSeries.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/BennyTMT/LLMsForTimeSeries"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires setup of different repositories and scripts for environment)", "resource_requirements": {"GPUs": "N/A", "memory": "N/A", "API_call": "N/A"}, "time_requirements": "2", "code_quality": "Well-documented code", "difficulty": 3, "stars": 102}}
{"id": "96771", "url": "https://nips.cc/virtual/2024/poster/96771", "title": "X-Ray: A Sequential 3D Representation For Generation", "authors": [], "abstract": "Abstract:We introduce X-Ray, a novel 3D sequential representation inspired by the penetrability of x-ray scans. X-Ray transforms a 3D object into a series of surface frames at different layers, making it suitable for generating 3D models from images. Our method utilizes ray casting from the camera center to capture geometric and textured details, including depth, normal, and color, across all intersected surfaces. This process efficiently condenses the whole 3D object into a multi-frame video format, motivating the utilize of a network architecture similar to those in video diffusion models. This design ensures an efficient 3D representation by focusing solely on surface information. Also, we propose a two-stage pipeline to generate 3D objects from X-Ray Diffusion Model and Upsampler. We demonstrate the practicality and adaptability of our X-Ray representation by synthesizing the complete visible and hidden surfaces of a 3D object from a single input image. Experimental results reveal the state-of-the-art superiority of our representation in enhancing the accuracy of 3D generation, paving the way for new 3D representation research and practical applications. Our project page is in \\url{https://tau-yihouxiang.github.io/projects/X-Ray/X-Ray.html}.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/tau-yihouxiang/X-Ray"}, "reproduce_difficulty": {"environment_setup": "Medium (requires conda and pip for installation)", "resource_requirements": {"GPUs": {"type": "NVIDIA GPU", "amount": 1}, "memory": "At least 8 GB RAM", "API_call": "Hugging Face API for dataset access"}, "time_requirements": 4, "code_quality": "Well-documented code with clear instructions", "difficulty": 3, "stars": 108}}
{"id": "95654", "url": "https://nips.cc/virtual/2024/poster/95654", "title": "PointMamba: A Simple State Space Model for Point Cloud Analysis", "authors": [], "abstract": "Abstract:Transformers have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability. However, the attention mechanism has quadratic complexity, making the design of a linear complexity method with global modeling appealing. In this paper, we propose PointMamba, transferring the success of Mamba, a recent representative state space model (SSM), from NLP to point cloud analysis tasks. Unlike traditional Transformers, PointMamba employs a linear complexity algorithm, presenting global modeling capacity while significantly reducing computational costs. Specifically, our method leverages space-filling curves for effective point tokenization and adopts an extremely simple, non-hierarchical Mamba encoder as the backbone. Comprehensive evaluations demonstrate that PointMamba achieves superior performance across multiple datasets while significantly reducing GPU memory usage and FLOPs. This work underscores the potential of SSMs in 3D vision-related tasks and presents a simple yet effective Mamba-based baseline for future research. The code is available at https://github.com/LMD0311/PointMamba.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/LMD0311/PointMamba"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires specific datasets and environment configuration, potentially using pip or conda)", "resource_requirements": {"GPUs": [{"type": "NVIDIA", "amount": 1}], "memory": "8GB or more recommended"}, "time_requirements": 2, "code_quality": "Well-documented", "difficulty": 3, "stars": 394}}
{"id": "94905", "url": "https://nips.cc/virtual/2024/poster/94905", "title": "Autoregressive Image Generation without Vector Quantization", "authors": [], "abstract": "Abstract:Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available athttps://github.com/LTH14/mar.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/LTH14/mar", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/LTH14/mar"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": "8-32 H100 GPUs", "memory": "Varies based on model size, up to 943M params", "api_calls": "None specified"}, "time_requirements": "1-2 days", "code_quality": "Well-documented code", "difficulty": 3, "stars": 1309}}
{"id": "96925", "url": "https://nips.cc/virtual/2024/poster/96925", "title": "The Road Less Scheduled", "authors": [], "abstract": "Abstract:Existing learning rate schedules that do not require specification of the optimization stopping step $T$ are greatly out-performed by learning rate schedules that depend on $T$. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available at https://github.com/facebookresearch/schedule_free. Schedule-Free AdamW is the core algorithm behind our winning entry to the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/facebookresearch/schedule_free"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 NVIDIA GPU (e.g. GTX 1080 or equivalent)", "memory": "8 GB RAM", "API_calls": "none"}, "time_requirements": "2", "code_quality": "well-documented code", "difficulty": 2, "stars": 2105}}
{"id": "95715", "url": "https://nips.cc/virtual/2024/poster/95715", "title": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization", "authors": [], "abstract": "Abstract:Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks. Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM. However, current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs. To address this, we propose accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM. Specifically, we quantize each weight matrix into binary matrices paired with group-wise scaling factors. The associated multiplications are reparameterized into (1) shifts between activations and scaling factors and (2) queries and adds according to the binary matrices. To reduce accuracy loss, we present a multi-objective optimization method to minimize both weight and output activation reparameterization errors. Additionally, based on varying sensitivity across layers to reparameterization, we develop an automated bit allocation strategy to further reduce memory usage and latency. Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity reductions of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3- and 2-bit precision, respectively, and more than 80% memory and energy reductions over the original LLMs. Codes and models are available at https://github.com/GATECH-EIC/ShiftAddLLM.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/GATECH-EIC/ShiftAddLLM"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires conda and setup of environment.yml)", "resource_requirements": {"GPUs": "1 or more GPUs (exact type not specified)", "memory": "Not specified", "API_call": "Hugging Face model inference"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 103}}
{"id": "96507", "url": "https://nips.cc/virtual/2024/poster/96507", "title": "DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation", "authors": [], "abstract": "Abstract:Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets.To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model.GenIR, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models. GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation \\& filtering. This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing a cost-effective, privacy-safe solution for IR dataset construction. The result is a large-scale dataset of one million high-quality images.Our second contribution,DreamClear, is a DiT-based image restoration model. It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration. To boost the model's adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM). It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address.Our exhaustive experiments confirm DreamClear's superior performance, underlining the efficacy of our dual strategy for real-world image restoration. Code and pre-trained models are available at: https://github.com/shallowdream204/DreamClear.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/shallowdream204/DreamClear"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": "2", "memory": "Not specified", "API_calls": "Not specified"}, "time_requirements": 12, "code_quality": "Well-documented code", "difficulty": 3, "stars": 984}}
{"id": "96428", "url": "https://nips.cc/virtual/2024/poster/96428", "title": "Invisible Image Watermarks Are Provably Removable Using Generative AI", "authors": [], "abstract": "Abstract:Invisible watermarks safeguard images' copyrights by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models.We propose a family of regeneration attacks to remove these invisible watermarks. The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image. This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and extensive empirical evaluations, we demonstrate that pixel-level invisible watermarks are vulnerable to this regeneration attack.Our results reveal that, across four different pixel-level watermarking schemes, the proposed method consistently achieves superior performance compared to existing attack techniques, with lower detection rates and higher image quality.However, watermarks that keep the image semantically similar can be an alternative defense against our attacks.Our finding underscores the need for a shift in research/industry emphasis from invisible watermarks to semantic-preserving watermarks. Code is available at https://github.com/XuandongZhao/WatermarkAttacker", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/XuandongZhao/WatermarkAttacker"}, "reproduce_difficulty": {"environment_setup": "Easy (pip install -r requirements.txt)", "resource_requirements": "1 GPU (NVIDIA recommended), 8GB RAM", "time_requirements": "2 hours", "code_quality": "Well-documented code", "difficulty": 2, "stars": 206}}
{"id": "95449", "url": "https://nips.cc/virtual/2024/poster/95449", "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation", "authors": [], "abstract": "Abstract:A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is that while increasing the number of vision tokens generally enhances visual understanding, it also significantly raises memory and computational costs, especially in long-term, dense video frame streaming scenarios. Although learnable approaches like Q-Former and Perceiver Resampler have been developed to reduce the vision token burden, they overlook the context causally modeled by LLMs (i.e., key-value cache), potentially leading to missed visual cues when addressing user queries. In this paper, we introduce a novel approach to reduce vision compute by leveraging redundant vision tokens ``skipping layers'' rather than decreasing the number of vision tokens. Our method, VideoLLM-MoD, is inspired by mixture-of-depths LLMs and addresses the challenge of numerous vision tokens in long-term or streaming video. Specifically, for certain transformer layer, we learn to skip the computation for a high proportion (e.g., 80\\%) of vision tokens, passing them directly to the next layer. This approach significantly enhances model efficiency, achieving approximately 42% time and 30% memory savings for the entire training. Moreover, our method reduces the computation in the context and avoid decreasing the vision tokens, thus preserving or even improving performance compared to the vanilla model. We conduct extensive experiments to demonstrate the effectiveness of VideoLLM-MoD, showing its state-of-the-art results on multiple benchmarks, including narration, forecasting, and summarization tasks in COIN, Ego4D, and Ego-Exo4D datasets. The code and checkpoints will be made available at github.com/showlab/VideoLLM-online.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/showlab/VideoLLM-online"}, "reproduce_difficulty": {"environment_setup": "Moderate (uses conda and pip for installation)", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1, "recommended": "A100 or 3090"}, "memory": "At least 16 GB", "additional_resources": "ffmpeg for video processing"}, "time_requirements": "Approximately 3 hours for setup and running initial commands", "code_quality": "Well-documented code", "difficulty": 3, "stars": 325}}
{"id": "94507", "url": "https://nips.cc/virtual/2024/poster/94507", "title": "NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing", "authors": [], "abstract": "Abstract:We propose a video editing framework, NaRCan, which integrates a hybrid deformation field and diffusion prior to generate high-quality natural canonical images to represent the input video. Our approach utilizes homography to model global motion and employs multi-layer perceptrons (MLPs) to capture local residual deformations, enhancing the model\u2019s ability to handle complex video dynamics. By introducing a diffusion prior from the early stages of training, our model ensures that the generated images retain a high-quality natural appearance, making the produced canonical images suitable for various downstream tasks in video editing, a capability not achieved by current canonical-based methods. Furthermore, we incorporate low-rank adaptation (LoRA) fine-tuning and introduce a noise and diffusion prior update scheduling technique that accelerates the training process by 14 times. Extensive experimental results show that our method outperforms existing approaches in various video editing tasks and produces coherent and high-quality edited video sequences. See our project page for video results:koi953215.github.io/NaRCan_page.", "pdf_url": "", "supplementary_url": "", "code_url": "https://koi953215.github.io/NaRCan_page/", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/koi953215/NaRCan"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "1 or more (specific type not mentioned)", "memory": "large amount of GPU memory required"}, "time_requirements": 4, "code_quality": "well-documented", "difficulty": 3, "stars": 162}}
{"id": "96055", "url": "https://nips.cc/virtual/2024/poster/96055", "title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "authors": [], "abstract": "Abstract:We propose Pure and Lightning ID customization (PuLID), a novel tuning-free ID customization method for text-to-image generation. By incorporating a Lightning T2I branch with a standard diffusion one, PuLID introduces both contrastive alignment loss and accurate ID loss, minimizing disruption to the original model and ensuring high ID fidelity. Experiments show that PuLID achieves superior performance in both ID fidelity and editability. Another attractive property of PuLID is that the image elements (\\eg, background, lighting, composition, and style) before and after the ID insertion are kept as consistent as possible. Codes and models are available at https://github.com/ToTheBeginning/PuLID", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/ToTheBeginning/PuLID"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA A100 or RTX 30 series", "amount": 1}, "memory": "16GB", "other": "Requires Python >= 3.9 and PyTorch >= 2.0"}, "time_requirements": "1-2", "code_quality": "well-documented", "difficulty": 3, "stars": 3134}}
{"id": "95747", "url": "https://nips.cc/virtual/2024/poster/95747", "title": "Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image Generation", "authors": [], "abstract": "Abstract:Controllable text-to-image (T2I) diffusion models have shown impressive performance in generating high-quality visual content through the incorporation of various conditions. Current methods, however, exhibit limited performance when guided by skeleton human poses, especially in complex pose conditions such as side or rear perspectives of human figures. To address this issue, we present Stable-Pose, a novel adapter model that introduces a coarse-to-fine attention masking strategy into a vision Transformer (ViT) to gain accurate pose guidance for T2I models. Stable-Pose is designed to adeptly handle pose conditions within pre-trained Stable Diffusion, providing a refined and efficient way of aligning pose representation during image synthesis. We leverage the query-key self-attention mechanism of ViTs to explore the interconnections among different anatomical parts in human pose skeletons. Masked pose images are used to smoothly refine the attention maps based on target pose-related features in a hierarchical manner, transitioning from coarse to fine levels. Additionally, our loss function is formulated to allocate increased emphasis to the pose region, thereby augmenting the model's precision in capturing intricate pose details.    We assessed the performance of Stable-Pose across five public datasets under a wide range of indoor and outdoor human pose scenarios. Stable-Pose achieved an AP score of 57.1 in the LAION-Human dataset, marking around 13\\% improvement over the established technique ControlNet.  The project link and code is available at https://github.com/ai-med/StablePose.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/ai-med/StablePose"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 2}, "memory": "16GB", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 100}}
{"id": "95671", "url": "https://nips.cc/virtual/2024/poster/95671", "title": "RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance", "authors": [], "abstract": "Abstract:Customizing diffusion models to generate identity-preserving images from user-provided reference images is an intriguing new problem. The prevalent approaches typically require training on extensive domain-specific images to achieve identity preservation, which lacks flexibility across different use cases. To address this issue, we exploit classifier guidance, a training-free technique that steers diffusion models using an existing classifier, for personalized image generation. Our study shows that based on a recent rectified flow framework, the major limitation of vanilla classifier guidance in requiring a special classifier can be resolved with a simple fixed-point solution, allowing flexible personalization with off-the-shelf image discriminators. Moreover, its solving procedure proves to be stable when anchored to a reference flow trajectory, with a convergence guarantee. The derived method is implemented on rectified flow with different off-the-shelf image discriminators, delivering advantageous personalization results for human faces, live subjects, and certain objects. Code is available at https://github.com/feifeiobama/RectifID.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/feifeiobama/RectifID"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires installation of several dependencies including deep learning libraries and models)", "resource_requirements": "A GPU with a minimum of 12.5GB memory is required.", "time_requirements": "Approximately 1 hour to run the main commands.", "code_quality": "Well-documented code with clear instructions for setup and usage.", "difficulty": 3, "stars": 124}}
{"id": "95573", "url": "https://nips.cc/virtual/2024/poster/95573", "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models", "authors": [], "abstract": "Abstract:Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at https://github.com/NVlabs/MaskLLM.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/NVlabs/MaskLLM"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA GPUs", "amount": 8}, "memory": "Approximately 40GB per GPU"}, "time_requirements": "4", "code_quality": "Well-documented code", "difficulty": 3, "stars": 152}}
{"id": "95465", "url": "https://nips.cc/virtual/2024/poster/95465", "title": "AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning", "authors": [], "abstract": "Abstract:Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demon- strate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/zou-group/avatar"}, "reproduce_difficulty": {"environment_setup": "Moderate (uses conda and pip for installation)", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "at least 16GB", "API_calls": "requires API keys for OpenAI and Anthropic"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 181}}
{"id": "95327", "url": "https://nips.cc/virtual/2024/poster/95327", "title": "Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration", "authors": [], "abstract": "Abstract:Membership Inference Attacks (MIA) aim to infer whether a target data record has been utilized for model training or not. Existing MIAs designed for large language models (LLMs) can be bifurcated into two types: reference-free and reference-based attacks. Although reference-based attacks appear promising performance by calibrating the probability measured on the target model with reference models, this illusion of privacy risk heavily depends on a reference dataset that closely resembles the training set. Both two types of attacks are predicated on the hypothesis that training records consistently maintain a higher probability of being sampled. However, this hypothesis heavily relies on the overfitting of target models, which will be mitigated by multiple regularization methods and the generalization of LLMs. Thus, these reasons lead to high false-positive rates of MIAs in practical scenarios.We propose a Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA). Specifically, we introduce a self-prompt approach, which constructs the dataset to fine-tune the reference model by prompting the target LLM itself. In this manner, the adversary can collect a dataset with a similar distribution from public APIs.Furthermore, we introduce probabilistic variation, a more reliable membership signal based on LLM memorization rather than overfitting, from which we rediscover the neighbour attack with theoretical grounding. Comprehensive evaluation conducted on three datasets and four exemplary LLMs shows that SPV-MIA raises the AUC of MIAs from 0.7 to a significantly high level of 0.9. Our code and dataset are available at: https://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": "1 or more GPUs (recommended multi-GPU setup), 16GB+ RAM", "time_requirements": "2-4 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 175}}
{"id": "95294", "url": "https://nips.cc/virtual/2024/poster/95294", "title": "Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers", "authors": [], "abstract": "Abstract:One of the roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. This work studies the problem of learning policy representations through heterogeneous pre-training on robot data across different embodiments and tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation. This general architecture aligns the specific proprioception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks. Leveraging the recent large-scale multi-embodiment real-world robotic datasets as well as simulation, deployed robots, and human video datasets, we investigate pre-training policies across heterogeneity. We conduct experiments to investigate the scaling behaviors of training objectives, to the extent of 52 datasets. HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20% on unseen tasks in multiple simulator benchmarks and real-world settings. See the project website (liruiw.github.io/hpt) for code and videos.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/liruiw/hpt"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_call": "Hugging Face models"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 465}}
{"id": "93727", "url": "https://nips.cc/virtual/2024/poster/93727", "title": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs", "authors": [], "abstract": "Abstract:Quantization of large language models (LLMs) faces significant challenges, particularly due to the presence of outlier activations that impede efficient low-bit representation. Traditional approaches predominantly address Normal Outliers, which are activations across all tokens with relatively large magnitudes. However, these methods struggle with smoothing Massive Outliers that display significantly larger values, which leads to significant performance degradation in low-bit quantization. In this paper, we introduce DuQuant, a novel approach that utilizes rotation and permutation transformations to more effectively mitigate both massive and normal outliers. First, DuQuant starts by constructing the rotation matrix, using specific outlier dimensions as prior knowledge, to redistribute outliers to adjacent channels by block-wise rotation. Second, We further employ a zigzag permutation to balance the distribution of outliers across blocks, thereby reducing block-wise variance. A subsequent rotation further smooths the activation landscape, enhancing model performance. DuQuant simplifies the quantization process and excels in managing outliers, outperforming the state-of-the-art baselines across various sizes and types of LLMs on multiple tasks, even with 4-bit weight-activation quantization. Our code is available at https://github.com/Hsu1023/DuQuant.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Hsu1023/DuQuant"}, "reproduce_difficulty": {"environment_setup": "Moderate - Requires conda for environment management and pip for package installation.", "resource_requirements": "1 GPU (NVIDIA recommended), with at least 16GB of memory for effective performance.", "time_requirements": "Approximately 2 hours to run the main command, including setup time.", "code_quality": "Well-documented code with clear instructions for installation and usage.", "difficulty": 3, "stars": 143}}
{"id": "95195", "url": "https://nips.cc/virtual/2024/poster/95195", "title": "RealCompo: Balancing Realism and Compositionality Improves Text-to-Image Diffusion Models", "authors": [], "abstract": "Abstract:Diffusion models have achieved remarkable advancements in text-to-image generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we proposeRealCompo, a newtraining-freeandtransferred-friendlytext-to-image generation framework, which aims to leverage the respective advantages of text-to-image models and spatial-aware image diffusion models (e.g., layout, keypoints and segmentation maps) to enhance both realism and compositionality of the generated images. An intuitive and novelbalanceris proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and spatial-aware image diffusion models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Notably, our RealCompo can be seamlessly extended with a wide range of spatial-aware image diffusion models and stylized diffusion models. Code is available at: https://github.com/YangLing0818/RealCompo", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/YangLing0818/RealCompo"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "1x NVIDIA GPU (e.g., GTX 1080 or better)", "memory": "At least 16 GB RAM", "API_calls": "Required for certain functionalities"}, "time_requirements": 1, "code_quality": "well-documented code", "difficulty": 3, "stars": 114}}
{"id": "95100", "url": "https://nips.cc/virtual/2024/poster/95100", "title": "U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers", "authors": [], "abstract": "Abstract:Diffusion Transformers (DiTs) introduce the transformer architecture to diffusion tasks for latent-space image generation. With an isotropic architecture that chains a series of transformer blocks, DiTs demonstrate competitive performance and good scalability; but meanwhile, the abandonment of U-Net by DiTs and their following improvements is worth rethinking. To this end, we conduct a simple toy experiment by comparing a U-Net architectured DiT with an isotropic one. It turns out that the U-Net architecture only gain a slight advantage amid the U-Net inductive bias, indicating potential redundancies within the U-Net-style DiT. Inspired by the discovery that U-Net backbone features are low-frequency-dominated, we perform token downsampling on the query-key-value tuple for self-attention and bring further improvements despite a considerable amount of reduction in computation. Based on self-attention with downsampled tokens, we propose a series of U-shaped DiTs (U-DiTs) in the paper and conduct extensive experiments to demonstrate the extraordinary performance of U-DiT models. The proposed U-DiT could outperform DiT-XL with only 1/6 of its computation cost. Codes are available at https://github.com/YuchuanTian/U-DiT.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/YuchuanTian/U-DiT"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"count": 1, "type": "NVIDIA", "memory": "8GB or more"}, "RAM": "16GB or more", "disk_space": "21GB for VAE features"}, "time_requirements": 5, "code_quality": "well-documented", "difficulty": 3, "stars": 185}}
{"id": "94872", "url": "https://nips.cc/virtual/2024/poster/94872", "title": "SGLang: Efficient Execution of Structured Language Model Programs", "authors": [], "abstract": "Abstract:Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to $6.4\\times$ higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/sgl-project/sglang"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": "1x NVIDIA A100", "memory": "16GB", "API_calls": "Yes, requires integration with external APIs"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 11236}}
{"id": "94868", "url": "https://nips.cc/virtual/2024/poster/94868", "title": "Pipeline Parallelism with Controllable Memory", "authors": [], "abstract": "Abstract:Pipeline parallelism has been widely explored, but most existing schedules lack a systematic methodology. In this paper, we propose a framework to decompose pipeline schedules as repeating a building block, and show that the lifespan of the building block decides the peak activation memory of the pipeline schedule. Guided by the observations, we find that almost all existing pipeline schedules, to the best of our knowledge, are memory inefficient. To address this, we introduce a family of memory efficient building blocks with controllable activation memory, which can reduce the peak activation memory to 1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable throughput. We can also achieve almost zero pipeline bubbles while maintaining the same activation memory as 1F1B. Our evaluations demonstrate that in pure pipeline parallelism settings, our methods outperform 1F1B by from 7\\% to 55\\% in terms of throughput. When employing a grid search over hybrid parallelism hyperparameters in practical scenarios, our methods demonstrate a 16\\% throughput improvement over the 1F1B baseline for large language models. The implementation is open-sourced at https://github.com/sail-sg/zero-bubble-pipeline-parallelism.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/sail-sg/zero-bubble-pipeline-parallelism"}, "reproduce_difficulty": {"environment_setup": "Moderate difficulty (requires installations via pip or building from source)", "resource_requirements": {"GPUs": "1-8 NVIDIA GPUs (recommended for optimal performance)", "memory": "At least 16GB RAM recommended"}, "time_requirements": "2-4 hours for setup and initial runs", "code_quality": "Well-documented code with examples and usage instructions", "difficulty": 3, "stars": 344}}
{"id": "94514", "url": "https://nips.cc/virtual/2024/poster/94514", "title": "Don't Look Twice: Faster Video Transformers with Run-Length Tokenization", "authors": [], "abstract": "Abstract:Video transformers are slow to train due to extremely large numbers of input tokens, even though many video tokens are repeated over time. Existing methods to remove uninformative tokens either have significant overhead, negating any speedup, or require tuning for different datasets and examples. We present Run-Length Tokenization (RLT), a simple approach to speed up video transformers inspired by run-length encoding for data compression. RLT efficiently finds and removes `runs' of patches that are repeated over time before model inference, then replaces them with a single patch and a positional encoding to represent the resulting token's new length. Our method is content-aware, requiring no tuning for different datasets, and fast, incurring negligible overhead. RLT yields a large speedup in training, reducing the wall-clock time to fine-tune a video transformer by 30% while matching baseline model performance. RLT also works without training, increasing model throughput by 35% with only 0.1% drop in accuracy.RLT speeds up training at 30 FPS by more than 100%, and on longer video datasets, can reduce the token count by up to 80\\%. Our project page is at  rccchoudhury.github.io/projects/rlt.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/rccchoudhury/RLT"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "datasets": ["Kinetics-400", "Something-Something V2"]}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 195}}
{"id": "94482", "url": "https://nips.cc/virtual/2024/poster/94482", "title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos", "authors": [], "abstract": "Abstract:We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designedtoken, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/showlab/VideoLISA"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "A10", "amount": 64}, "memory": "Not specified", "API_call": "Yes, requires Hugging Face API for transformers"}, "time_requirements": 5, "code_quality": "Well-documented code", "difficulty": 3, "stars": 105}}
{"id": "93038", "url": "https://nips.cc/virtual/2024/poster/93038", "title": "Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving", "authors": [], "abstract": "Abstract:Autonomous driving has advanced significantly due to sensors, machine learning, and artificial intelligence improvements. However, prevailing methods struggle with intricate scenarios and causal relationships, hindering adaptability and interpretability in varied environments. To address the above problems, we introduce LeapAD, a novel paradigm for autonomous driving inspired by the human cognitive process. Specifically, LeapAD emulates human attention by selecting critical objects relevant to driving decisions, simplifying environmental interpretation, and mitigating decision-making complexities. Additionally, LeapAD incorporates an innovative dual-process decision-making module, which consists of an Analytic Process (System-II) for thorough analysis and reasoning, along with a Heuristic Process (System-I) for swift and empirical processing. The Analytic Process leverages its logical reasoning to accumulate linguistic driving experience, which is then transferred to the Heuristic Process by supervised fine-tuning. Through reflection mechanisms and a growing memory bank, LeapAD continuously improves itself from past mistakes in a closed-loop environment. Closed-loop testing in CARLA shows that LeapAD outperforms all methods relying solely on camera input, requiring 1-2 orders of magnitude less labeled data. Experiments also demonstrate that as the memory bank expands, the Heuristic Process with only 1.8B parameters can inherit the knowledge from a GPT-4 powered Analytic Process and achieve continuous performance improvement. Project page: https://pjlab-adg.github.io/LeapAD", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/pjlab-adg/LeapAD"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires Docker and a mid-end GPU)", "resource_requirements": "1 mid-end GPU, 8GB RAM minimum, CARLA simulator", "time_requirements": "Approximately 2-4 hours to set up and run main commands", "code_quality": "Well-documented code with detailed installation and usage instructions", "difficulty": 3, "stars": 115}}
{"id": "94431", "url": "https://nips.cc/virtual/2024/poster/94431", "title": "Depth Anything V2", "authors": [], "abstract": "Abstract:This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with sparse depth annotations to facilitate future research. Models are available at https://github.com/DepthAnything/Depth-Anything-V2.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/DepthAnything/Depth-Anything-V2"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "8GB", "api_calls": "None specified"}, "time_requirements": "1", "code_quality": "Well-documented code", "difficulty": 3, "stars": 4755}}
{"id": "93338", "url": "https://nips.cc/virtual/2024/poster/93338", "title": "An Image is Worth 32 Tokens for Reconstruction and Generation", "authors": [], "abstract": "Abstract:Recent advancements in generative models have highlighted the crucial role of image tokenization in the efficient synthesis of high-resolution images. Tokenization, which transforms images into latent representations, reduces computational demands compared to directly processing pixels and enhances the effectiveness and efficiency of the generation process. Prior methods, such as VQGAN, typically utilize 2D latent grids with fixed downsampling factors. However, these 2D tokenizations face challenges in managing the inherent redundancies present in images, where adjacent regions frequently display similarities. To overcome this issue, we introduceTransformer-based 1-DimensionalTokenizer (TiTok), an innovative approach that tokenizes images into 1D latent sequences. TiTok provides a more compact latent representation, yielding substantially more efficient and effective representations than conventional techniques. For example, a 256 \u00d7 256 \u00d7 3 image can be reduced to just32discrete tokens, a significant reduction from the 256 or 1024 tokens obtained by prior methods. Despite its compact nature, TiTok achieves competitive performance to state-of-the-art approaches. Specifically, using the same generator framework, TiTok attains1.97gFID, outperforming MaskGIT baseline significantly by 4.21 at ImageNet 256 \u00d7 256 benchmark. The advantages of TiTok become even more significant when it comes to higher resolution. At ImageNet 512 \u00d7 512 benchmark, TiTok not only outperforms state-of-the-art diffusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image tokens by 64\u00d7, leading to410\u00d7 fastergeneration process. Our best-performing variant can significantly surpasses DiT-XL/2 (gFID2.13vs. 3.04) while still generating high-quality samples74\u00d7 faster. Codes and models are available at https://github.com/bytedance/1d-tokenizer", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/bytedance/1d-tokenizer"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 NVIDIA GPU (minimum)", "memory": "16GB RAM", "API_calls": "None specified"}, "time_requirements": "2 hours", "code_quality": "well-documented code", "difficulty": 2, "stars": 698}}
{"id": "94539", "url": "https://nips.cc/virtual/2024/poster/94539", "title": "SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection", "authors": [], "abstract": "Abstract:Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR object detection. The dataset and code is available at \\url{https://github.com/zcablii/SARDet_100K}.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/zcablii/SARDet_100K"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_calls": "None"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 469}}
{"id": "95036", "url": "https://nips.cc/virtual/2024/poster/95036", "title": "One-Step Effective Diffusion Network for Real-World  Image Super-Resolution", "authors": [], "abstract": "Abstract:The pre-trained text-to-image diffusion models have been increasingly employed to tackle the real-world image super-resolution (Real-ISR) problem due to their powerful generative image priors. Most of the existing methods start from random noise to reconstruct the high-quality (HQ) image under the guidance of the given low-quality (LQ) image. While promising results have been achieved, such Real-ISR methods require multiple diffusion steps to reproduce the HQ image, increasing the computational cost. Meanwhile, the random noise introduces uncertainty in the output, which is unfriendly to image restoration tasks. To address these issues, we propose a one-step effective diffusion network, namely OSEDiff, for the Real-ISR problem. We argue that the LQ image contains rich information to restore its HQ counterpart, and hence the given LQ image can be directly taken as the starting point for diffusion, eliminating the uncertainty introduced by random noise sampling. We finetune the pre-trained diffusion network with trainable layers to adapt it to complex image degradations. To ensure that the one-step diffusion model could yield HQ Real-ISR output, we apply variational score distillation in the latent space to conduct KL-divergence regularization. As a result, our OSEDiff model can efficiently and effectively generate HQ images in just one diffusion step. Our experiments demonstrate that OSEDiff achieves comparable or even better Real-ISR results, in terms of both objective metrics and subjective evaluations, than previous diffusion model-based Real-ISR methods that require dozens or hundreds of steps. The source codes are released at https://github.com/cswry/OSEDiff.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/cswry/OSEDiff"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA A100", "amount": 4}, "memory": "16GB", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 333}}
{"id": "95524", "url": "https://nips.cc/virtual/2024/poster/95524", "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering", "authors": [], "abstract": "Abstract:Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our \\textit{G-Retriever} method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, \\textit{G-Retriever} performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/G-Retriever}}", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/XiaoxinHe/G-Retriever"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": "1 x A GPU with CUDA support (e.g., NVIDIA)", "memory": "At least 16 GB RAM", "additional_requirements": "Hugging Face account for model access"}, "time_requirements": 3, "code_quality": "Well-documented", "difficulty": 3, "stars": 409}}
{"id": "95851", "url": "https://nips.cc/virtual/2024/poster/95851", "title": "DOGS: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction  Via Gaussian Consensus", "authors": [], "abstract": "Abstract:The recent advances in 3D Gaussian Splatting (3DGS) show promising results on the novel view synthesis (NVS) task. With its superior rendering performance and high-fidelity rendering quality, 3DGS is excelling at its previous NeRF counterparts. The most recent 3DGS method focuses either on improving the instability of rendering efficiency or reducing the model size. On the other hand, the training efficiency of 3DGS on large-scale scenes has not gained much attention. In this work, we propose DoGaussian, a method that trains 3DGS distributedly. Our method first decomposes a scene into $K$ blocks and then introduces the Alternating Direction Method of Multipliers (ADMM) into the training procedure of 3DGS. During training, our DoGaussian maintains one global 3DGS model on the master node and $K$ local 3DGS models on the slave nodes. The $K$ local 3DGS models are dropped after training and we only query the global 3DGS model during inference. The training time is reduced by scene decomposition, and the training convergence and stability are guaranteed through the consensus on the shared 3D Gaussians. Our method accelerates the training of 3DGS by $6+$ times when evaluated on large-scale scenes while concurrently achieving state-of-the-art rendering quality. Our code is publicly available at [https://github.com/AIBluefisher/DOGS](https://github.com/AIBluefisher/DOGS).", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/AIBluefisher/DOGS"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "storage": "Minimum 10GB free space required for datasets"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 195}}
{"id": "95975", "url": "https://nips.cc/virtual/2024/poster/95975", "title": "AutoTimes: Autoregressive Time Series Forecasters via Large Language Models", "authors": [], "abstract": "Abstract:Foundation models of time series have not been fully developed due to the limited availability of time series corpora and the underexploration of scalable pre-training. Based on the similar sequential formulation of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, the inherent autoregressive property and decoder-only architecture of LLMs have not been fully considered, resulting in insufficient utilization of LLM abilities. To fully revitalize the general-purpose token transition and multi-step generation capability of large language models, we propose AutoTimes to repurpose LLMs as autoregressive time series forecasters, which projects time series into the embedding space of language tokens and autoregressively generates future predictions with arbitrary lengths. Compatible with any decoder-only LLMs, the consequent forecaster exhibits the flexibility of the lookback length and scalability with larger LLMs. Further, we formulate time series as prompts, extending the context for prediction beyond the lookback window, termed in-context forecasting. By introducing LLM-embedded textual timestamps, AutoTimes can utilize chronological information to align multivariate time series. Empirically, AutoTimes achieves state-of-the-art with 0.1% trainable parameters and over $5\\times$ training/inference speedup compared to advanced LLM-based forecasters. Code is available at this repository: https://github.com/thuml/AutoTimes.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/thuml/AutoTimes"}, "reproduce_difficulty": {"environment_setup": "Moderate difficulty; requires installing dependencies via pip and downloading models from Hugging Face.", "resource_requirements": "1 GPU (recommended RTX 3090 with 24GB), memory sufficient for model and dataset, and access to large language models (LLAMA-7B preferred).", "time_requirements": "Approximately 15 minutes to run the main commands after setup.", "code_quality": "Well-documented code with clear instructions in the README.", "difficulty": 3, "stars": 156}}
{"id": "96101", "url": "https://nips.cc/virtual/2024/poster/96101", "title": "Segment Any Change", "authors": [], "abstract": "Abstract:Visual foundation models have achieved remarkable results in zero-shot image classification and segmentation, but zero-shot change detection remains an open problem. In this paper, we propose the segment any change models (AnyChange), a new type of change detection model that supports zero-shot prediction and generalization on unseen change types and data distributions.AnyChange is built on the segment anything model (SAM) via our training-free adaptation method, bitemporal latent matching.By revealing and exploiting intra-image and inter-image semantic similarities in SAM's latent space, bitemporal latent matching endows SAM with zero-shot change detection capabilities in a training-free way. We also propose a point query mechanism to enable AnyChange's zero-shot object-centric change detection capability.We perform extensive experiments to confirm the effectiveness of AnyChange for zero-shot change detection.AnyChange sets a new record on the SECOND benchmark for unsupervised change detection, exceeding the previous SOTA by up to 4.4\\% F$_1$ score, and achieving comparable accuracy with negligible manual annotations (1 pixel per image) for supervised change detection. Code is available at https://github.com/Z-Zheng/pytorch-change-models.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Z-Zheng/pytorch-change-models"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires Python and dependencies installation via pip)", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "8GB or more"}, "time_requirements": "2", "code_quality": "Well-documented code", "difficulty": 3, "stars": 106}}
{"id": "96571", "url": "https://nips.cc/virtual/2024/poster/96571", "title": "Text2CAD: Generating Sequential CAD Designs from Beginner-to-Expert Level Text Prompts", "authors": [], "abstract": "Abstract:Prototyping complex computer-aided design (CAD) models in modern softwares can be very time-consuming. This is due to the lack of intelligent systems that can quickly generate simpler intermediate parts. We propose Text2CAD, the first AI framework for generating text-to-parametric CAD models using designer-friendly instructions for all skill levels. Furthermore, we introduce a data annotation pipeline for generating text prompts based on natural language instructions for the DeepCAD dataset using Mistral and LLaVA-NeXT. The dataset contains $\\sim170$K models and $\\sim660$K text annotations, from abstract CAD descriptions (e.g., _generate two concentric cylinders_) to detailed specifications (e.g., _draw two circles with center_ $(x,y)$ and _radius_ $r_{1}$, $r_{2}$, \\textit{and extrude along the normal by} $d$...). Within the Text2CAD framework, we propose an end-to-end transformer-based auto-regressive network to generate parametric CAD models from input texts. We evaluate the performance of our model through a mixture of metrics, including visual quality, parametric precision, and geometrical accuracy. Our proposed framework shows great potential in AI-aided design applications. Project page is available at https://sadilkhan.github.io/text2cad-project/.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/sadilkhan/text2cad"}, "reproduce_difficulty": {"environment_setup": "Easy (using conda with an environment.yml file)", "resource_requirements": {"GPUs": "1 NVIDIA GPU (CUDA compatible)", "memory": "At least 8 GB RAM recommended", "API_calls": "Depends on the model used for inference"}, "time_requirements": "Approximately 2-3 hours for training depending on the dataset size", "code_quality": "Well-documented code", "difficulty": 2, "stars": 141}}
{"id": "96788", "url": "https://nips.cc/virtual/2024/poster/96788", "title": "SMART: Scalable Multi-agent Real-time Motion Generation via Next-token Prediction", "authors": [], "abstract": "Abstract:Data-driven autonomous driving motion generation tasks are frequently impacted by the limitations of dataset size and the domain gap between datasets, which precludes their extensive application in real-world scenarios. To address this issue, we introduce SMART, a novel autonomous driving motion generation paradigm that models vectorized map and agent trajectory data into discrete sequence tokens. These tokens are then processed through a decoder-only transformer architecture to train for the next token prediction task across spatial-temporal series. This GPT-style method allows the model to learn the motion distribution in real driving scenarios. SMART achieves state-of-the-art performance across most of the metrics on the generative Sim Agents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset (WOMD), demonstrating remarkable inference speed. Moreover, SMART represents the generative model in the autonomous driving motion domain, exhibiting zero-shot generalization capabilities: Using only the NuPlan dataset for training and WOMD for validation, SMART achieved a competitive score of 0.72 on the Sim Agents challenge. Lastly, we have collected over 1 billion motion tokens from multiple datasets, validating the model's scalability. These results suggest that SMART has initially emulated two important properties: scalability and zero-shot generalization, and preliminarily meets the needs of large-scale real-time simulation applications. We have released all the code to promote the exploration of models for motion generation in the autonomous driving field. The source code is available at https://github.com/rainmaker22/SMART.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/rainmaker22/SMART"}, "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_calls": "No external API calls required"}, "time_requirements": 5, "code_quality": "Well-documented code", "difficulty": 3, "stars": 109}}
{"id": "96021", "url": "https://nips.cc/virtual/2024/poster/96021", "title": "SHMT: Self-supervised Hierarchical Makeup Transfer via Latent Diffusion Models", "authors": [], "abstract": "Abstract:This paper studies the challenging task of makeup transfer, which aims to apply diverse makeup styles precisely and naturally to a given facial image.  Due to the absence of paired data, current methods typically synthesize sub-optimal pseudo ground truths to guide the model training, resulting in low makeup fidelity. Additionally, different makeup styles generally have varying effects on the person face, but existing methods struggle to deal with this diversity. To address these issues, we propose a novel Self-supervised Hierarchical Makeup Transfer (SHMT) method via latent diffusion models. Following a \"decoupling-and-reconstruction\" paradigm, SHMT works in a self-supervised manner, freeing itself from the misguidance of imprecise pseudo-paired data. Furthermore, to accommodate a variety of makeup styles, hierarchical texture details are decomposed via a Laplacian pyramid and selectively introduced to the content representation. Finally, we design a novel Iterative Dual Alignment (IDA) module that dynamically adjusts the injection condition of the diffusion model, allowing the alignment errors caused by the domain gap between content and makeup representations to be corrected. Extensive quantitative and qualitative analyses demonstrate the effectiveness of our method. Our code is available at https://github.com/Snowfallingplum/SHMT.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Snowfallingplum/SHMT"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires conda environment setup)", "resource_requirements": {"GPUs": {"type": "NVIDIA GPU", "amount": 1}, "memory": "4GB or more recommended", "api_calls": "N/A"}, "time_requirements": "2 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 163}}
{"id": "94183", "url": "https://nips.cc/virtual/2024/poster/94183", "title": "Harmonizing Visual Text Comprehension and Generation", "authors": [], "abstract": "Abstract:In this work, we present TextHarmony, a unified and versatile multimodal generative model proficient in comprehending and generating visual text. Simultaneously generating images and texts typically results in performance degradation due to the inherent inconsistency between vision and language modalities. To overcome this challenge, existing approaches resort to modality-specific data for supervised fine-tuning, necessitating distinct model instances. We propose Slide-LoRA, which dynamically aggregates modality-specific and modality-agnostic LoRA experts, partially decoupling the multimodal generation space. Slide-LoRA harmonizes the generation of vision and language within a singular model instance, thereby facilitating a more unified generative process. Additionally, we develop a high-quality image caption dataset, DetailedTextCaps-100K, synthesized with a sophisticated closed-source MLLM to enhance visual text generation capabilities further. Comprehensive experiments across various benchmarks demonstrate the effectiveness of the proposed approach. Empowered by Slide-LoRA, TextHarmony achieves comparable performance to modality-specific fine-tuning results with only a 2% increase in parameters and shows an average improvement of 2.5% in visual text comprehension tasks and 4.0% in visual text generation tasks. Our work delineates the viability of an integrated approach to multimodal generation within the visual text domain, setting a foundation for subsequent inquiries. Code is available at https://github.com/bytedance/TextHarmony.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/bytedance/TextHarmony"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": "1 or more (specific type not mentioned)", "memory": "Not specified", "API_calls": "Not specified"}, "time_requirements": "2-3 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 112}}
{"id": "94617", "url": "https://nips.cc/virtual/2024/poster/94617", "title": "VMamba: Visual State Space Model", "authors": [], "abstract": "Abstract:Designing computationally efficient network architectures remains an ongoing necessity in computer vision. In this paper, we adapt Mamba, a state-space language model, into VMamba, a vision backbone with linear time complexity. At the core of VMamba is a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D bridges the gap between the ordered nature of 1D selective scan and the non-sequential structure of 2D vision data, which facilitates the collection of contextual information from various sources and perspectives. Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements. Extensive experiments demonstrate VMamba\u2019s promising performance across diverse visual perception tasks, highlighting its superior input scaling efficiency compared to existing benchmark models. Source code is available at https://github.com/MzeroMiko/VMamba", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/MzeroMiko/VMamba"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": {"type": "NVIDIA A100", "amount": 1}, "memory": "16GB", "API_calls": "not specified"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 2419}}
{"id": "94779", "url": "https://nips.cc/virtual/2024/poster/94779", "title": "MVInpainter: Learning Multi-View Consistent Inpainting to Bridge 2D and 3D Editing", "authors": [], "abstract": "Abstract:Novel View Synthesis (NVS) and 3D generation have recently achieved prominent improvements. However, these works mainly focus on confined categories or synthetic 3D assets, which are discouraged from generalizing to challenging in-the-wild scenes and fail to be employed with 2D synthesis directly. Moreover, these methods heavily depended on camera poses, limiting their real-world applications. To overcome these issues, we propose MVInpainter, re-formulating the 3D editing as a multi-view 2D inpainting task. Specifically, MVInpainter partially inpaints multi-view images with the reference guidance rather than intractably generating an entirely novel view from scratch, which largely simplifies the difficulty of in-the-wild NVS and leverages unmasked clues instead of explicit pose conditions. To ensure cross-view consistency, MVInpainter is enhanced by video priors from motion components and appearance guidance from concatenated reference key\\&value attention. Furthermore, MVInpainter incorporates slot attention to aggregate high-level optical flow features from unmasked regions to control the camera movement with pose-free training and inference. Sufficient scene-level experiments on both object-centric and forward-facing datasets verify the effectiveness of MVInpainter, including diverse tasks, such as multi-view object removal, synthesis, insertion, and replacement. The project page is https://ewrfcas.github.io/MVInpainter/.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/ewrfcas/MVInpainter"}, "reproduce_difficulty": {"environment_setup": "Moderate - Requires conda and pip for installation, along with specific commands to set up the environment.", "resource_requirements": {"GPUs": "8 GPUs required (CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7)", "memory": "Not specified but requires substantial memory for processing.", "API_calls": "None specified."}, "time_requirements": "Approximately 10 hours for training and setup.", "code_quality": "Well-documented code with clear instructions.", "difficulty": 3, "stars": 105}}
{"id": "96474", "url": "https://nips.cc/virtual/2024/poster/96474", "title": "ContextCite: Attributing Model Generation to Context", "authors": [], "abstract": "Abstract:How do language models use information provided as context when generating a response?Can we infer whether a particular generated statement is actually grounded in the context, a misinterpretation, or fabricated?To help answer these questions, we introduce the problem ofcontext attribution: pinpointing the parts of the context (if any) thatleda model to generate a particular statement.We then present ContextCite, a simple and scalable method for context attribution that can be applied on top of any existing language model.Finally, we showcase the utility of ContextCite through three applications:(1) helping verify generated statements(2) improving response quality by pruning the context and(3) detecting poisoning attacks.We provide code for ContextCite at https://github.com/MadryLab/context-cite.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/MadryLab/context-cite"}, "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"gpus": {"type": "NVIDIA P100", "amount": 8}, "memory": "not specified", "api_calls": "not specified"}, "time_requirements": 3.5, "code_quality": "well-documented code", "difficulty": 2, "stars": 209}}
{"id": "93725", "url": "https://nips.cc/virtual/2024/poster/93725", "title": "Phased Consistency Models", "authors": [], "abstract": "Abstract:Consistency Models (CMs) have made significant progress in accelerating the generation of diffusion models. However, their application to high-resolution, text-conditioned image generation in the latent space remains unsatisfactory. In this paper, we identify three key flaws in the current design of Latent Consistency Models~(LCMs). We investigate the reasons behind these limitations and propose Phased Consistency Models (PCMs), which generalize the design space and address the identified limitations. Our evaluations demonstrate that PCMs outperform LCMs across 1--16 step generation settings. While PCMs are specifically designed for multi-step refinement, they achieve comparable 1-step generation results to previously state-of-the-art specifically designed 1-step methods. Furthermore, we show the methodology of PCMs is versatile and applicable to video generation, enabling us to train the state-of-the-art few-step text-to-video generator. Our code is available at https://github.com/G-U-N/Phased-Consistency-Model.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/G-U-N/Phased-Consistency-Model"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "1-8 NVIDIA GPUs (recommended, can achieve good results with 1)", "memory": "at least 16GB RAM recommended", "API_call": "Hugging Face API for pre-trained models"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 440}}
{"id": "93882", "url": "https://nips.cc/virtual/2024/poster/93882", "title": "General Detection-based Text Line Recognition", "authors": [], "abstract": "Abstract:We introduce a general detection-based approach to text line recognition, be it printed (OCR) or handwritten text (HTR), with latin, chinese or ciphered characters. Detection-based approaches have until now largely been discarded for HTR because reading characters separately is often challenging, and character-level annotation is difficult and expensive. We overcome these challenges thanks to three main insights: (i) synthetic pre-training with diverse enough data to learn reasonable character localization in any script; (ii) modern transformer-based detectors can jointly detect a large number of instances and, if trained with an adequate masking strategy, leverage consistency between the different detections; (iii) once a pre-trained detection model with approximate character localization is available, it is possible to fine-tune it with line-level annotation on real data, even with a different alphabet.  Our approach thus builds on a completely different paradigm than most state-of-the-art methods, which rely on autoregressive decoding, predicting character values one by one, while we treat a complete line in parallel. Remarkably, our method demonstrates good performance on range of scripts, usually tackled with specialized approaches: latin script, chinese script, and ciphers, for which we significantly improve state-of-the-art performances. Our code and models are available athttps://github.com/raphael-baena/DTLR.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/raphael-baena/DTLR", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/raphael-baena/DTLR"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires Python, PyTorch, and CUDA setup)", "resource_requirements": {"GPUs": "1 GPU (NVIDIA recommended, CUDA compatible)", "memory": "At least 8GB of RAM recommended", "API_calls": "None specified"}, "time_requirements": "Approximately 4-6 hours for pretraining and fine-tuning", "code_quality": "Well-documented code with clear installation and setup instructions", "difficulty": 3, "stars": 130}}
{"id": "93994", "url": "https://nips.cc/virtual/2024/poster/93994", "title": "Lumina-Next : Making Lumina-T2X Stronger and Faster with Next-DiT", "authors": [], "abstract": "Abstract:Lumina-T2X is a nascent family of Flow-based Large Diffusion Transformers (Flag-DiT) that establishes a unified framework for transforming noise into various modalities, such as images and videos, conditioned on text instructions. Despite its promising capabilities, Lumina-T2X still encounters challenges including training instability, slow inference, and extrapolation artifacts. In this paper, we present Lumina-Next, an improved version of Lumina-T2X, showcasing stronger generation performance with increased training and inference efficiency. We begin with a comprehensive analysis of the Flag-DiT architecture and identify several suboptimal components, which we address by introducing the Next-DiT architecture with 3D RoPE and sandwich normalizations. To enable better resolution extrapolation, we thoroughly compare different context extrapolation methods applied to text-to-image generation with 3D RoPE, and propose Frequency- and Time-Aware Scaled RoPE tailored for diffusion transformers. Additionally, we introduce a sigmoid time discretization schedule for diffusion sampling, which achieves high-quality generation in 5-10 steps combined with higher-order ODE solvers. Thanks to these improvements, Lumina-Next not only improves the basic text-to-image generation but also demonstrates superior resolution extrapolation capabilities as well as multilingual generation using decoder-based LLMs as the text encoder, all in a zero-shot manner. To further validate Lumina-Next as a versatile generative framework, we instantiate it on diverse tasks including visual recognition, multi-views, audio, music, and point cloud generation, showcasing strong performance across these domains. By releasing all codes and model weights at https://github.com/Alpha-VLLM/Lumina-T2X, we aim to advance the development of next-generation generative AI capable of universal modeling.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/Alpha-VLLM/Lumina-T2X"}, "reproduce_difficulty": {"environment_setup": "Easy (pip install available)", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB+ recommended", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 2160}}
{"id": "94024", "url": "https://nips.cc/virtual/2024/poster/94024", "title": "Scaling Retrieval-Based Language Models with a Trillion-Token Datastore", "authors": [], "abstract": "Abstract:Scaling laws with respect to the amount of training data and the number of parameters allow us to predict the cost-benefit trade-offs of pretraining language models (LMs) in different configurations. In this paper, we consider another dimension of scaling: the amount of data available at inference time. Specifically, we find that increasing the size of the datastore used by a retrieval-based LM monotonically improves language modeling and several downstream tasks without obvious saturation, such that a smaller model augmented with a large datastore outperforms a larger LM-only model on knowledge-intensive tasks. By plotting compute-optimal scaling curves with varied datastore, model, and pretraining data sizes, we show that using larger datastores can significantly improve model performance for the same training compute budget. We carry out our study by constructing a 1.4 trillion-token datastore named MassiveDS, which is the largest and the most diverse open-sourced datastore for retrieval-based LMs to date, and designing an efficient pipeline for studying datastore scaling in an accessible manner. Finally, we analyze the effect of improving the retriever, datastore quality filtering, and other design choices on our observed scaling trends. Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. To facilitate future research, we open-source our datastore and code at https://github.com/RulinShao/retrieval-scaling.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/RulinShao/retrieval-scaling"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "100G", "API_calls": "Requires access to Hugging Face datasets and models."}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 190}}
{"id": "95173", "url": "https://nips.cc/virtual/2024/poster/95173", "title": "VideoTetris: Towards Compositional Text-to-Video Generation", "authors": [], "abstract": "Abstract:Diffusion models have demonstrated great success in text-to-video (T2V) generation. However, existing methods may face challenges when handling complex (long) video generation scenarios that involve multiple objects or dynamic changes in object numbers. To address these limitations, we propose VideoTetris, a novel framework that enables compositional T2V generation. Specifically, we propose spatio-temporal compositional diffusion to precisely follow complex textual semantics by manipulating and composing the attention maps of denoising networks spatially and temporally. Moreover, we propose a new dynamic-aware data processing pipeline and a consistency regularization method to enhance the consistency of auto-regressive video generation. Extensive experiments demonstrate that our VideoTetris achieves impressive qualitative and quantitative results in compositional T2V generation. Code is available at: https://github.com/YangLing0818/VideoTetris", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/YangLing0818/VideoTetris"}, "reproduce_difficulty": {"environment_setup": "Moderate (Requires Anaconda)", "resource_requirements": "8 GB RAM, 1 GPU (NVIDIA recommended)", "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 214}}
{"id": "95398", "url": "https://nips.cc/virtual/2024/poster/95398", "title": "Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration", "authors": [], "abstract": "Abstract:Mobile device operation tasks are increasingly becoming a popular multi-modal AI application scenario. Current Multi-modal Large Language Models (MLLMs), constrained by their training data, lack the capability to function effectively as operation assistants. Instead, MLLM-based agents, which enhance capabilities through tool invocation, are gradually being applied to this scenario. However, the two major navigation challenges in mobile device operation tasks \u2014 task progress navigation and focus content navigation \u2014 are difficult to effectively solve under the single-agent architecture of existing work. This is due to the overly long token sequences and the interleaved text-image data format, which limit performance. To address these navigation challenges effectively, we propose Mobile-Agent-v2, a multi-agent architecture for mobile device operation assistance. The architecture comprises three agents: planning agent, decision agent, and reflection agent. The planning agent condenses lengthy, interleaved image-text history operations and screens summaries into a pure-text task progress, which is then passed on to the decision agent. This reduction in context length makes it easier for decision agent to navigate the task progress. To retain focus content, we design a memory unit that updates with task progress by decision agent. Additionally, to correct erroneous operations, the reflection agent observes the outcomes of each operation and handles any mistake accordingly. Experimental results indicate that Mobile-Agent-v2 achieves over a 30% improvement in task completion compared to the single-agent architecture of Mobile-Agent. The code is open-sourced at https://github.com/X-PLUG/MobileAgent.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/X-PLUG/MobileAgent"}, "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA RTX 3080", "amount": 1}, "memory": "16 GB RAM", "API_calls": "None specified"}, "time_requirements": "1-2 hours", "code_quality": "Well-documented", "difficulty": 3, "stars": 3542}}
{"id": "95732", "url": "https://nips.cc/virtual/2024/poster/95732", "title": "HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors", "authors": [], "abstract": "Abstract:Despite recent advancements in high-fidelity human reconstruction techniques, the requirements for densely captured images or time-consuming per-instance optimization significantly hinder their applications in broader scenarios. To tackle these issues, we presentHumanSplat, which predicts the 3D Gaussian Splatting properties of any human from a single input image in a generalizable manner.Specifically, HumanSplat comprises a 2D multi-view diffusion model and a latent reconstruction Transformer with human structure priors that adeptly integrate geometric priors and semantic features within a unified framework. A hierarchical loss that incorporates human semantic information is devised to achieve high-fidelity texture modeling and impose stronger constraints on the estimated multiple views. Comprehensive experiments on standard benchmarks and in-the-wild images demonstrate that HumanSplat surpasses existing state-of-the-art methods in achieving photorealistic novel-view synthesis. Project page: https://humansplat.github.io.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/HumanSplat/HumanSplat"}, "reproduce_difficulty": {"environment_setup": "Moderate (requires bash scripts for setup)", "resource_requirements": "GPUs recommended but not specified; 8 GB RAM minimum; requires access to specific datasets", "time_requirements": "Approximately 2 hours to set up and run the main command", "code_quality": "Well-documented code", "difficulty": 3, "stars": 120}}
{"id": "95751", "url": "https://nips.cc/virtual/2024/poster/95751", "title": "Dense Connector for MLLMs", "authors": [], "abstract": "Abstract:Do we fully leverage the potential of visual encoder in Multimodal Large Language Models (MLLMs)?The recent outstanding performance of MLLMs in multimodal understanding has garnered broad attention from both academia and industry. In the current MLLM rat race, the focus seems to be predominantly on the linguistic side. We witness the rise of larger and higher-quality instruction datasets, as well as the involvement of larger-sized LLMs. Yet, scant attention has been directed towards the visual signals utilized by MLLMs, often assumed to be the final high-level features extracted by a frozen visual encoder. In this paper, we introduce theDense Connector- a simple, effective, and plug-and-play vision-language connector that significantly enhances existing MLLMs by leveraging multi-layer visual features, with minimal additional computational overhead. Building on this, we also propose the Efficient Dense Connector, which achieves performance comparable to LLaVA-v1.5 with only 25% of the visual tokens. Furthermore, our model, trained solely on images, showcases remarkable zero-shot capabilities in video understanding as well. Experimental results across various vision encoders, image resolutions, training dataset scales, varying sizes of LLMs (2.7B\u219270B), and diverse architectures of MLLMs (e.g., LLaVA-v1.5, LLaVA-NeXT and Mini-Gemini) validate the versatility and scalability of our approach, achieving state-of-the-art performance across 19 image and video benchmarks. We hope that this work will provide valuable experience and serve as a basic module for future MLLM development. Code is available at https://github.com/HJYao00/DenseConnector.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/HJYao00/DenseConnector"}, "reproduce_difficulty": {"environment_setup": "Moderate (Requires conda and pip for installation)", "resource_requirements": {"GPUs": "1-2 GPUs recommended, type not specified", "memory": "Not specified, but high-performance GPUs recommended", "API_calls": "Not specified"}, "time_requirements": "Estimated 2-3 hours for setup and initial training", "code_quality": "Well-documented code with installation and usage instructions", "difficulty": 3, "stars": 156}}
{"id": "95770", "url": "https://nips.cc/virtual/2024/poster/95770", "title": "TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables", "authors": [], "abstract": "Abstract:Deep models have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous variables can provide valuable external information for endogenous variables. Thus, unlike well-established multivariate or univariate forecasting paradigms that either treat all the variables equally or ignore exogenous information, this paper focuses on a more practical setting: time series forecasting with exogenous variables. We propose a novel approach, TimeXer, to ingest external information to enhance the forecasting of endogenous variables. With deftly designed embedding layers, TimeXer empowers the canonical Transformer with the ability to reconcile endogenous and exogenous information, where patch-wise self-attention and variate-wise cross-attention are used simultaneously. Moreover, global endogenous tokens are learned to effectively bridge the causal information underlying exogenous series into endogenous temporal patches. Experimentally, TimeXer achieves consistent state-of-the-art performance on twelve real-world forecasting benchmarks and exhibits notable generality and scalability. Code is available at this repository: https://github.com/thuml/TimeXer.", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/thuml/TimeXer"}, "reproduce_difficulty": {"environment_setup": "easy (pip install -r requirements.txt)", "resource_requirements": {"GPUs": "1 or more NVIDIA GPUs", "memory": "at least 8 GB RAM", "additional": "Python with PyTorch"}, "time_requirements": "2 hours", "code_quality": "well-documented code", "difficulty": 3, "stars": 182}}
{"id": "95805", "url": "https://nips.cc/virtual/2024/poster/95805", "title": "HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting", "authors": [], "abstract": "Abstract:High dynamic range (HDR) novel view synthesis (NVS) aims to create photorealistic images from novel viewpoints using HDR imaging techniques. The rendered HDR images capture a wider range of brightness levels containing more details of the scene than normal low dynamic range (LDR) images. Existing HDR NVS methods are mainly based on NeRF. They suffer from long training time and slow inference speed. In this paper, we propose a new framework, High Dynamic Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views and reconstruct LDR images with a user input exposure time. Specifically, we design a Dual Dynamic Range (DDR) Gaussian point cloud model that uses spherical harmonics to fit HDR color and employs an MLP-based tone-mapper to render LDR color. The HDR and LDR colors are then fed into two Parallel Differentiable Rasterization (PDR) processes to reconstruct HDR and LDR views. To establish the data foundation for the research of 3D Gaussian splatting-based methods in HDR NVS, we recalibrate the camera parameters and compute the initial positions for Gaussian point clouds. Comprehensive experiments show that HDR-GS surpasses the state-of-the-art NeRF-based method by 3.84 and 1.91 dB on LDR and HDR NVS while enjoying 1000$\\times$ inference speed and only costing 6.3\\% training time. Code and data are released at https://github.com/caiyuanhao1998/HDR-GS", "pdf_url": "", "supplementary_url": "", "code_url": "", "bibtex": "", "keywords": [], "reproduce_eval": {"code": "https://github.com/caiyuanhao1998/HDR-GS"}, "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "8GB+"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 335}}
