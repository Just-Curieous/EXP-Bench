{"id": "19292", "url": "https://iclr.cc/virtual/2024/poster/19292", "title": "Zipformer: A faster and better encoder for automatic speech recognition", "authors": [], "abstract": "Abstract:The Conformer has become the most popular encoder model for automatic speech recognition (ASR).  It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer.  Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4)  new activation functions SwooshR and SwooshL work better than Swish.  We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster converge and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate the effectiveness of our proposed Zipformer over other state-of-the-art ASR models. Our code is publicly available at https://github.com/k2-fsa/icefall.", "pdf_url": "https://openreview.net/pdf?id=9WD9KwssyT", "supplementary_url": "", "code_url": "https://github.com/k2-fsa/icefall", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA V100 or A100", "amount": "2-8"}, "memory": "32GB or more recommended"}, "time_requirements": "1-2", "code_quality": "well-documented code", "difficulty": 3, "stars": 1023}}
{"id": "19033", "url": "https://iclr.cc/virtual/2024/poster/19033", "title": "The Reversal Curse: LLMs trained on \u201cA is B\u201d fail to learn \u201cB is A\u201d", "authors": [], "abstract": "Abstract:We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form ''AisB'', it will not automatically generalize to the reverse direction ''BisA''. This is theReversal Curse. For instance, if a model is trained on ''Valentina Tereshkova was the first woman to travel to space'', it will not automatically be able to answer the question, ''Who was the first woman to travel to space?''. Moreover, the likelihood of the correct answer (''Valentina Tershkova'') will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if ''AisB'' occurs, ''BisA'' is more likely to occur. It is worth noting, however, that if ''AisB'' appearsin-context, models can deduce the reverse relationship. We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as ''Uriah Hawthorne is the composer ofAbyssal Melodies'' and showing that they fail to correctly answer ''Who composedAbyssal Melodies?''. The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation.We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as ''Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]'' and the reverse ''Who is Mary Lee Pfeiffer's son?''. GPT-4 correctly answers questions like the former 79\\% of the time, compared to 33\\% for the latter. Code available at: https://github.com/lukasberglund/reversal_curse.", "pdf_url": "https://openreview.net/pdf?id=GPKTIktA0k", "supplementary_url": "", "code_url": "https://github.com/lukasberglund/reversal_curse", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1", "type": "NVIDIA CUDA compatible", "memory": "At least 16GB RAM", "API_call": "OpenAI API key required"}, "time_requirements": "5", "code_quality": "well-documented code", "difficulty": 3, "stars": 284}}
{"id": "19044", "url": "https://iclr.cc/virtual/2024/poster/19044", "title": "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning", "authors": [], "abstract": "Abstract:With the advance of text-to-image (T2I) diffusion models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. However, adding motion dynamics to existing high-quality personalized T2Is and enabling them to generate animations remains an open challenge. In this paper, we present AnimateDiff, a practical framework for animating personalized T2I models without requiring model-specific tuning. At the core of our framework is a plug-and-play motion module that can be trained once and seamlessly integrated into any personalized T2Is originating from the same base T2I. Through our proposed training strategy, the motion module effectively learns transferable motion priors from real-world videos. Once trained, the motion module can be inserted into a personalized T2I model to form a personalized animation generator. We further propose MotionLoRA, a lightweight fine-tuning technique for AnimateDiff that enables a pre-trained motion module to adapt to new motion patterns, such as different shot types, at a low training and data collection cost. We evaluate AnimateDiff and MotionLoRA on several public representative personalized T2I models collected from the community. The results demonstrate that our approaches help these models generate temporally smooth animation clips while preserving the visual quality and motion diversity. Codes and pre-trained weights are available at https://github.com/guoyww/AnimateDiff.", "pdf_url": "https://openreview.net/pdf?id=Fx2SbBgcte", "supplementary_url": "", "code_url": "https://github.com/guoyww/AnimateDiff", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1, "memory": "13GB"}, "RAM": "16GB"}, "time_requirements": 2, "code_quality": "well-documented", "difficulty": 3, "stars": 11093}}
{"id": "17666", "url": "https://iclr.cc/virtual/2024/poster/17666", "title": "BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection", "authors": [], "abstract": "Abstract:We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extractbackdoor functionalityof a given backdoored model to abackdoor expertmodel. The approach is straightforward --- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model~(dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by an ensemble strategy with a finetuned auxiliary model, our defense,BaDExpert(Backdoor InputDetection with BackdoorExpert), effectively mitigates 17 SOTA backdoor attacks while minimally impacting clean utility. The effectiveness of BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet) across various model architectures (ResNet, VGG, MobileNetV2 and Vision Transformer). Our code is integrated into our research toolbox:https://github.com/vtu81/backdoor-toolbox.", "pdf_url": "https://openreview.net/pdf?id=s56xikpD92", "supplementary_url": "", "code_url": "https://github.com/vtu81/backdoor-toolbox", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Easy (pip install -r requirement.txt after manually installing PyTorch with CUDA)", "resource_requirements": "GPUs with CUDA support (exact types not specified), 4GB+ recommended memory", "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 165}}
{"id": "19269", "url": "https://iclr.cc/virtual/2024/poster/19269", "title": "MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images", "authors": [], "abstract": "Abstract:This paper studies zero-shot anomaly classification (AC) and segmentation (AS) in industrial vision.We reveal that the abundant normal and abnormal cues implicit in unlabeled test images can be exploited for anomaly determination, which is ignored by prior methods.Our key observation is that for the industrial product images, the normal image patches could find a relatively large number of similar patches in other unlabeled images,while the abnormal ones only have a few similar patches.We leverage such a discriminative characteristic to design a novel zero-shot AC/AS method by Mutual Scoring (MuSc) of the unlabeled images,  which does not need any training or prompts.Specifically, we perform Local Neighborhood Aggregation with Multiple Degrees (LNAMD) to obtain the patch features that are capable of representing anomalies in varying sizes.Then we propose the Mutual Scoring Mechanism (MSM) to leverage the unlabeled test images to assign the anomaly score to each other. Furthermore, we present an optimization approach named Re-scoring with Constrained Image-level Neighborhood (RsCIN) for image-level anomaly classification to suppress the false positives caused by noises in normal images.The superior performance on the challenging MVTec AD and VisA datasets demonstrates the effectiveness of our approach. Compared with the state-of-the-art zero-shot approaches, MuSc achieves a $\\textbf{21.1}$% PRO absolute gain (from 72.7\\% to 93.8\\%) on MVTec AD, a $\\textbf{19.4}$% pixel-AP gain and a $\\textbf{14.7}$% pixel-AUROC gain on VisA.In addition, our zero-shot approach outperforms most of the few-shot approaches and is comparable to some one-class methods.Code is available at https://github.com/xrli-U/MuSc.", "pdf_url": "https://openreview.net/pdf?id=AHgc5SMdtd", "supplementary_url": "", "code_url": "https://github.com/xrli-U/MuSc", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": "1"}, "memory": "8GB", "API_calls": "None"}, "time_requirements": "1", "code_quality": "well-documented", "difficulty": 3, "stars": 358}}
{"id": "19281", "url": "https://iclr.cc/virtual/2024/poster/19281", "title": "Domain-Agnostic Molecular Generation with Chemical Feedback", "authors": [], "abstract": "Abstract:The generation of molecules with desired properties has become increasingly popular, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face challenges such as generating syntactically or chemically flawed molecules, having narrow domain focus, and struggling to create diverse and feasible molecules due to limited annotated data or external molecular databases.To tackle these challenges, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. Through the reconstruction of over 100 million molecular SELFIES, MolGen internalizes structural and grammatical insights. This is further enhanced by domain-agnostic molecular prefix tuning, fostering robust knowledge transfer across diverse domains. Importantly, our chemical feedback paradigm steers the model away from \"molecular hallucinations\", ensuring alignment between the model's estimated probabilities and real-world chemical preferences. Extensive experiments on well-known benchmarks underscore MolGen's optimization capabilities in properties such as penalized logP, QED, and molecular docking. Additional analyses confirm its proficiency in accurately capturing molecule distributions, discerning intricate structural patterns, and efficiently exploring the chemical space (https://github.com/zjunlp/MolGen).", "pdf_url": "https://openreview.net/pdf?id=9rPyHyjfwP", "supplementary_url": "", "code_url": "https://github.com/zjunlp/MolGen", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA GPU", "amount": 1}, "memory": "16GB RAM", "API_calls": "N/A"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 149}}
{"id": "18244", "url": "https://iclr.cc/virtual/2024/poster/18244", "title": "Periodicity Decoupling Framework for Long-term Series Forecasting", "authors": [], "abstract": "Abstract:Convolutional neural network (CNN)-based and Transformer-based methods have recently made significant strides in time series forecasting, which excel at modeling local temporal variations or capturing long-term dependencies. However, real-world time series usually contain intricate temporal patterns, thus making it challenging for existing methods that mainly focus on temporal variations modeling from the 1D time series directly. Based on the intrinsic periodicity of time series, we propose a novel Periodicity Decoupling Framework (PDF) to capture 2D temporal variations of decoupled series for long-term series forecasting. Our PDF mainly consists of three components: multi-periodic decoupling block (MDB), dual variations modeling block (DVMB), and variations aggregation block (VAB). Unlike the previous methods that model 1D temporal variations, our PDF mainly models 2D temporal variations, decoupled from 1D time series by MDB. After that, DVMB attempts to further capture short-term and long-term variations, followed by VAB to make final predictions. Extensive experimental results across seven real-world long-term time series datasets demonstrate the superiority of our method over other state-of-the-art methods, in terms of both forecasting performance and computational efficiency. Code is available at https://github.com/Hank0626/PDF.", "pdf_url": "https://openreview.net/pdf?id=dp27P5HBBt", "supplementary_url": "", "code_url": "https://github.com/Hank0626/PDF", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate difficulty; requires installation of dependencies via pip with a requirements.txt file.", "resource_requirements": {"GPUs": "At least one GPU (specific type not mentioned)", "memory": "Not specified", "api_calls": "Not specified"}, "time_requirements": "Approximately 1-2 hours to run the main command after setup.", "code_quality": "Well-documented; clear instructions provided for setup and usage.", "difficulty": 3, "stars": 116}}
{"id": "18318", "url": "https://iclr.cc/virtual/2024/poster/18318", "title": "AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection", "authors": [], "abstract": "Abstract:Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliarydata to detect anomalies without any training sample in a target dataset. Itis a crucial task when training data is not accessible due to various concerns, e.g.,data privacy, yet it is challenging since the models need to generalize to anomaliesacross different domains where the appearance of foreground objects, abnormalregions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-languagemodels (VLMs), such as CLIP, have demonstrated strong zero-shot recognitionability in various vision tasks, including anomaly detection. However, their ZSADperformance is weak since the VLMs focus more on modeling the class semanticsof the foreground objects rather than the abnormality/normality in the images. Inthis paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIPfor accurate ZSAD across different domains. The key insight of AnomalyCLIPis to learn object-agnostic text prompts that capture generic normality and abnormalityin an image regardless of its foreground objects. This allows our model tofocus on the abnormal image regions rather than the object semantics, enablinggeneralized normality and abnormality recognition on diverse types of objects.Large-scale experiments on 17 real-world anomaly detection datasets show thatAnomalyCLIP achieves superior zero-shot performance of detecting and segmentinganomalies in datasets of highly diverse class semantics from various defectinspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.", "pdf_url": "https://openreview.net/pdf?id=buC4E91xZE", "supplementary_url": "", "code_url": "https://github.com/zqhang/AnomalyCLIP", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA RTX 3090", "amount": 1}, "memory": "24GB"}, "time_requirements": 2, "code_quality": "well-documented", "difficulty": 3, "stars": 339}}
{"id": "19388", "url": "https://iclr.cc/virtual/2024/poster/19388", "title": "Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models", "authors": [], "abstract": "Abstract:Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we find and fix an average of6.16\\%label errors in11datasets constructed from the above benchmarks. The data credibility and downstream learning performance can be remarkably improved by directly fixing label errors, indicating the significance of cleaning existing real-world datasets. Code is available athttps://github.com/Docta-ai/docta.", "pdf_url": "https://openreview.net/pdf?id=6bcAD6g688", "supplementary_url": "", "code_url": "https://github.com/Docta-ai/docta", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 GPU recommended for encoding features", "memory": "Not specified, but likely requires standard resources for data processing", "API_call": "None"}, "time_requirements": "2", "code_quality": "well-documented code", "difficulty": 2, "stars": 2706}}
{"id": "17776", "url": "https://iclr.cc/virtual/2024/poster/17776", "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems", "authors": [], "abstract": "Abstract:Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is actively maintained with the latest code, serving as a live benchmark publicly available at https://github.com/Leolty/repobench.", "pdf_url": "https://openreview.net/pdf?id=pPjZIOuQuF", "supplementary_url": "", "code_url": "https://github.com/Leolty/repobench", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "easy (just requires running a git clone and installing dependencies via requirements.txt)", "resource_requirements": {"GPUs": "1 GPU (CUDA-enabled)", "memory": "Not specified", "API_call": "Not mentioned"}, "time_requirements": "1-2 hours depending on the dataset size", "code_quality": "well-documented code with clear instructions", "difficulty": 2, "stars": 145}}
{"id": "18013", "url": "https://iclr.cc/virtual/2024/poster/18013", "title": "Knowledge Fusion of Large Language Models", "authors": [], "abstract": "Abstract:While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures\u2014Llama-2, MPT, and OpenLLaMA\u2014across various benchmarks and tasks. Our findings confirm that the fusion of LLMs can improve the performance of the target model across a range of capabilities such as reasoning, commonsense, and code generation. Our code, model weights, and data are public at \\url{https://github.com/fanqiwan/FuseLLM}.", "pdf_url": "https://openreview.net/pdf?id=jiDsk12qcz", "supplementary_url": "", "code_url": "https://github.com/fanqiwan/FuseLLM", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate (requires specific dependencies and possibly Docker for model integration)", "resource_requirements": "GPUs: 1-4 NVIDIA A100 or equivalent, 32GB+ RAM recommended", "time_requirements": "Approximately 2 hours to run main command", "code_quality": "Well-documented code with clear instructions", "difficulty": 3, "stars": 535}}
{"id": "18865", "url": "https://iclr.cc/virtual/2024/poster/18865", "title": "SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations", "authors": [], "abstract": "Abstract:We consider using deep neural networks to solve time-dependent partial differential equations (PDEs), where multi-scale processing is crucial for modeling complex, time-evolving dynamics. While the U-Net architecture with skip connections is commonly used by prior studies to enable multi-scale processing, our analysis shows that the need for features to evolve across layers results in temporally misaligned features in skip connections, which limits the model\u2019s performance. To address this limitation, we propose SineNet, consisting of multiple sequentially connected U-shaped network blocks, referred to as waves. In SineNet, high-resolution features are evolved progressively through multiple stages, thereby reducing the amount of misalignment within each stage. We furthermore analyze the role of skip connections in enabling both parallel and sequential processing of multi-scale information. Our method is rigorously tested on multiple PDE datasets, including the Navier-Stokes equations and shallow water equations, showcasing the advantages of our proposed approach over conventional U-Nets with a comparable parameter budget. We further demonstrate that increasing the number of waves in SineNet while maintaining the same number of parameters leads to a monotonically improved performance. The results highlight the effectiveness of SineNet and the potential of our approach in advancing the state-of-the-art in neural PDE solver design. Our code is available as part of AIRS (https://github.com/divelab/AIRS).", "pdf_url": "https://openreview.net/pdf?id=LSYhE2hLWG", "supplementary_url": "", "code_url": "https://github.com/divelab/AIRS", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate (requires Python packages, possible Docker setup)", "resource_requirements": "1 GPU (NVIDIA recommended), 16 GB RAM", "time_requirements": "4 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 572}}
{"id": "18447", "url": "https://iclr.cc/virtual/2024/poster/18447", "title": "MogaNet: Multi-order Gated Aggregation Network", "authors": [], "abstract": "Abstract:By contextualizing the kernel as global as possible, Modern ConvNets have shown great potential in computer vision tasks. However, recent progress on \\textit{multi-order game-theoretic interaction} within deep neural networks (DNNs) reveals the representation bottleneck of modern ConvNets, where the expressive interactions have not been effectively encoded with the increased kernel size. To tackle this challenge, we propose a new family of modern ConvNets, dubbed MogaNet, for discriminative visual representation learning in pure ConvNet-based models with favorable complexity-performance trade-offs. MogaNet encapsulates conceptually simple yet effective convolutions and gated aggregation into a compact module, where discriminative features are efficiently gathered and contextualized adaptively. MogaNet exhibits great scalability, impressive efficiency of parameters, and competitive performance compared to state-of-the-art ViTs and ConvNets on ImageNet and various downstream vision benchmarks, including COCO object detection, ADE20K semantic segmentation, 2D\\&3D human pose estimation, and video prediction. Notably, MogaNet hits 80.0\\% and 87.8\\% accuracy with 5.2M and 181M parameters on ImageNet-1K, outperforming ParC-Net and ConvNeXt-L, while saving 59\\% FLOPs and 17M parameters, respectively. The source code is available at https://github.com/Westlake-AI/MogaNet.", "pdf_url": "https://openreview.net/pdf?id=XhYWgjqCrV", "supplementary_url": "", "code_url": "https://github.com/Westlake-AI/MogaNet", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": "1-4 NVIDIA GPUs (V100, A100, etc.)", "memory": "At least 16GB RAM recommended", "API_calls": "None specified"}, "time_requirements": "2-4 hours", "code_quality": "Well-documented", "difficulty": 3, "stars": 220}}
{"id": "19610", "url": "https://iclr.cc/virtual/2024/poster/19610", "title": "TopoMLP: A Simple yet Strong Pipeline for Driving Topology Reasoning", "authors": [], "abstract": "Abstract:Topology reasoning aims to comprehensively understand road scenes and present drivable routes in autonomous driving. It requires detecting road centerlines (lane) and traffic elements, further reasoning their topology relationship, \\textit{i.e.}, lane-lane topology, and lane-traffic topology. In this work, we first present that the topology score relies heavily on detection performance on lane and traffic elements. Therefore, we introduce a powerful 3D lane detector and an improved 2D traffic element detector to extend the upper limit of topology performance. Further, we propose TopoMLP, a simple yet high-performance pipeline for driving topology reasoning. Based on the impressive detection performance, we develop two simple MLP-based heads for topology generation. TopoMLP achieves state-of-the-art performance on OpenLane-V2 dataset, \\textit{i.e.}, 41.2\\% OLS with ResNet-50 backbone. It is also the 1st solution for 1st OpenLane Topology in Autonomous Driving Challenge. We hope such simple and strong pipeline can provide some new insights to the community. Code is at https://github.com/wudongming97/TopoMLP.", "pdf_url": "https://openreview.net/pdf?id=0gTW5JUFTW", "supplementary_url": "", "code_url": "https://github.com/wudongming97/TopoMLP", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate (conda/pip dependencies detailed in setup.md)", "resource_requirements": "1-2 GPUs (NVIDIA RTX 2080 or better), 16GB RAM or more, access to OpenLane dataset", "time_requirements": "2-4 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 179}}
{"id": "17388", "url": "https://iclr.cc/virtual/2024/poster/17388", "title": "AgentBench: Evaluating LLMs as Agents", "authors": [], "abstract": "Abstract:The potential of Large Language Model (LLM) as agents has been widely acknowledged recently.Thus, there is an urgent need to quantitatively evaluate LLMs as agents on challenging tasks in interactive environments.We present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities.Our extensive test over 29 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B.We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.Improving instruction following and training on high quality multi-round alignment data could improve agent performance.And different from existing assumptions, training on code present ambivalent impacts on different agent tasks.Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.", "pdf_url": "https://openreview.net/pdf?id=zAdUB0aCTQ", "supplementary_url": "", "code_url": "https://github.com/THUDM/AgentBench", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "15GB", "API_calls": "OpenAI API"}, "time_requirements": 1, "code_quality": "Well-documented code", "difficulty": 3, "stars": 2406}}
{"id": "19366", "url": "https://iclr.cc/virtual/2024/poster/19366", "title": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models", "authors": [], "abstract": "Abstract:The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively. Code is available at https://github.com/SheltonLiu-N/AutoDAN.", "pdf_url": "https://openreview.net/pdf?id=7Jwpw4qKkb", "supplementary_url": "", "code_url": "https://github.com/SheltonLiu-N/AutoDAN", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "conda", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "4GB", "API_call": "OpenAI API key required"}, "time_requirements": 1, "code_quality": "Well-documented code", "difficulty": 2, "stars": 297}}
{"id": "18130", "url": "https://iclr.cc/virtual/2024/poster/18130", "title": "SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation", "authors": [], "abstract": "Abstract:With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often suffer limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' for MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting data points). To the best of our knowledge, SalUn is the first principled MU approach that can effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation tasks. As highlighted below, For example, SalUn yields a stability advantage in high-variance random data forgetting, e.g., with a 0.2% gap compared to exact unlearning on the CIFAR-10 dataset. Moreover, in preventing conditional diffusion models from generating harmful images, SalUn achieves nearly 100% unlearning accuracy, outperforming current state-of-the-art baselines like Erased Stable Diffusion and Forget-Me-Not. Codes are available at https://github.com/OPTML-Group/Unlearn-Saliency.WARNING: This paper contains model outputs that may be offensive in nature.", "pdf_url": "https://openreview.net/pdf?id=gn0mIhQGNM", "supplementary_url": "", "code_url": "https://github.com/OPTML-Group/Unlearn-Saliency", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate (requires installation of dependencies via pip or conda, may have specific version requirements)", "resource_requirements": "1-2 GPUs (NVIDIA RTX 2080 or better recommended), 16GB+ RAM, potential API calls for image generation tasks", "time_requirements": "2-4 hours", "code_quality": "Well-documented code with clear instructions and examples", "difficulty": 3, "stars": 114}}
{"id": "17881", "url": "https://iclr.cc/virtual/2024/poster/17881", "title": "KW-Design: Pushing the Limit of Protein Design via Knowledge Refinement", "authors": [], "abstract": "Abstract:Recent studies have shown competitive performance in protein inverse folding, while most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. Given the great success of pretrained models on diverse protein-related tasks and the fact that recovery is highly correlated with confidence, we wonder whether this knowledge can push the limits of protein design further. As a solution, we propose a knowledge-aware module that refines low-quality residues. We also introduce a memory-retrieval mechanism to save more than 50\\% of the training time. We extensively evaluate our proposed method on the CATH, TS50, TS500, and PDB datasets and our results show that our KW-Design method outperforms the previous PiFold method by approximately 9\\% on the CATH dataset. KW-Design is the first method that achieves 60+\\% recovery on all these benchmarks. We also provide additional analysis to demonstrate the effectiveness of our proposed method. The code is publicly available via \\href{https://github.com/A4Bio/ProteinInvBench}{GitHub}.", "pdf_url": "https://openreview.net/pdf?id=mpqMVWgqjn", "supplementary_url": "", "code_url": "https://github.com/A4Bio/ProteinInvBench", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate (conda and pip installation required)", "resource_requirements": "1 GPU (NVIDIA, preferably with CUDA support), 16GB RAM", "time_requirements": "2 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 174}}
{"id": "18658", "url": "https://iclr.cc/virtual/2024/poster/18658", "title": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI", "authors": [], "abstract": "Abstract:The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches. The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks. Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields. Our code is available at https://github.com/935963004/LaBraM.", "pdf_url": "https://openreview.net/pdf?id=QzTpTRVtrP", "supplementary_url": "", "code_url": "https://github.com/935963004/LaBraM", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate (requires conda for environment setup with specific package versions)", "resource_requirements": {"GPUs": {"type": "NVIDIA GeForce RTX 3090", "amount": 8}, "memory": "At least 16GB recommended", "other": "Raw EEG files need conversion to hdf5 format"}, "time_requirements": 50, "code_quality": "Well-documented code with clear instructions", "difficulty": 3, "stars": 309}}
{"id": "18462", "url": "https://iclr.cc/virtual/2024/poster/18462", "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models", "authors": [], "abstract": "Abstract:Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model families and validate its effectiveness in different fine-tuning datasets and downstream scenarios. The code is made available at https://github.com/yuhuixu1993/qa-lora.", "pdf_url": "https://openreview.net/pdf?id=WvFoJccpo8", "supplementary_url": "", "code_url": "https://github.com/yuhuixu1993/qa-lora", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "conda", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "8GB", "API_call": "N/A"}, "time_requirements": "2", "code_quality": "well-documented code", "difficulty": 2, "stars": 127}}
{"id": "18128", "url": "https://iclr.cc/virtual/2024/poster/18128", "title": "Generative Judge for Evaluating Alignment", "authors": [], "abstract": "Abstract:The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regardinggenerality(i.e., assessing performance across diverse scenarios),flexibility(i.e., examining under different protocols), andinterpretability(i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters,Auto-J, designed to address these challenges. Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. To demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. Experimentally,Auto-Joutperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. We also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://github.com/GAIR-NLP/auto-j.", "pdf_url": "https://openreview.net/pdf?id=gtkFw6sZGS", "supplementary_url": "", "code_url": "https://github.com/GAIR-NLP/auto-j", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1, "memory": "8GB"}, "additional_requirements": "Python 3.10, CUDA"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 229}}
{"id": "18312", "url": "https://iclr.cc/virtual/2024/poster/18312", "title": "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World", "authors": [], "abstract": "Abstract:We present the All-Seeing (AS) project: a large-scale dataset and model for recognizing and understanding everything in the open world.Using a scalable data engine that incorporates human feedback and efficient models in the loop, we create a new dataset (AS-1B) with over 1.2 billion regions annotated with semantic tags, question-answering pairs, and detailed captions. It covers a wide range of 3.5 million common and rare concepts in the real world and has 132.2 billion tokens that describe the concepts and their attributes. Leveraging this new dataset, we develop the All-Seeing model (ASM), a unified framework for panoptic visual recognition and understanding. The model is trained with open-ended language prompts and locations, which allows it to generalize to various vision and language tasks with remarkable zero-shot performance, including both region- and image-level retrieval, region recognition, captioning, and question-answering. We hope that this project can serve as a foundation for vision-language artificial general intelligence research. Code is available at https://github.com/OpenGVLab/all-seeing.", "pdf_url": "https://openreview.net/pdf?id=c2R7ajodcI", "supplementary_url": "", "code_url": "https://github.com/OpenGVLab/all-seeing", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_calls": "Required for model training and evaluation"}, "time_requirements": "2", "code_quality": "Well-documented code", "difficulty": 3, "stars": 475}}
{"id": "18889", "url": "https://iclr.cc/virtual/2024/poster/18889", "title": "An Extensible Framework for Open Heterogeneous Collaborative Perception", "authors": [], "abstract": "Abstract:Collaborative perception aims to mitigate the limitations of single-agent perception, such as occlusions, by facilitating data exchange among multiple agents. However, most current works consider a homogeneous scenario where all agents use identity sensors and perception models. In reality, heterogeneous agent types may continually emerge and inevitably face a domain gap when collaborating with existing agents. In this paper, we introduce a new open heterogeneous problem: how to accommodate continually emerging new heterogeneous agent types into collaborative perception, while ensuring high perception performance and low integration cost? To address this problem, we propose HEterogeneous ALliance (HEAL), a novel extensible collaborative perception framework. HEAL first establishes a unified feature space with initial agents via a novel multi-scale foreground-aware Pyramid Fusion network. When heterogeneous new agents emerge with previously unseen modalities or models, we align them to the established unified space with an innovative backward alignment. This step only involves individual training on the new agent type, thus presenting extremely low training costs and high extensibility. To enrich agents' data heterogeneity, we bring OPV2V-H, a new large-scale dataset with more diverse sensor types. Extensive experiments on OPV2V-H and DAIR-V2X datasets show that HEAL surpasses SOTA methods in performance while reducing the training parameters by 91.5\\% when integrating 3 new agent types. We further implement a comprehensive codebase at: https://github.com/yifanlu0227/HEAL", "pdf_url": "https://openreview.net/pdf?id=KkrDUGIASk", "supplementary_url": "", "code_url": "https://github.com/yifanlu0227/HEAL", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate difficulty, requires conda for environment setup and specific package installations.", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 2}, "memory": "at least 16GB of RAM", "additional_requirements": "CUDA toolkit version compatible with PyTorch."}, "time_requirements": 2, "code_quality": "Well-documented code with clear installation and usage instructions.", "difficulty": 3, "stars": 181}}
{"id": "19128", "url": "https://iclr.cc/virtual/2024/poster/19128", "title": "CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction", "authors": [], "abstract": "Abstract:Open-vocabulary dense prediction tasks including object detection and image segmentation have been advanced by the success of Contrastive Language-Image Pre-training (CLIP). CLIP models, particularly those incorporating vision transformers (ViTs), have exhibited remarkable generalization ability in zero-shot image classification. However, when transferring the vision-language alignment of CLIP from global image representation to local region representation for the open-vocabulary dense prediction tasks, CLIP ViTs suffer from the domain shift from full images to local image regions. In this paper, we embark on an in-depth analysis of the region-language alignment in CLIP models, which is essential for downstream open-vocabulary dense prediction tasks. Subsequently, we propose an approach named CLIPSelf, which adapts the image-level recognition ability of CLIP ViT to local image regions without needing any region-text pairs. CLIPSelf empowers ViTs to distill itself by aligning a region representation extracted from its dense feature map with the image-level representation of the corresponding image crop. With the enhanced CLIP ViTs, we achieve new state-of-the-art performance on open-vocabulary object detection, semantic segmentation, and panoptic segmentation across various benchmarks. Models and code are released at https://github.com/wusize/CLIPSelf.", "pdf_url": "https://openreview.net/pdf?id=DjzvJCRsVf", "supplementary_url": "", "code_url": "https://github.com/wusize/CLIPSelf", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "8GB", "API_calls": "None"}, "time_requirements": "2", "code_quality": "well-documented code", "difficulty": 2, "stars": 183}}
{"id": "19398", "url": "https://iclr.cc/virtual/2024/poster/19398", "title": "Personalize Segment Anything Model with One Shot", "authors": [], "abstract": "Abstract:Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful promptable framework, revolutionizing the segmentation field. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored, e.g., automatically segmenting your pet dog in numerous images. In this paper, we introduce a training-free Personalization approach for SAM, termed PerSAM. Given only one-shot data, i.e., a single image with a reference mask, we first obtain a positive-negative location prior for the target concept in new images. Then, aided by target visual semantics, we empower SAM for personalized object segmentation via two proposed techniques: target-guided attention and target-semantic prompting. In this way, we can effectively customize the general-purpose SAM for private use without any training. To further alleviate the ambiguity of segmentation scales, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. To demonstrate our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized object segmentation, and also test our methods on various one-shot image and video segmentation benchmarks. Besides, we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. By mitigating the disturbance of training-set backgrounds, our approach showcases better target appearance generation and higher fidelity to the input text prompt. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM.", "pdf_url": "https://openreview.net/pdf?id=6Gzkhoc6YS", "supplementary_url": "", "code_url": "https://github.com/ZrrSkywalker/Personalize-SAM", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "at least 8GB", "API_call": "none"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 1560}}
{"id": "18846", "url": "https://iclr.cc/virtual/2024/poster/18846", "title": "LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving", "authors": [], "abstract": "Abstract:A map, as crucial information for downstream applications of an autonomous driving system, is usually represented in lanelines or centerlines. However, existing literature on map learning primarily focuses on either detecting geometry-based lanelines or perceiving topology relationships of centerlines. Both of these methods ignore the intrinsic relationship of lanelines and centerlines, that lanelines bind centerlines. While simply predicting both types of lane in one model is mutually excluded in learning objective, we advocate lane segment as a new representation that seamlessly incorporates both geometry and topology information. Thus, we introduce LaneSegNet, the first end-to-end mapping network generating lane segments to obtain a complete representation of the road structure. Our algorithm features two key modifications. One is a lane attention module to capture pivotal region details within the long-range feature space. Another is an identical initialization strategy for reference points, which enhances the learning of positional priors for lane attention. On the OpenLane-V2 dataset, LaneSegNet outperforms previous counterparts by a substantial gain across three tasks, i.e., map element detection (+4.8 mAP), centerline perception (+6.9 DET$_l$), and the newly defined one, lane segment perception (+5.6 mAP). Furthermore, it obtains a real-time inference speed of 14.7 FPS. Code is accessible at https://github.com/OpenDriveLab/LaneSegNet.", "pdf_url": "https://openreview.net/pdf?id=LsURkIPYR5", "supplementary_url": "", "code_url": "https://github.com/OpenDriveLab/LaneSegNet", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "conda", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 8}, "memory": "9.4G", "API_call": "not specified"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 295}}
{"id": "17938", "url": "https://iclr.cc/virtual/2024/poster/17938", "title": "Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting", "authors": [], "abstract": "Abstract:Transformers for time series forecasting mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. We propose Pathformer, a multi-scale Transformer with adaptive pathways. It integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multi-scale Transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics of the input, improving the accuracy and generalization of Pathformer. Extensive experiments on eleven real-world datasets demonstrate that Pathformer not only achieves state-of-the-art performance by surpassing all current models but also exhibits stronger generalization abilities under various transfer scenarios. The code is made available at  https://github.com/decisionintelligence/pathformer.", "pdf_url": "https://openreview.net/pdf?id=lJkOCMP2aW", "supplementary_url": "", "code_url": "https://github.com/decisionintelligence/pathformer", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 NVIDIA GPU (e.g., Tesla K80 or better recommended)", "memory": "At least 16GB of RAM", "API_call": "None"}, "time_requirements": "0.5", "code_quality": "well-documented", "difficulty": 2, "stars": 187}}
{"id": "18712", "url": "https://iclr.cc/virtual/2024/poster/18712", "title": "EasyTPP: Towards Open Benchmarking Temporal Point Processes", "authors": [], "abstract": "Abstract:Continuous-time event sequences play a vital role in real-world domains such as healthcare, finance, online shopping, social networks, and so on. To model such data, temporal point processes (TPPs) have emerged as the most natural and competitive models, making a significant impact in both academic and application communities. Despite the emergence of many powerful models in recent years, there hasn't been a central benchmark for these models and future research endeavors. This lack of standardization impedes researchers and practitioners from comparing methods and reproducing results, potentially slowing down progress in this field. In this paper, we present EasyTPP, the first central repository of research assets (e.g., data, models, evaluation programs, documentations) in the area of event sequence modeling. Our EasyTPP makes several unique contributions to this area: a unified interface of using existing datasets and adding new datasets; a wide range of evaluation programs that are easy to use and extend as well as facilitate reproducible research; implementations of popular neural TPPs, together with a rich library of modules by composing which one could quickly build complex models. We will actively maintain this benchmark and welcome contributions from other researchers and practitioners. Our benchmark will help promote reproducible research in this field, thus accelerating research progress as well as making more significant real-world impacts. The code and data are available at \\url{https://github.com/ant-research/EasyTemporalPointProcess}.", "pdf_url": "https://openreview.net/pdf?id=PJwAkg0z7h", "supplementary_url": "", "code_url": "https://github.com/ant-research/EasyTemporalPointProcess", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "8GB", "API_call": "none"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 2, "stars": 280}}
{"id": "18100", "url": "https://iclr.cc/virtual/2024/poster/18100", "title": "Lemur: Harmonizing Natural Language and Code for Language Agents", "authors": [], "abstract": "Abstract:We introduce Lemur and Lemur-Chat, openly accessible language models optimizedfor both natural language and coding capabilities to serve as the backboneof versatile language agents. The evolution from language chat models tofunctional language agents demands that models not only master human interaction,reasoning, and planning but also ensure grounding in the relevant environments.This calls for a harmonious blend of language and coding capabilitiesin the models. Lemur and Lemur-Chat are proposed to address this necessity,demonstrating balanced proficiencies in both domains, unlike existingopen-source models that tend to specialize in either. Through meticulous pretrainingusing a code-intensive corpus and instruction fine-tuning on text and codedata, our models achieve state-of-the-art averaged performance across diversetext and coding benchmarks. Comprehensive experiments demonstrate Lemur\u2019ssuperiority over existing open-source models and its proficiency across variousagent tasks involving human communication, tool usage, and interaction underfully- and partially- observable environments. The harmonization between naturaland programming languages enables Lemur-Chat to significantly narrow thegap with proprietary models on agent abilities, providing key insights into developingadvanced open-source agents adept at reasoning, planning, and operatingseamlessly across environments. Our model and code have been open-sourced athttps://github.com/OpenLemur/Lemur.", "pdf_url": "https://openreview.net/pdf?id=hNhwSmtXRh", "supplementary_url": "", "code_url": "https://github.com/OpenLemur/Lemur", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Requires conda for environment setup, specific commands provided in README.", "resource_requirements": "Requires GPU for optimal performance (suggested models listed), with specific versions of PyTorch and CUDA mentioned.", "time_requirements": "Estimated time for main command execution is around 1-2 hours.", "code_quality": "Well-documented code with clear setup instructions and example usage.", "difficulty": 2, "stars": 542}}
{"id": "18660", "url": "https://iclr.cc/virtual/2024/poster/18660", "title": "TorchRL: A data-driven decision-making library for PyTorch", "authors": [], "abstract": "Abstract:PyTorch has ascended as a premier machine learning framework, yet it lacks a native and comprehensive library for decision and control tasks suitable for large development teams dealing with complex real-world data and environments. To address this issue, we propose TorchRL, a generalistic control library for PyTorch that provides well-integrated, yet standalone components. We introduce a new and flexible PyTorch primitive, the TensorDict, which facilitates streamlined algorithm development across the many branches of Reinforcement Learning (RL) and control. We provide a detailed description of the building blocks and an extensive overview of the library across domains and tasks. Finally, we experimentally demonstrate its reliability and flexibility, and show comparative benchmarks to demonstrate its computational efficiency. TorchRL fosters long-term support and is publicly available on GitHub for greater reproducibility and collaboration within the research community. The code is open-sourced on GitHub.", "pdf_url": "https://openreview.net/pdf?id=QxItoEAVMb", "supplementary_url": "", "code_url": "https://github.com/pytorch/rl", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate (requires Python and PyTorch installation, possibly additional libraries)", "resource_requirements": {"GPUs": "1 or more NVIDIA GPUs recommended", "memory": "At least 8GB of RAM recommended", "API_calls": "Depends on the specific use case but may require API access for certain environments"}, "time_requirements": "Approximately 2-3 hours for initial setup and running examples", "code_quality": "Well-documented code with examples and tutorials", "difficulty": 3, "stars": 2570}}
{"id": "18860", "url": "https://iclr.cc/virtual/2024/poster/18860", "title": "Zoology: Measuring and Improving  Recall in Efficient Language Models", "authors": [], "abstract": "Abstract:Attention-free language models that combine gating and convolutions are growing in popularity due to their efficiency and increasingly competitive performance. To better understand these architectures, we pretrain a suite of 17 attention and gated-convolution language models, finding that SoTA gated-convolution architectures still underperform attention by up to 2.1 perplexity points on the Pile. In fine-grained analysis, we find 82% of the gap is explained by each model's ability to recall information that is previously mentioned in-context, e.g. \"Hakuna Matata means no worries Hakuna Matata it means no\" -> ??. On this task, termed \"associative recall\", we find that attention outperforms gated-convolutions by a large margin: a 70M parameter attention model outperforms a 1.4 billion parameter gated-convolution model on associative recall. This is surprising because prior work shows gated convolutions can perfectly solve synthetic tests for AR capability.  To close the gap between synthetics and real language, we develop a new formalization of the task called multi-query associative recall (MQAR) that better reflects actual language. We perform an empirical and theoretical study of MQAR that elucidates differences in the parameter-efficiency of attention and gated-convolution recall. Informed by our analysis, we evaluate simple convolution-attention hybrids and show that hybrids with input-dependent sparse attention patterns can close 97.4% of the gap to attention, while maintaining sub-quadratic scaling. Code is at: https://github.com/HazyResearch/zoology.", "pdf_url": "https://openreview.net/pdf?id=LY3ukUANko", "supplementary_url": "", "code_url": "https://github.com/HazyResearch/zoology", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "8GB", "additional_resources": "Access to Weights and Biases for logging"}, "time_requirements": 2, "code_quality": "well-documented", "difficulty": 3, "stars": 183}}
{"id": "19394", "url": "https://iclr.cc/virtual/2024/poster/19394", "title": "SaProt: Protein Language Modeling with Structure-aware Vocabulary", "authors": [], "abstract": "Abstract:Large-scale protein language models (PLMs), such as the ESM family, have achieved remarkable performance in various downstream tasks related to protein structure and function by undergoing unsupervised training on residue sequences. They have become essential tools for researchers and practitioners in biology.  However, a limitation of vanilla PLMs is their lack of explicit consideration for protein structure information, which suggests the potential for further improvement. Motivated by this, we introduce the concept of a ``structure-aware vocabulary\" that  integrates residue tokens with structure tokens.    The structure tokens are  derived  by encoding the 3D structure of proteins using Foldseek. We then propose SaProt, a large-scale general-purpose PLM trained on an extensive dataset comprising approximately 40 million protein sequences and structures. Through extensive evaluation, our SaProt model surpasses well-established and renowned baselines across 10 significant downstream tasks, demonstrating its exceptional capacity and broad applicability. We have made the code, pre-trained model, and all relevant materials available at https://github.com/westlake-repl/SaProt.", "pdf_url": "https://openreview.net/pdf?id=6MRm3G4NiU", "supplementary_url": "", "code_url": "https://github.com/westlake-repl/SaProt", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": "1-2 GPUs (NVIDIA recommended)", "memory": "Minimum 16 GB RAM recommended", "API_calls": "None specified"}, "time_requirements": "2-3 hours", "code_quality": "Well-documented", "difficulty": 3, "stars": 414}}
{"id": "19575", "url": "https://iclr.cc/virtual/2024/poster/19575", "title": "InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation", "authors": [], "abstract": "Abstract:Diffusion models have revolutionized text-to-image generation with its exceptional quality and creativity. However, its multi-step sampling process is known to be slow, often requiring tens of inference steps to obtain satisfactory results. Previous attempts to improve its sampling speed and reduce computational costs through distillation have been unsuccessful in achieving a functional one-step model.In this paper, we explore a recent method called Rectified Flow, which, thus far, has only been applied to small datasets. The core of Rectified Flow lies in its \\emph{reflow} procedure, which straightens the trajectories of probability flows, refines the coupling between noises and images, and facilitates the distillation process with student models. We propose a novel text-conditioned pipeline to turn Stable Diffusion (SD) into an ultra-fast one-step model, in which we find reflow plays a critical role in improving the assignment between noise and images. Leveraging our new pipeline, we create, to the best of our knowledge, the first one-step diffusion-based text-to-image generator with SD-level image quality, achieving an FID (Fr\u00e9chet Inception Distance) of $23.3$ on MS COCO 2017-5k, surpassing the previous state-of-the-art technique, progressive distillation, by a significant margin ($37.2$ $\\rightarrow$ $23.3$ in FID). By utilizing an expanded network with 1.7B parameters, we further improve the FID to $22.4$. We call our one-step models \\emph{InstaFlow}. On MS COCO 2014-30k, InstaFlow yields an FID of $13.1$ in just $0.09$ second, the best in $\\leq 0.1$ second regime, outperforming the recent StyleGAN-T ($13.9$ in $0.1$ second). Notably, the training of InstaFlow only costs 199 A100 GPU days. Codes and pre-trained models are available at \\url{github.com/gnobitab/InstaFlow}.", "pdf_url": "https://openreview.net/pdf?id=1k4yZbbDqX", "supplementary_url": "", "code_url": "https://github.com/gnobitab/InstaFlow", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "A100", "amount": 1}, "memory": "Not specified", "API_calls": "Not specified"}, "time_requirements": 1, "code_quality": "Well-documented code", "difficulty": 3, "stars": 1269}}
{"id": "17439", "url": "https://iclr.cc/virtual/2024/poster/17439", "title": "TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series", "authors": [], "abstract": "Abstract:We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based attentional copulas (TACTiS), wherein the number of distributional parameters now scales linearly with the number of variables instead of factorially. The new objective requires the introduction of a training curriculum, which goes hand-in-hand with necessary changes to the original architecture. We show that the resulting model has significantly better training dynamics and achieves state-of-the-art performance across diverse real-world forecasting tasks, while maintaining the flexibility of prior work, such as seamless handling of unaligned and unevenly-sampled time series. Code is made available at https://github.com/ServiceNow/TACTiS.", "pdf_url": "https://openreview.net/pdf?id=xtOydkE1Ku", "supplementary_url": "", "code_url": "https://github.com/ServiceNow/TACTiS", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Easy (requirements.txt available for pip installation)", "resource_requirements": "No specific hardware requirements mentioned, likely can run on a standard machine.", "time_requirements": "1-2 hours for running experiments and training models.", "code_quality": "Well-documented code with clear structure.", "difficulty": 2, "stars": 134}}
{"id": "18277", "url": "https://iclr.cc/virtual/2024/poster/18277", "title": "LLaMA-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention", "authors": [], "abstract": "Abstract:With the rising tide of large language models (LLMs), there has been a growing interest in developing general-purpose instruction-following models, e.g., ChatGPT. To this end, we present LLaMA-Adapter, a lightweight adaption method for efficient instruction tuning of LLaMA. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning. Specifically, a zero-initialized attention mechanism is proposed. It adopts a learnable zero gating to adaptively inject the instructional cues into LLaMA within self-attention layers, contributing to a stable training process and superior final performance. In this way, LLaMA-Adapter can generate high-quality responses to diverse language instructions, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, by incorporating an image encoder, our approach can be simply extended to a multi-modal LLM for image-conditioned instruction following, which achieves superior multi-modal reasoning capacity on several popular benchmarks (MME, MMBench, LVLM-eHub). Furthermore, we also verify the proposed zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa, CLIP) on traditional vision and language tasks, demonstrating the effectiveness and generalizability of our approach. Code and models are released at https://github.com/OpenGVLab/LLaMA-Adapter.", "pdf_url": "https://openreview.net/pdf?id=d4UiXAHN2W", "supplementary_url": "", "code_url": "https://github.com/OpenGVLab/LLaMA-Adapter", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA A100", "amount": 1}, "memory": "16 GB", "API_calls": "None"}, "time_requirements": "1", "code_quality": "Well-documented", "difficulty": 3, "stars": 5821}}
{"id": "17574", "url": "https://iclr.cc/virtual/2024/poster/17574", "title": "Unpaired Image-to-Image Translation via Neural Schr\u00f6dinger Bridge", "authors": [], "abstract": "Abstract:Diffusion models are a powerful class of generative models which simulate stochastic differential equations (SDEs) to generate data from noise. While diffusion models have achieved remarkable progress, they have limitations in unpaired image-to-image (I2I) translation tasks due to the Gaussian prior assumption. Schr\u00f6dinger Bridge (SB), which learns an SDE to translate between two arbitrary distributions, have risen as an attractive solution to this problem. Yet, to our best knowledge, none of SB models so far have been successful at unpaired translation between high-resolution images. In this work, we propose Unpaired Neural Schr\u00f6dinger Bridge (UNSB), which expresses the SB problem as a sequence of adversarial learning problems. This allows us to incorporate advanced discriminators and regularization to learn a SB between unpaired data. We show that UNSB is scalable and successfully solves various unpaired I2I translation tasks. Code: \\url{https://github.com/cyclomon/UNSB}", "pdf_url": "https://openreview.net/pdf?id=uQBW7ELXfO", "supplementary_url": "", "code_url": "https://github.com/cyclomon/UNSB", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "Not specified", "API_call": "Not specified"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 192}}
{"id": "17698", "url": "https://iclr.cc/virtual/2024/poster/17698", "title": "Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models", "authors": [], "abstract": "Abstract:Recent work has showcased the significant potential of diffusion models in pose-guided person image synthesis.However, owing to the inconsistency in pose between the source and target images, synthesizing an image with a distinct pose, relying exclusively on the source image and target pose information, remains a formidable challenge.This paper presents Progressive Conditional Diffusion Models (PCDMs) that incrementally bridge the gap between person images under the target and source poses through three stages.Specifically, in the first stage, we design a simple prior conditional diffusion model that predicts the global features of the target image by mining the global alignment relationship between pose coordinates and image appearance.Then, the second stage establishes a dense correspondence between the source and target images using the global features from the previous stage, and an inpainting conditional diffusion model is proposed to further align and enhance the contextual features, generating a coarse-grained person image.In the third stage, we propose a refining conditional diffusion model to utilize the coarsely generated image from the previous stage as a condition, achieving texture restoration and enhancing fine-detail consistency.The three-stage PCDMs work progressively to generate the final high-quality and high-fidelity synthesized image.Both qualitative and quantitative results demonstrate the consistency and photorealism of our proposed PCDMs under challenging scenarios.The code and model will be available at https://github.com/tencent-ailab/PCDMs.", "pdf_url": "https://openreview.net/pdf?id=rHzapPnCgT", "supplementary_url": "", "code_url": "https://github.com/tencent-ailab/PCDMs", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate difficulty; requires installation of various Python packages using pip, including specific versions of libraries.", "resource_requirements": {"GPUs": "1 or more NVIDIA GPUs recommended for training.", "memory": "At least 16GB of RAM recommended.", "API_calls": "Not specified in the README."}, "time_requirements": "Approximately 4-5 hours to fully train and test all stages.", "code_quality": "Well-documented code, with clear instructions and comments.", "difficulty": 3, "stars": 165}}
{"id": "19209", "url": "https://iclr.cc/virtual/2024/poster/19209", "title": "Large Language Models as Optimizers", "authors": [], "abstract": "Abstract:Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.", "pdf_url": "https://openreview.net/pdf?id=Bb4VGOWELI", "supplementary_url": "", "code_url": "https://github.com/google-deepmind/opro", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "None specified, but likely requires GPUs for training/evaluation.", "memory": "Not explicitly mentioned, but likely requires a decent amount of RAM for processing.", "API_call": "Requires API keys for both OpenAI and Google PaLM."}, "time_requirements": "2-3 hours", "code_quality": "well-documented", "difficulty": 3, "stars": 506}}
{"id": "18251", "url": "https://iclr.cc/virtual/2024/poster/18251", "title": "Decoding Natural Images from EEG for Object Recognition", "authors": [], "abstract": "Abstract:Electroencephalography (EEG) signals, known for convenient non-invasive acquisition but low signal-to-noise ratio, have recently gained substantial attention due to the potential to decode natural images. This paper presents a self-supervised framework to demonstrate the feasibility of learning image representations from EEG signals, particularly for object recognition. The framework utilizes image and EEG encoders to extract features from paired image stimuli and EEG responses. Contrastive learning aligns these two modalities by constraining their similarity. Our approach achieves state-of-the-art results on a comprehensive EEG-image dataset, with a top-1 accuracy of 15.6% and a top-5 accuracy of 42.8% in 200-way zero-shot tasks. Moreover, we perform extensive experiments to explore the biological plausibility by resolving the temporal, spatial, spectral, and semantic aspects of EEG signals. Besides, we introduce attention modules to capture spatial correlations, providing implicit evidence of the brain activity perceived from EEG data. These findings yield valuable insights for neural decoding and brain-computer interfaces in real-world scenarios. Code available at https://github.com/eeyhsong/NICE-EEG.", "pdf_url": "https://openreview.net/pdf?id=dhLIno8FmH", "supplementary_url": "", "code_url": "https://github.com/eeyhsong/NICE-EEG", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_calls": "None specified"}, "time_requirements": 5, "code_quality": "well-documented code", "difficulty": 3, "stars": 128}}
{"id": "17404", "url": "https://iclr.cc/virtual/2024/poster/17404", "title": "Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion", "authors": [], "abstract": "Abstract:Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 1.92). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, unlike CM, CTM's access to the score function can streamline the adoption of established controllable/conditional generation methods from the diffusion community. This access also enables the computation of likelihood. The code is available at https://github.com/sony/ctm.", "pdf_url": "https://openreview.net/pdf?id=ymjI8feDTD", "supplementary_url": "", "code_url": "https://github.com/sony/ctm", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": "1 or more"}, "memory": "16GB or more", "API_call": "Requires access to specific datasets"}, "time_requirements": "5-10", "code_quality": "Well-documented code", "difficulty": 4, "stars": 269}}
{"id": "18640", "url": "https://iclr.cc/virtual/2024/poster/18640", "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning", "authors": [], "abstract": "Abstract:Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of \\emph{explanations as features}: we prompt an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an \\emph{LLM-to-LM interpreter} to translate these explanations into informative features for downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art results on well-established TAG datasets, including \\texttt{Cora}, \\texttt{PubMed}, \\texttt{ogbn-arxiv}, as well as our newly introduced dataset, \\texttt{tape-arxiv23}. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on \\texttt{ogbn-arxiv}. Lastly, we believe the versatility of the proposed method extends beyond TAGs and holds the potential to enhance other tasks involving graph-text data~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/TAPE}}.", "pdf_url": "https://openreview.net/pdf?id=RXFVcynVe1", "supplementary_url": "", "code_url": "https://github.com/XiaoxinHe/TAPE", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "conda", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 4}, "memory": "8 GB", "api_call": "None specified"}, "time_requirements": 6, "code_quality": "well-documented code", "difficulty": 3, "stars": 214}}
{"id": "19203", "url": "https://iclr.cc/virtual/2024/poster/19203", "title": "Complete and Efficient Graph Transformers for Crystal Material Property Prediction", "authors": [], "abstract": "Abstract:Crystal structures are characterized by atomic bases within a primitive unit cell that repeats along a regular lattice throughout 3D space. The periodic and infinite nature of crystals poses unique challenges for geometric graph representation learning. Specifically, constructing graphs that effectively capture  the complete  geometric information of crystals and handle chiral crystals remains an unsolved and challenging problem. In this paper, we introduce a novel approach that utilizes the periodic patterns of unit cells to establish the lattice-based representation for each atom, enabling efficient and expressive graph representations of crystals. Furthermore, we propose ComFormer, a SE(3) transformer designed specifically for crystalline materials. ComFormer includes two variants; namely, iComFormer that employs invariant geometric descriptors of Euclidean distances and angles, and eComFormer that utilizes equivariant vector representations.  Experimental results demonstrate the state-of-the-art predictive accuracy of ComFormer variants on various tasks across three widely-used crystal benchmarks. Our code is publicly available as part of the AIRS library (https://github.com/divelab/AIRS).", "pdf_url": "https://openreview.net/pdf?id=BnQY9XiRAS", "supplementary_url": "", "code_url": "https://github.com/divelab/AIRS", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate difficulty; requires Python and possibly specific libraries that may need to be installed via pip or conda.", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB RAM recommended", "API_calls": "Not specified"}, "time_requirements": "Approximately 2 hours to run the main command.", "code_quality": "Well-documented code with clear instructions.", "difficulty": 3, "stars": 572}}
{"id": "17493", "url": "https://iclr.cc/virtual/2024/poster/17493", "title": "MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo", "authors": [], "abstract": "Abstract:Recent advancements in learning-based Multi-View Stereo (MVS) methods have prominently featured transformer-based models with attention mechanisms. However, existing approaches have not thoroughly investigated the profound influence of transformers on different MVS modules, resulting in limited depth estimation capabilities. In this paper, we introduce MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. Formally, our approach involves infusing cross-view information into the pre-trained DINOv2 model to facilitate MVS learning. Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively. Additionally, we uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling, and the position of layer normalization. Comprehensive experiments on DTU, Tanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the proposed method. Notably, MVSFormer++ achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks. Codes and models are available at https://github.com/maybeLx/MVSFormerPlusPlus.", "pdf_url": "https://openreview.net/pdf?id=wXWfvSpYHh", "supplementary_url": "", "code_url": "https://github.com/maybeLx/MVSFormerPlusPlus", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Easy (pip install requirements.txt)", "resource_requirements": {"GPUs": {"type": "A6000", "amount": 4}, "memory": "48GB", "other": "CUDA, Python"}, "time_requirements": 24, "code_quality": "Well-documented code", "difficulty": 2, "stars": 205}}
{"id": "17829", "url": "https://iclr.cc/virtual/2024/poster/17829", "title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization", "authors": [], "abstract": "Abstract:We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time.  Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}", "pdf_url": "https://openreview.net/pdf?id=oDdzXQzP2F", "supplementary_url": "", "code_url": "https://github.com/transformer-vq/transformer_vq", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "pip", "resource_requirements": {"GPUs": {"type": "TPU or GPU", "amount": "Depending on the user's choice"}, "memory": "Not specified", "API_call": "Requires WANDB API key"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 187}}
{"id": "19212", "url": "https://iclr.cc/virtual/2024/poster/19212", "title": "Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions", "authors": [], "abstract": "Abstract:Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs\u2019 underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. The code and models are available at https://github.com/DCDmllm/Cheetah.", "pdf_url": "https://openreview.net/pdf?id=BXY6fe7q31", "supplementary_url": "", "code_url": "https://github.com/DCDmllm/Cheetah", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "other_requirements": "Vicuna-7B and LLaMA2-7B weights"}, "time_requirements": 1, "code_quality": "Well-documented code", "difficulty": 3, "stars": 345}}
{"id": "18439", "url": "https://iclr.cc/virtual/2024/poster/18439", "title": "Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing", "authors": [], "abstract": "Abstract:Calibration measures and reliability diagrams are two fundamental tools for measuring and interpreting the calibration of probabilistic predictors. Calibration measures quantify the degree of miscalibration, and reliability diagrams visualize the structure of this miscalibration. However, the most common constructions of reliability diagrams and calibration measures --- binning and ECE --- both suffer from well-known flaws (e.g. discontinuity). We show that a simple modification fixes both constructions: first smooth the observations using an RBF kernel, then compute the Expected Calibration Error (ECE) of this smoothed function. We prove that with a careful choice of bandwidth, this method yields a calibration measure that is well-behaved in the sense of (Blasiok, Gopalan, Hu, and Nakkiran 2023) --- a consistent calibration measure. We call this measure the SmoothECE. Moreover, the reliability diagram obtained from this smoothed function visually encodes the SmoothECE, just as binned reliability diagrams encode the BinnedECE.   We also release a Python package with simple, hyperparameter-free methods for measuring and plotting calibration: \"pip install relplot.\"Code at: https://github.com/apple/ml-calibration", "pdf_url": "https://openreview.net/pdf?id=XwiA1nDahv", "supplementary_url": "", "code_url": "https://github.com/apple/ml-calibration", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "None", "memory": "Standard memory required for running Python scripts.", "API_calls": "None"}, "time_requirements": "1", "code_quality": "well-documented", "difficulty": 2, "stars": 139}}
{"id": "17426", "url": "https://iclr.cc/virtual/2024/poster/17426", "title": "Adapting Large Language Models via Reading Comprehension", "authors": [], "abstract": "Abstract:We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension--practice after reading improves the ability to answer questions based on the learned knowledge--we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specific reading comprehension texts can improve the model's performance even on general benchmarks, showing the potential to develop a general model across even more domains. Our model, code, and data are available at https://github.com/microsoft/LMOps.", "pdf_url": "https://openreview.net/pdf?id=y886UXPEZ0", "supplementary_url": "", "code_url": "https://github.com/microsoft/LMOps", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate difficulty, requires Python dependencies which can be managed using pip, or conda for environment management.", "resource_requirements": {"GPUs": "1-2 NVIDIA GPUs recommended", "memory": "At least 16GB RAM", "API_calls": "Usage of external APIs may be needed depending on the model"}, "time_requirements": "Estimated 2-3 hours to run main experiments and obtain results.", "code_quality": "Well-documented code with clear instructions and examples provided.", "difficulty": 3, "stars": 3867}}
{"id": "17451", "url": "https://iclr.cc/virtual/2024/poster/17451", "title": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs", "authors": [], "abstract": "Abstract:In human-written articles, we often leverage the subtleties of text style, such as bold and italics, to guide the attention of readers. These textual emphases are vital for the readers to grasp the conveyed information.  When interacting with large language models (LLMs), we have a similar need -- steering the model to pay closer attention to user-specified information, e.g., an instruction. Existing methods, however, are constrained to process plain text and do not support such a mechanism. This motivates us to introduce PASTA -- Post-hoc Attention STeering Approach, a method that allows LLMs to read text with user-specified emphasis marks. To this end, PASTA identifies a small subset of attention heads and applies precise attention reweighting on them, directing the model attention to user-specified parts. Like prompting, PASTA is applied at inference time and does not require changing any model parameters. Experiments demonstrate that PASTA can substantially enhance an LLM's ability to follow user instructions or integrate new knowledge from user inputs, leading to a significant performance improvement on a variety of tasks, e.g., an average accuracy improvement of 22\\% for LLAMA-7B. Our code is publicly available at https://github.com/QingruZhang/PASTA .", "pdf_url": "https://openreview.net/pdf?id=xZDWO0oejD", "supplementary_url": "", "code_url": "https://github.com/QingruZhang/PASTA", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "conda, pip", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "8GB", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 113}}
{"id": "17644", "url": "https://iclr.cc/virtual/2024/poster/17644", "title": "Evaluating Representation Learning on the Protein Structure Universe", "authors": [], "abstract": "Abstract:We introduce ProteinWorkshop, a comprehensive benchmark suite for representation learning on protein structures with Geometric Graph Neural Networks. We consider large-scale pre-training and downstream tasks on both experimental and predicted structures to enable the systematic evaluation of the quality of the learned structural representation and their usefulness in capturing functional relationships for downstream tasks. We find that: (1) large-scale pretraining on AlphaFold structures and auxiliary tasks consistently improve the performance of both rotation-invariant and equivariant GNNs, and (2) more expressive equivariant GNNs benefit from pretraining to a greater extent compared to invariant models.We aim to establish a common ground for the machine learning and computational biology communities to rigorously compare and advance protein structure representation learning. Our open-source codebase reduces the barrier to entry for working with large protein structure datasets by providing: (1) storage-efficient dataloaders for large-scale structural databases including AlphaFoldDB and ESM Atlas, as well as (2) utilities for constructing new tasks from the entire PDB. ProteinWorkshop is available at: github.com/a-r-j/ProteinWorkshop.", "pdf_url": "https://openreview.net/pdf?id=sTYuRVrdK3", "supplementary_url": "", "code_url": "https://github.com/a-r-j/ProteinWorkshop", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": "1 NVIDIA GPU recommended", "memory": "8 GB minimum, 16 GB recommended", "API_call": "N/A"}, "time_requirements": "2", "code_quality": "Well-documented code", "difficulty": 3, "stars": 223}}
{"id": "17595", "url": "https://iclr.cc/virtual/2024/poster/17595", "title": "Zero Bubble (Almost) Pipeline Parallelism", "authors": [], "abstract": "Abstract:Pipeline parallelism is one of the key components for large-scale distributed training, yet its efficiency suffers from pipeline bubbles which were deemed inevitable. In this work, we introduce a scheduling strategy that, to our knowledge, is the first to successfully achieve zero pipeline bubbles under synchronous training semantics. The key idea behind this improvement is to split the backward computation into two parts, one that computes gradient for the input and another that computes for the parameters. Based on this idea, we handcraft novel pipeline schedules that significantly outperform the baseline methods. We further develop an algorithm that automatically finds an optimal schedule based on specific model configuration and memory limit. Additionally, to truly achieve zero bubble, we introduce a novel technique to bypass synchronizations during the optimizer step. Experimental evaluations show that our method outperforms the 1F1B schedule up to 15\\% in throughput under a similar memory limit. This number can be further pushed to 30\\% when the memory constraint is relaxed. We believe our results mark a major step forward in harnessing the true potential of pipeline parallelism. The source code based on Megatron-LM is publicly avaiable at \\url{https://github.com/sail-sg/zero-bubble-pipeline-parallelism}.", "pdf_url": "https://openreview.net/pdf?id=tuzTN0eIO5", "supplementary_url": "", "code_url": "https://github.com/sail-sg/zero-bubble-pipeline-parallelism", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 8}, "memory": "16 GB", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "well-documented", "difficulty": 3, "stars": 344}}
{"id": "17626", "url": "https://iclr.cc/virtual/2024/poster/17626", "title": "Controlling Vision-Language Models for Multi-Task Image Restoration", "authors": [], "abstract": "Abstract:Vision-language models such as CLIP have shown great impact on diverse downstream tasks for zero-shot or label-free predictions. However, when it comes to low-level vision such as image restoration their performance deteriorates dramatically due to corrupted inputs. In this paper, we present a degradation-aware vision-language model (DA-CLIP) to better transfer pretrained vision-language models to low-level vision tasks as a multi-task framework for image restoration. More specifically, DA-CLIP trains an additional controller that adapts the fixed CLIP image encoder to predict high-quality feature embeddings. By integrating the embedding into an image restoration network via cross-attention, we are able to pilot the model to learn a high-fidelity image reconstruction. The controller itself will also output a degradation feature that matches the real corruptions of the input, yielding a natural classifier for different degradation types. In addition, we construct a mixed degradation dataset with synthetic captions for DA-CLIP training. Our approach advances state-of-the-art performance on both degradation-specific and unified image restoration tasks, showing a promising direction of prompting image restoration with large-scale pretrained vision-language models. Our code is available at https://github.com/Algolzw/daclip-uir.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/Algolzw/daclip-uir", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate difficulty, requires Python 3.8, specific dependencies managed via pip, with instructions for a virtual environment.", "resource_requirements": "Single GPU (NVIDIA with CUDA 11.4), at least 8GB VRAM recommended. Memory requirement not explicitly stated.", "time_requirements": "Approximately 2-3 hours for training depending on dataset size and GPU speed.", "code_quality": "Well-documented code with clear instructions and comments.", "difficulty": 3, "stars": 721}}
{"id": "18933", "url": "https://iclr.cc/virtual/2024/poster/18933", "title": "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting", "authors": [], "abstract": "Abstract:The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-forward network on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting. Code is available at this repository: https://github.com/thuml/iTransformer.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/thuml/iTransformer", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "The setup requires installing dependencies via pip with a requirements.txt file. No specific mention of conda or docker.", "resource_requirements": {"GPUs": "The README does not specify the type or number of GPUs required, but it is implied that GPU acceleration may be beneficial.", "memory": "Memory requirements are not explicitly mentioned.", "API_calls": "None specified."}, "time_requirements": 3, "code_quality": "The code quality is described as good with clear instructions on usage, but the documentation could be improved further.", "difficulty": 3, "stars": 1482}}
{"id": "17801", "url": "https://iclr.cc/virtual/2024/poster/17801", "title": "Recursive Generalization Transformer for Image Super-Resolution", "authors": [], "abstract": "Abstract:Transformer architectures have exhibited remarkable performance in image super-resolution (SR). Since the quadratic computational complexity of the self-attention (SA) in Transformer, existing methods tend to adopt SA in a local region to reduce overheads. However, the local design restricts the global context exploitation, which is crucial for accurate image reconstruction. In this work, we propose the Recursive Generalization Transformer (RGT) for image SR, which can capture global spatial information and is suitable for high-resolution images. Specifically, we propose the recursive-generalization self-attention (RG-SA). It recursively aggregates input features into representative feature maps, and then utilizes cross-attention to extract global information. Meanwhile, the channel dimensions of attention matrices ($query$, $key$, and $value$) are further scaled to mitigate the redundancy in the channel domain. Furthermore, we combine the RG-SA with local self-attention to enhance the exploitation of the global context, and propose the hybrid adaptive integration (HAI) for module integration. The HAI allows the direct and effective fusion between features at different levels (local or global). Extensive experiments demonstrate that our RGT outperforms recent state-of-the-art methods quantitatively and qualitatively. Code and pre-trained models are available at https://github.com/zhengchen1999/RGT.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/zhengchen1999/RGT", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "4 NVIDIA GPUs", "memory": "not specified", "API_call": "not specified"}, "time_requirements": "2-4 hours", "code_quality": "well-documented code", "difficulty": 3, "stars": 157}}
{"id": "17875", "url": "https://iclr.cc/virtual/2024/poster/17875", "title": "OctoPack: Instruction Tuning Code Large Language Models", "authors": [], "abstract": "Abstract:Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/bigcode-project/octopack", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA A100 or equivalent", "amount": 1}, "memory": "16GB or more", "API_calls": "may be required for model evaluation"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 456}}
{"id": "17990", "url": "https://iclr.cc/virtual/2024/poster/17990", "title": "FasterViT: Fast Vision Transformers with Hierarchical Attention", "authors": [], "abstract": "Abstract:We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/NVlabs/FasterViT", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": [{"type": "NVIDIA", "amount": 1}], "memory": "at least 8 GB", "API_calls": "N/A"}, "time_requirements": 2, "code_quality": "well-documented", "difficulty": 2, "stars": 821}}
{"id": "19263", "url": "https://iclr.cc/virtual/2024/poster/19263", "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "authors": [], "abstract": "Abstract:Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling.Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat and https://huggingface.co/openchat.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/imoneoi/openchat", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "RTX 3090", "amount": 1}, "memory": "80GB", "API_calls": "OpenAI API"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 5313}}
{"id": "18327", "url": "https://iclr.cc/virtual/2024/poster/18327", "title": "R-MAE: Regions Meet Masked Autoencoders", "authors": [], "abstract": "Abstract:In this work, we explore regions as a potential visual analogue of words for self-supervised image representation learning. Inspired by Masked Autoencoding (MAE), a generative pre-training baseline, we propose masked region autoencoding to learn from groups of pixels or regions. Specifically, we design an architecture which efficiently addresses the one-to-many mapping between images and regions, while being highly effective especially with high-quality regions. When integrated with MAE, our approach (R-MAE) demonstrates consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks, with negligible computational overheads. Beyond the quantitative evaluation, our analysis indicates the models pre-trained with masked region autoencoding unlock the potential for interactive segmentation. The code is provided at https://github.com/facebookresearch/r-mae.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/facebookresearch/r-mae", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "2", "resource_requirements": {"GPUs": "1 or more NVIDIA GPUs with CUDA >= 11", "memory": "8 GB or more recommended", "API_calls": "None specified"}, "time_requirements": "10", "code_quality": "4", "difficulty": 3, "stars": 112}}
{"id": "18531", "url": "https://iclr.cc/virtual/2024/poster/18531", "title": "CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents", "authors": [], "abstract": "Abstract:The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization\u2019s profound alignment with human society requires sophisticated learning and prior knowledge, while its ever-changing space and action space demand robust reasoning for generalization. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further research, we present initial results for both paradigms. The canonical RL-based agents exhibit reasonable performance in mini-games, whereas both RL- and LLM-based agents struggle to make substantial progress in the full game. Overall, CivRealm stands as a unique learning and reasoning challenge for decision-making agents. The code is available at https://github.com/bigai-ai/civrealm.", "pdf_url": "https://openreview.net/pdf?id=UBVNwD3hPN", "supplementary_url": "", "code_url": "https://github.com/bigai-ai/civrealm", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "1x NVIDIA GPU (recommended for training agents)", "memory": "8GB RAM minimum, 16GB recommended", "API_call": "Requires access to Freeciv-web server"}, "time_requirements": "2", "code_quality": "well-documented code", "difficulty": 3, "stars": 106}}
{"id": "17730", "url": "https://iclr.cc/virtual/2024/poster/17730", "title": "Relay Diffusion: Unifying diffusion process across resolutions for image synthesis", "authors": [], "abstract": "Abstract:Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that *the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain*. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\\times$256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at \\url{https://github.com/THUDM/RelayDiffusion}.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/THUDM/RelayDiffusion", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate (requires conda for environment setup)", "resource_requirements": {"GPUs": {"type": "Nvidia A100", "amount": 1}, "memory": "At least 16GB", "other": "Support for memory-efficient attention"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 288}}
{"id": "18518", "url": "https://iclr.cc/virtual/2024/poster/18518", "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models", "authors": [], "abstract": "Abstract:Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that \\method is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios. The code is made available at https://github.com/KimMeen/Time-LLM.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/KimMeen/Time-LLM", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA A100", "amount": 1}, "memory": "16GB", "API_calls": "None"}, "time_requirements": 2, "code_quality": "well-documented", "difficulty": 3, "stars": 1787}}
{"id": "18540", "url": "https://iclr.cc/virtual/2024/poster/18540", "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback", "authors": [], "abstract": "Abstract:With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowd workers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.Code is available at https://github.com/PKU-Alignment/safe-rlhf.Warning: This paper contains example data that may be offensive or harmful.", "pdf_url": "https://openreview.net/pdf?id=TyFrPOKYXw", "supplementary_url": "", "code_url": "https://github.com/PKU-Alignment/safe-rlhf", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA A800-80GB", "amount": 8}, "memory": "Not specified", "API_call": "OpenAI API key required for evaluation"}, "time_requirements": 5, "code_quality": "Well-documented code", "difficulty": 3, "stars": 1421}}
{"id": "18794", "url": "https://iclr.cc/virtual/2024/poster/18794", "title": "Efficient Streaming Language Models with Attention Sinks", "authors": [], "abstract": "Abstract:Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges.Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory.Secondly, popular LLMs cannot generalize to longer texts than the training sequence length.Window attention, where only the most recent KVs are cached, is a natural approach --- but we show that it fails when the text length surpasses the cache size.We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important.Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning.We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2$\\times$ speedup.Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/mit-han-lab/streaming-llm", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Medium (requires conda for environment setup and multiple pip installations)", "resource_requirements": {"GPUs": "1 Nvidia GPU (CUDA enabled)", "memory": "At least 8GB RAM recommended"}, "time_requirements": "2 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 6814}}
{"id": "19390", "url": "https://iclr.cc/virtual/2024/poster/19390", "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models", "authors": [], "abstract": "Abstract:We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost.Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset. All our code, models, dataset, and demo are available at https://github.com/dvlab-research/LongLoRA.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/dvlab-research/LongLoRA", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 8}, "memory": "32GB", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "Well-documented code", "difficulty": 3, "stars": 2648}}
{"id": "18554", "url": "https://iclr.cc/virtual/2024/poster/18554", "title": "Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models", "authors": [], "abstract": "Abstract:Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. Each component aims to improve the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large models' performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. Mol-Instructions is publicly available for ongoing research and will undergo regular updates to enhance its applicability (https://github.com/zjunlp/Mol-Instructions).", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/zjunlp/Mol-Instructions", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "1-2 NVIDIA GPUs (e.g., RTX 2080 or higher)", "memory": "16GB RAM minimum", "API_calls": "Hugging Face API for model weights"}, "time_requirements": "2-3 hours", "code_quality": "well-documented code", "difficulty": 3, "stars": 267}}
{"id": "18576", "url": "https://iclr.cc/virtual/2024/poster/18576", "title": "TiC-CLIP: Continual Training of CLIP Models", "authors": [], "abstract": "Abstract:Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataComp, TiC-YFCC, and TiC-Redcaps. TiC-DataComp, our largest dataset, contains over 12.7B timestamped image-text pairs spanning 9 years (2014-2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\\approx 8\\%$ zero-shot accuracy on our curated retrieval task from 2021-2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the last checkpoint  and replays old data reduces compute by $2.5\\times$ when compared to the standard practice of retraining from scratch. Code is available at https://github.com/apple/ml-tic-clip.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/apple/ml-tic-clip", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 8}, "memory": "16GB", "API_calls": "required for data retrieval"}, "time_requirements": 3, "code_quality": "well-documented code", "difficulty": 3, "stars": 102}}
{"id": "18607", "url": "https://iclr.cc/virtual/2024/poster/18607", "title": "Controlled Text Generation via Language Model Arithmetic", "authors": [], "abstract": "Abstract:As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important. In this work, we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of generated text while outperforming state-of-the-art on the task of toxicity reduction. We release an open source easy-to-use implementation of our framework at https://github.com/eth-sri/language-model-arithmetic.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/eth-sri/language-model-arithmetic", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "Nvidia H100", "amount": 1}, "memory": "80GB", "API_call": "PERSPECTIVE API, OpenAI API"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 215}}
{"id": "19476", "url": "https://iclr.cc/virtual/2024/poster/19476", "title": "Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space", "authors": [], "abstract": "Abstract:Recent advances in tabular data generation have greatly enhanced synthetic data quality. However, extending diffusion models to tabular data is challenging due to the intricately varied distributions and a blend of data types of tabular data. This paper introduces TabSyn, a methodology that synthesizes tabular data by leveraging a diffusion model within a variational autoencoder (VAE) crafted latent space. The key advantages of the proposed Tabsyn include (1) Generality: the ability to handle a broad spectrum of data types by converting them into a single unified space and explicitly capturing inter-column relations; (2) Quality: optimizing the distribution of latent embeddings to enhance the subsequent training of diffusion models, which helps generate high-quality synthetic data; (3) Speed: much fewer number of reverse steps and faster synthesis speed than existing diffusion-based methods. Extensive experiments on six datasets with five metrics demonstrate that Tabsyn outperforms existing methods. Specifically, it reduces the error rates by 86% and 67% for column-wise distribution and pair-wise column correlation estimations compared with the most competitive baselines. The code has been made available at https://github.com/amazon-science/tabsyn.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/amazon-science/tabsyn", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate (Use of conda to create environments and install dependencies)", "resource_requirements": {"GPUs": "1 NVIDIA GPU with CUDA support (Recommended: NVIDIA GPU with CUDA 11.7)", "Memory": "Not specified", "API calls": "Not specified"}, "time_requirements": "Approximately 3-4 hours for full training and evaluation", "code_quality": "Well-documented code with clear instructions and examples", "difficulty": 3, "stars": 125}}
{"id": "17618", "url": "https://iclr.cc/virtual/2024/poster/17618", "title": "Matryoshka Diffusion Models", "authors": [], "abstract": "Abstract:Diffusion models are the de-facto approach for generating high-quality images and videos, but learning high-dimensional models remains a formidable task due to computational and optimization challenges. Existing methods often resort to training cascaded models in pixel space, or using a downsampled latent space of a separately trained auto-encoder. In this paper, we introduce Matryoshka Diffusion (MDM), an end-to-end framework for high-resolution image and video synthesis. We propose a diffusion process that denoises inputs at multiple resolutions jointly and uses a NestedUNet architecture where features and parameters for small-scale inputs are nested within those of large scales. In addition, MDM enables a progressive training schedule from lower to higher resolutions, which leads to significant improvements in optimization for high-resolution generation. We demonstrate the effectiveness of our approach on various benchmarks, including class-conditioned image generation, high-resolution text-to-image, and text-to-video applications. Remarkably, we can train a single pixel-space model at resolutions of up to 1024x1024 pixels, demonstrating strong zero-shot generalization using the CC12M dataset, which contains only 12 million images. Code and pre-trained checkpoints are released at https://github.com/apple/ml-mdm.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/apple/ml-mdm", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": {"type": "any", "amount": "1 or more"}, "memory": "16GB or more recommended"}, "time_requirements": "2-4", "code_quality": "well-documented code", "difficulty": 3, "stars": 481}}
{"id": "18751", "url": "https://iclr.cc/virtual/2024/poster/18751", "title": "DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models", "authors": [], "abstract": "Abstract:Despite the ability of text-to-image (T2I) diffusion models to generate high-quality images, transferring this ability to accurate image editing remains a challenge. In this paper, we propose a novel image editing method, DragonDiffusion, enabling Drag-style manipulation on Diffusion models. Specifically, we treat image editing as the change of feature correspondence in a pre-trained diffusion model. By leveraging feature correspondence, we develop energy functions that align with the editing target, transforming image editing operations into gradient guidance. Based on this guidance approach, we also construct multi-scale guidance that considers both semantic and geometric alignment. Furthermore, we incorporate a visual cross-attention strategy based on a memory bank design to ensure consistency between the edited result and original image. Benefiting from these efficient designs, all content editing and consistency operations come from the feature correspondence without extra model fine-tuning. Extensive experiments demonstrate that our method has promising performance on various image editing tasks, including within a single image (e.g., object moving, resizing, and content dragging) or across images (e.g., appearance replacing and object pasting). Code is available at https://github.com/MC-E/DragonDiffusion.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/MC-E/DragonDiffusion", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Medium (requires Python >= 3.8 and PyTorch >= 2.0.1, installation via pip or conda)", "resource_requirements": "16GB GPU memory for editing a 768x768 image", "time_requirements": "0.5", "code_quality": "Well-documented code", "difficulty": 3, "stars": 746}}
{"id": "19560", "url": "https://iclr.cc/virtual/2024/poster/19560", "title": "RETSim: Resilient and Efficient Text Similarity", "authors": [], "abstract": "Abstract:This paper introduces RETSim (Resilient and Efficient Text Similarity), a lightweight, multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings, achieving new state-of-the-art performance on dataset deduplication, adversarial text retrieval benchmarks, and spam clustering tasks. Additionally, we introduce the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset), enabling the evaluation of models on typo-laden near-duplicate text retrieval in a multilingual setting. RETSim and the W4NT3D benchmark are released under the MIT License at https://github.com/google/unisim.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/google/unisim", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "1 or more GPUs recommended for acceleration", "memory": "not specified", "API_calls": "not specified"}, "time_requirements": 1, "code_quality": "well-documented code", "difficulty": 2, "stars": 132}}
{"id": "19579", "url": "https://iclr.cc/virtual/2024/poster/19579", "title": "ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation", "authors": [], "abstract": "Abstract:Domain shifts such as sensor type changes and geographical situation variations are prevalent in Autonomous Driving (AD), which poses a challenge since AD model relying on the previous domain knowledge can be hardly directly deployed to a new domain without additional costs. In this paper, we provide a new perspective and approach of alleviating the domain shifts, by proposing a Reconstruction-Simulation-Perception (ReSimAD) scheme. Specifically, the implicit reconstruction process is based on the knowledge from the previous old domain, aiming to convert the domain-related knowledge into domain-invariant representations, e.g., 3D scene-level meshes. Besides, the point clouds simulation process of multiple new domains is conditioned on the above reconstructed 3D meshes, where the target-domain-like simulation samples can be obtained, thus reducing the cost of collecting and annotating new-domain data for the subsequent perception process. For experiments, we consider different cross-domain situations such as Waymo-to-KITTI, Waymo-to-nuScenes, etc, to verify the zero-shot target-domain perception using ReSimAD. Results demonstrate that our method is beneficial to boost the domain generalization ability, even promising for 3D pre-training. Code and simulated points are available at: https://github.com/PJLab-ADG/3DTrans", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/PJLab-ADG/3DTrans", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate difficulty due to the need for specific configurations and dataset preparation.", "resource_requirements": {"GPUs": {"type": "NVIDIA A100", "amount": 4}, "memory": "not specified", "API_calls": "not specified"}, "time_requirements": 24, "code_quality": "Well-documented code with references to multiple supporting documents.", "difficulty": 3, "stars": 547}}
{"id": "19600", "url": "https://iclr.cc/virtual/2024/poster/19600", "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models", "authors": [], "abstract": "Abstract:Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/bytedance/SALMONN", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate (requires specific package installations and dependencies)", "resource_requirements": {"GPUs": "A100-SXM-80GB", "memory": "Not specified", "API_calls": "Not specified"}, "time_requirements": 5, "code_quality": "Well-documented code", "difficulty": 3, "stars": 1171}}
{"id": "18620", "url": "https://iclr.cc/virtual/2024/poster/18620", "title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents", "authors": [], "abstract": "Abstract:Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately.SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. We release our benchmark at https://github.com/microsoft/SmartPlay", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/microsoft/SmartPlay", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate (requires conda and pip for installation)", "resource_requirements": {"GPUs": "N/A", "CPU": "N/A", "RAM": "N/A", "API_calls": "N/A"}, "time_requirements": 1, "code_quality": "Well-documented", "difficulty": 3, "stars": 134}}
{"id": "18569", "url": "https://iclr.cc/virtual/2024/poster/18569", "title": "Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition", "authors": [], "abstract": "Abstract:Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pre-trained foundation models in visual place recognition (VPR). Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address. To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR. Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model. Besides, to guide effective adaptation, we propose a mutual nearest neighbor local feature loss, which ensures proper dense local features are produced for local matching and avoids time-consuming spatial verification in re-ranking. Experimental results show that our method outperforms the state-of-the-art methods with less training data and training time, and uses about only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based spatial verification. It ranks 1st on the MSLS challenge leaderboard (at the time of submission). The code is released at https://github.com/Lu-Feng/SelaVPR.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/Lu-Feng/SelaVPR", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "api_calls": "None specified"}, "time_requirements": 2, "code_quality": "Well-documented", "difficulty": 3, "stars": 206}}
{"id": "17578", "url": "https://iclr.cc/virtual/2024/poster/17578", "title": "BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks", "authors": [], "abstract": "Abstract:The genome sequence contains the blueprint for governing cellular processes.   While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and challenging. This has sparked interest in unsupervised language modeling of genomic DNA, a paradigm that has seen great success for protein sequence data.   Although various DNA language models have been proposed, evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data. In this study, we introduceBEND, aBENchmark forDNA language models, featuring  a collection of realistic and biologically meaningful downstream tasks defined on the human genome.  We find that embeddings from current DNA LMs can approach performance of expert methods on some tasks, but only capture limited information about long-range features.  BEND is available at https://github.com/frederikkemarin/BEND.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/frederikkemarin/BEND", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "storage": "10GB"}, "time_requirements": 2, "code_quality": "well-documented code", "difficulty": 3, "stars": 106}}
{"id": "17520", "url": "https://iclr.cc/virtual/2024/poster/17520", "title": "ModernTCN: A Modern Pure Convolution Structure for General Time Series Analysis", "authors": [], "abstract": "Abstract:Recently, Transformer-based and MLP-based models have emerged rapidly andwon dominance in time series analysis. In contrast, convolution is losing steamin time series tasks nowadays for inferior performance. This paper studies theopen question of how to better use convolution in time series analysis and makesefforts to bring convolution back to the arena of time series analysis. To this end,we modernize the traditional TCN and conduct time series related modificationsto make it more suitable for time series tasks. As the outcome, we proposeModernTCN and successfully solve this open question through a seldom-exploredway in time series community. As a pure convolution structure, ModernTCN stillachieves the consistent state-of-the-art performance on five mainstream time seriesanalysis tasks while maintaining the efficiency advantage of convolution-basedmodels, therefore providing a better balance of efficiency and performance thanstate-of-the-art Transformer-based and MLP-based models. Our study furtherreveals that, compared with previous convolution-based models, our ModernTCNhas much larger effective receptive fields (ERFs), therefore can better unleash thepotential of convolution in time series analysis. Code is available at this repository:https://github.com/luodhhh/ModernTCN.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/luodhhh/ModernTCN", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate difficulty, requires installation of Python 3.7 and dependencies via pip.", "resource_requirements": {"GPUs": "1 NVIDIA GPU (recommended for faster computation)", "memory": "At least 8GB RAM recommended"}, "time_requirements": "Approximately 1-2 hours to set up and run the main experiments.", "code_quality": "Well-documented code with clear instructions.", "difficulty": 3, "stars": 258}}
{"id": "19323", "url": "https://iclr.cc/virtual/2024/poster/19323", "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models", "authors": [], "abstract": "Abstract:Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, leading to low performance, especially in extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization ($\\textbf{OmniQuant}$) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activation outliers by shifting the challenge of quantization from activations to weights. Operating within a differentiable framework using block-wise error minimization, OmniQuant can optimize the quantization process efficiently for both weight-only and weight-activation quantization. For instance, the LLaMA-2 model family size 7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4 (4-bit weight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16. Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices. Codes are available at \\url{https://github.com/OpenGVLab/OmniQuant}.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/OpenGVLab/OmniQuant", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate (requires conda and pip)", "resource_requirements": {"GPUs": "1 A100 80GB or equivalent", "memory": "At least 4.5GB for some models, up to 87GB for larger models"}, "time_requirements": "2-3 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 777}}
{"id": "19049", "url": "https://iclr.cc/virtual/2024/poster/19049", "title": "Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization", "authors": [], "abstract": "Abstract:Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model's potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at https://github.com/jy0205/LaVIT.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/jy0205/LaVIT", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "3", "resource_requirements": {"GPUs": "1-2 NVIDIA GPUs (e.g., Tesla V100, A100)", "memory": "Minimum 16GB RAM", "API_calls": "Depends on the specific implementation for inference"}, "time_requirements": "2-5", "code_quality": "4", "difficulty": 3, "stars": 566}}
{"id": "19008", "url": "https://iclr.cc/virtual/2024/poster/19008", "title": "On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs", "authors": [], "abstract": "Abstract:Large Language Models (LLMs) have recently showcased their remarkable capacities, not only in natural language processing tasks but also across diverse domains such as clinical medicine, legal consultation, and education. LLMs become more than mere applications, evolving into assistants capable of addressing diverse user requests. This narrows the distinction between human beings and artificial intelligence agents, raising intriguing questions regarding the potential manifestation of personalities, temperaments, and emotions within LLMs. In this paper, we propose a framework, PsychoBench, for evaluating diverse psychological aspects of LLMs. Comprising thirteen scales commonly used in clinical psychology, PsychoBench further classifies these scales into four distinct categories: personality traits, interpersonal relationships, motivational tests, and emotional abilities. Our study examines five popular models, namely text-davinci-003, ChatGPT, GPT-4, LLaMA-2-7b, and LLaMA-2-13b. Additionally, we employ a jailbreak approach to bypass the safety alignment protocols and test the intrinsic natures of LLMs. We have made PsychoBench openly accessible via https://github.com/CUHK-ARISE/PsychoBench.", "pdf_url": "https://openreview.net/pdf?id=H3UayAQWoE", "supplementary_url": "", "code_url": "https://github.com/CUHK-ARISE/PsychoBench", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate - Requires installation via pip for Python and OpenAI API key setup.", "resource_requirements": {"GPUs": "1 x GPU (e.g., NVIDIA A100 or similar)", "memory": "At least 8 GB of RAM recommended.", "API_call": "OpenAI API key required for model access."}, "time_requirements": "Approximately 1 hour to run the main command and obtain results.", "code_quality": "Well-documented code with clear usage instructions.", "difficulty": 3, "stars": 110}}
{"id": "18134", "url": "https://iclr.cc/virtual/2024/poster/18134", "title": "DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks", "authors": [], "abstract": "Abstract:Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a general and flexible protocol for dynamic evaluation of LLMs. Based on our framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo and GPT-4. Experiments show that LLMs perform worse in DyVal-generated evaluation samples with different complexities, highlighting the significance of dynamic evaluation.We also analyze the failure cases and results of different prompting methods.Moreover, DyVal-generated samples are not only evaluation sets, but also helpful data for fine-tuning to improve the performance of LLMs on existing benchmarks.We hope that DyVal can shed light on future evaluation research of LLMs. Code is available at: https://github.com/microsoft/promptbench.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/microsoft/promptbench", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Moderate (requires conda and pip)", "resource_requirements": {"GPUs": "1 or more (NVIDIA recommended)", "memory": "At least 16GB RAM recommended"}, "time_requirements": "2 hours", "code_quality": "Well-documented code", "difficulty": 3, "stars": 2549}}
{"id": "18879", "url": "https://iclr.cc/virtual/2024/poster/18879", "title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages", "authors": [], "abstract": "Abstract:Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a (quasi)-zero-shot manner, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MPM, we build large multimodal models VisCPM in image-to-text and text-to-image generation, which achieve state-of-the-art (open-source) performance in Chinese. To facilitate future research, we open-source codes and model weights at https://github.com/OpenBMB/VisCPM.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/OpenBMB/VisCPM", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": "1 or more NVIDIA GPUs with at least 5GB VRAM for VisCPM-Chat and 17GB VRAM for VisCPM-Paint", "memory": "At least 5GB of RAM, depending on the task", "API_call": "API available for VisCPM-Chat"}, "time_requirements": "2 hours", "code_quality": "well-documented", "difficulty": 3, "stars": 1053}}
{"id": "17573", "url": "https://iclr.cc/virtual/2024/poster/17573", "title": "In-context Autoencoder for Context Compression in a Large Language Model", "authors": [], "abstract": "Abstract:We propose the In-context Autoencoder (ICAE), leveraging the power of a large language model (LLM) to compress a long context into short compact memory slots that can be directly conditioned on by the LLM for various purposes. ICAE is first pretrained using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, it is fine-tuned on instruction data for producing desirable responses to various prompts. Experiments demonstrate that our lightweight ICAE, introducing about 1% additional parameters, effectively achieves $4\\times$ context compression based on Llama, offering advantages in both improved latency and GPU memory cost during inference, and showing an interesting insight in memorization as well as potential for scalability. These promising results imply a novel perspective on the connection between working memory in cognitive science and representation learning in LLMs, revealing ICAE's significant implications in addressing the long context problem and suggesting further research in LLM context management. Our data, code and models are available at https://github.com/getao/icae.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/getao/icae", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "Medium", "resource_requirements": {"GPUs": {"type": "NVIDIA", "amount": 1}, "memory": "16GB", "API_calls": "None specified"}, "time_requirements": 2, "code_quality": "Well-documented", "difficulty": 3, "stars": 110}}
{"id": "19207", "url": "https://iclr.cc/virtual/2024/poster/19207", "title": "PB-LLM: Partially Binarized Large Language Models", "authors": [], "abstract": "Abstract:This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of na\u00efve applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian matrix and successfully recover the reasoning capacity of PB-LLM in low-bit. Under QAT, we freeze the salient weights during training, explore the derivation of optimal scaling factors crucial for minimizing the quantization error, and propose a scaling mechanism based on this derived scaling strategy for residual binarized weights. Those explorations and the developed methodologies significantly contribute to rejuvenating the performance of low-bit quantized LLMs and present substantial advancements in the field of network binarization for LLMs. Code is available at https://github.com/hahnyuan/PB-LLM.", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/hahnyuan/PB-LLM", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "medium", "resource_requirements": {"GPUs": {"count": 4, "type": "NVIDIA", "description": "Multiple GPUs required for training and evaluation"}, "memory": "16GB or more recommended"}, "time_requirements": "5", "code_quality": "well-documented code", "difficulty": 3, "stars": 151}}
{"id": "17642", "url": "https://iclr.cc/virtual/2024/poster/17642", "title": "DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines", "authors": [], "abstract": "Abstract:The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded \u201cprompt templates\u201d, i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, or imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric, by creating and collecting demonstrations. We conduct two case studies, showing that succinct DSPy programs can express and optimize pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, DSPy can automatically produce pipelines that outperform out-of-the-box few-shot prompting as well as expert-created demonstrations for GPT-3.5 and Llama2-13b-chat. On top of that, DSPy programs compiled for relatively small LMs like 770M parameter T5 and Llama2-13b-chat are competitive with many approaches that rely on large and proprietary LMs like GPT-3.5 and on expert-written prompt chains. DSPy is available at https://github.com/stanfordnlp/dspy", "pdf_url": "", "supplementary_url": "", "code_url": "https://github.com/stanfordnlp/dspy", "bibtex": "", "keywords": [], "reproduce_difficulty": {"environment_setup": "easy", "resource_requirements": {"GPUs": "None specified", "memory": "Not specified", "API_call": "Not specified"}, "time_requirements": "2", "code_quality": "well-documented", "difficulty": 2, "stars": 22252}}
